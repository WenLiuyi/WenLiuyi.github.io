<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>并行训练系列：1. Overview | Liuyi Wen's Blog</title><meta name="author" content="Liuyi Wen"><meta name="copyright" content="Liuyi Wen"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="该篇摘自The Ultra-Scale Playbook: Training LLMs on GPU Clusters. 在单个GPU上训练 在单个GPU上训练，通常包括三个步骤： 1. forward pass：将输入传入模型，产生输出； 2. backward pass：计算梯度； 3. optimization：使用梯度更新参数。  batch size的影响 超参数batch size：小"><meta property="og:type" content="article"><meta property="og:title" content="并行训练系列：1. Overview"><meta property="og:url" content="http://wenliuyi.github.io/posts/ca7e7de.html"><meta property="og:site_name" content="Liuyi Wen&#39;s Blog"><meta property="og:description" content="该篇摘自The Ultra-Scale Playbook: Training LLMs on GPU Clusters. 在单个GPU上训练 在单个GPU上训练，通常包括三个步骤： 1. forward pass：将输入传入模型，产生输出； 2. backward pass：计算梯度； 3. optimization：使用梯度更新参数。  batch size的影响 超参数batch size：小"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://wenliuyi.github.io/img/WechatIMG105.jpg"><meta property="article:published_time" content="2025-03-31T02:02:38.000Z"><meta property="article:modified_time" content="2025-09-18T15:55:56.406Z"><meta property="article:author" content="Liuyi Wen"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="http://wenliuyi.github.io/img/WechatIMG105.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "并行训练系列：1. Overview",
  "url": "http://wenliuyi.github.io/posts/ca7e7de.html",
  "image": "http://wenliuyi.github.io/img/WechatIMG105.jpg",
  "datePublished": "2025-03-31T02:02:38.000Z",
  "dateModified": "2025-09-18T15:55:56.406Z",
  "author": [
    {
      "@type": "Person",
      "name": "Liuyi Wen",
      "url": "http://wenliuyi.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://wenliuyi.github.io/posts/ca7e7de.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>(()=>{const e={set:(e,t,o)=>{if(!o)return;const a=Date.now()+864e5*o;localStorage.setItem(e,JSON.stringify({value:t,expiry:a}))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const{value:o,expiry:a}=JSON.parse(t);if(!(Date.now()>a))return o;localStorage.removeItem(e)}};window.btf={saveToLocal:e,getScript:(e,t={})=>new Promise((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,Object.entries(t).forEach(([e,t])=>n.setAttribute(e,t)),n.onload=n.onreadystatechange=()=>{n.readyState&&!/loaded|complete/.test(n.readyState)||o()},n.onerror=a,document.head.appendChild(n)}),getCSS:(e,t)=>new Promise((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onload=n.onreadystatechange=()=>{n.readyState&&!/loaded|complete/.test(n.readyState)||o()},n.onerror=a,document.head.appendChild(n)}),addGlobalFn:(e,t,o=!1,a=window)=>{if(e.startsWith("pjax"))return;const n=a.globalFn||{};n[e]=n[e]||{},n[e][o||Object.keys(n[e]).length]=t,a.globalFn=n}};const t=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},o=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};btf.activateDarkMode=t,btf.activateLightMode=o;const a=e.get("theme");"dark"===a?t():"light"===a&&o();const n=e.get("aside-status");void 0!==n&&document.documentElement.classList.toggle("hide-aside","hide"===n);/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})()</script><script>const GLOBAL_CONFIG={root:"/",algolia:{appId:"VE36MEFVE6",apiKey:"f9b9ca5a3cdb9455658600dba6ae7706",indexName:"hexo-algolia indexing key",hitsPerPage:6,languages:{input_placeholder:"搜索文章",hits_empty:"未找到符合您查询的内容：${query}",hits_stats:"找到 ${hits} 条结果，耗时 ${time} 毫秒"}},localSearch:void 0,translate:void 0,highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:!1,highlightFullpage:!1,highlightMacStyle:!1},copy:{success:"复制成功",error:"复制失败",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"null",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!1,islazyloadPlugin:!1,isAnchor:!1,percent:{toc:!0,rightside:!1},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"并行训练系列：1. Overview",isHighlightShrink:!1,isToc:!0,pageType:"post"}</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Liuyi Wen's Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">并行训练系列：1. Overview</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div></div></nav><div id="post-info"><h1 class="post-title">并行训练系列：1. Overview</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-03-31T02:02:38.000Z" title="发表于 2025-03-31 10:02:38">2025-03-31</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-09-18T15:55:56.406Z" title="更新于 2025-09-18 23:55:56">2025-09-18</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Parallelism/">Parallelism</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><p>该篇摘自<a target="_blank" rel="noopener" href="https://huggingface.co/spaces/nanotron/ultrascale-playbook">The Ultra-Scale Playbook: Training LLMs on GPU Clusters</a>.</p><h2 id="在单个gpu上训练">在单个GPU上训练</h2><p>在单个GPU上训练，通常包括三个步骤： 1. forward pass：将输入传入模型，产生输出； 2. backward pass：计算梯度； 3. optimization：使用梯度更新参数。</p><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image.png"></p><h3 id="batch-size的影响">batch size的影响</h3><p>超参数<strong>batch size</strong>：小的batch size在训练初期有助于快速完成训练过程，达到一个较优learning point；但在训练后期，小的batch size导致梯度噪声增大，模型难以收敛至最优性能点；大的batch size虽然能给出精确的梯度估计，但会降低每个训练样本的利用效率，从而导致收敛变慢，并可能浪费计算资源。</p><p>batch size影响在给定dataset上的训练时间：小的batch size在相同数量样本上，需要更多的优化步骤（优化步骤是计算密集型的，导致训练时间比大的batch size更长）。但是，batch size大小通常可以在最优值附近大幅调整，而不会对模型的最终性能产生重大影响（前提是在最优值附近）。</p><p>在LLM的预训练中，batch size通常定义为：token的数量（bst：Batch Size Tokens），使得训练次数和训练中使用的输入序列长度基本独立。在单个机器上训练，<code>bs</code>（样本计数）和<code>bst</code>（token计数）可由下计算： <span class="math display">\[ bst=bs * seq \]</span> 其中，<code>seq</code>为输入序列长度。</p><blockquote><p>近期 LLM 训练的理想批量大小通常在每批次 400 万到 6000 万个 token 之间。批量大小和训练语料库的规模近年来一直在稳步增加：Llama 1 的训练使用了大约 400 万个 token 的批量大小，训练了 1.4 万亿个 tokens，而 DeepSeek 则使用了大约 6000 万个 token 的批量大小，训练了 14 万亿个 tokens。</p></blockquote><p>然而，一个挑战是在将模型训练扩展到大的batch size时，将遇到<strong>显存不足</strong>的问题：当 GPU 的显存不足以容纳目标batch size的完整批次时，该怎么办？</p><h3 id="transformer上的内存使用">Transformer上的内存使用</h3><p>当训练一个神经网络时，将以下内容存储在内存中：模型权重、模型梯度、优化器状态、（用于计算梯度的）激活值。</p><p>以上内容作为<strong>tensor（张量）</strong>存储在内存中，分别对应不同的shapes和precisions。</p><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-1.png"></p><p>训练基本步骤：前向传播时，激活值迅速增加；反向传播时，梯度逐渐积累，且计算梯度的激活值会逐步被清除；最后，执行优化步骤，此时需要所有的梯度，并更新优化器状态；然后才开始下一次的前向传播。</p><blockquote><p>第一步和后续步骤明显不同的原因：激活值快速增加，再保持一段时间的平稳。在第一步中，torch 的缓存分配器进行大量准备工作，预先分配内存；后续步骤不再需要寻找空闲内存块，从而加速）</p></blockquote><h4 id="weightsgradsoptimizer-state的内存">weights/grads/optimizer state的内存</h4><p>对于一个简单的transformer LLM，参数数量如下： <span class="math display">\[ N=h*v+L*(12*h^2+13*h)+2*h \]</span> <span class="math inline">\(h\)</span>是隐藏层维度，<span class="math inline">\(v\)</span>是词汇大小，<span class="math inline">\(L\)</span>是模型的层数；可以看到，当隐藏层维度较大时，主导项是<span class="math inline">\(h^2\)</span>项。</p><p>**内存需求：参数数量*每个参数的字节数**</p><blockquote><p>传统FP32训练中，参数、梯度均需4字节，优化器（例如Adam）需要存储动量和方差，为每个参数增加另外两个4字节。</p><p><span class="math inline">\(m_{params}=4*N\)</span></p><p><span class="math inline">\(m_{grad}=4*N\)</span></p><p><span class="math inline">\(m_{opt}=(4+4)*N\)</span></p><p>若使用高低混合精度训练，当前默认做法是：使用BF16进行大部分计算（每个参数、梯度分别需要2字节），额外复制一份模型权重和梯度为 FP32，因此每个参数总共需要 12 字节。即：</p><p><span class="math inline">\(m_{params}=2*N\)</span></p><p><span class="math inline">\(m_{grad}=2*N\)</span></p><p><span class="math inline">\(m_{params_{fp32}}=4*N\)</span></p><p><span class="math inline">\(m_{opt}=(4+4)*N\)</span></p><p>混合精度本身并不会节省整体内存，它只是将内存在三个组件之间重新分配。在前向和反向传播中使用半精度计算可以： 1. 在 GPU 上使用经过优化的低精度操作，这些操作更快； 2. 减少前向传播过程中的激活内存需求，而激活内存占用了大量内存。</p><p>若使用 FP8 训练代替 BF16，内存使用量会进一步减少（但它的稳定性较差）。</p><table><colgroup><col style="width:18%"><col style="width:45%"><col style="width:36%"></colgroup><thead><tr><th>模型参数数量</th><th>FP32 或 BF16（不使用 FP32 梯度累积）</th><th>BF16（使用 FP32 梯度累积）</th></tr></thead><tbody><tr><td>1B</td><td>16 GB</td><td>20 GB</td></tr><tr><td>7B</td><td>112 GB</td><td>140 GB</td></tr><tr><td>70B</td><td>1120 GB</td><td>1400 GB</td></tr><tr><td>405B</td><td>6480 GB</td><td>8100 GB</td></tr></tbody></table><p>可以观察到，一旦达到 7B 参数，权重和优化器的内存需求就会显著增加，并超过典型 GPU 内存的大小。</p></blockquote><h4 id="activations的内存">activations的内存</h4><p>依赖于模型的输入。总内存如下： <span class="math display">\[ m_{act}=L*seq*bs*h*(34+\frac{5*n_{heads}*seq}{h}) \]</span> 其中，<span class="math inline">\(L\)</span>是层数，<span class="math inline">\(seq\)</span>是序列长度，<span class="math inline">\(bs\)</span>是batch size，<span class="math inline">\(h\)</span>是模型的隐藏维度，<span class="math inline">\(n_{heads}\)</span>是注意力头的数量。</p><p>可以观察到，内存使用量会随着批量大小线性增长，并随着序列长度的平方增长，那么：激活内存是最容易“膨胀”的部分。</p><p>对于短序列（或者小批量大小），激活几乎可以忽略不计；但从大约 2-4k 个 token 开始，它们就会占用大量内存，而参数、梯度和优化器状态的使用，则基本上与序列长度和批量大小无关。</p><h3 id="控制activation增长的策略">控制activation增长的策略</h3><h4 id="activation重计算gradient-checkpoints">activation重计算（gradient checkpoints）</h4><p>也叫做：梯度检查点，重物化。在前向传播时，抛弃一些activations；在后向传播时，实时重新计算activations。</p><ul><li>Full（全量重计算）：在Transformer的每层transition point上，设置activations checkpoints：要求每层进行一次前向传播，即在反向传播过程中增加一次完整的前向传播。<ul><li>可以节省最多内存，但在计算上最昂贵。</li></ul></li><li>Selective（选择性重计算）：注意力激活值增长较多且在FLOP上计算便宜，因此抛弃他们。<ul><li>对于一个GPT-3（175B）模型，可以减少70%的激活内存，而计算成本仅为2.7%；DeepSeek V3使用“多头潜在注意力”（MLA）来优化激活内存。</li></ul></li></ul><p>当前大多数框架使用<strong>Flash Attention</strong>，在其优化策略中，原生集成了activation重计算：在反向传播中，计算（而非存储）注意力分数和矩阵。</p><blockquote><p>activation重计算略微增加FLOPs的数量；但显著减少内存开销。该策略对具备小型高速内存的硬件尤为有利（比如GPU）。</p></blockquote><h4 id="梯度累积gradient-accumulation">梯度累积（gradient accumulation）</h4><p>梯度累积将batch拆分为若干个小的micro-batch；依次在每个micro-batch上进行前向、反向传播，计算梯度，在执行优化步骤前，将所有micro-batch梯度相加。实际上，优化步骤是基于梯度的平均值（而非总和）进行的，因此结果与梯度累积步骤的数量无关。有： <span class="math display">\[ bs=gbs=mbs*grad_{acc} \]</span> 其中：每次前向传播的batch size为<span class="math inline">\(mbs\)</span>；每两个优化步骤之间的batch size为<span class="math inline">\(gbs\)</span>。假设在每进行8次前向/反向传播后执行一次优化步骤，则<span class="math inline">\(gbs\)</span>将是<span class="math inline">\(mbs\)</span>的8倍。</p><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-2.png"></p><p>梯度累积的一个缺点：在每个优化步骤中，需要执行多个连续的前向/反向传播，从而增加计算开销，减慢计算速度。</p><p>然而，每个micro-batch的前向/反向传播可以并行运行。前向/反向传播是相互独立的，唯一的区别是输入样本。因此可以将训练扩展至多个GPU！</p><h2 id="并行策略">并行策略</h2><h3 id="数据并行data-parallelism">数据并行（Data Parallelism）</h3><p>思想：将模型复制到多个GPU上；在每个GPU上，对不同的micro batches执行前向/反向传播。</p><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-3.png"></p><p>在每个GPU上使用不同的micro batch，那么每个GPU上的梯度不同；为了保持不同GPU上的模型实例同步，使用<strong>all-reduce对模型实例的梯度进行平均</strong>，该过程在优化之前的反向传播中执行。</p><ul><li>all-reduce原语：处理 GPU 实例和节点之间的同步和通信。 <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-4.png"></li></ul><p>一个朴素的实现方式：等待反向传播完成所有的梯度计算；触发all-reduce操作，进行通信以同步这些梯度。然而，这会导致通信时GPU空闲，而我们希望通信和计算能并行。有哪些方法呢？</p><h4 id="优化策略">优化策略</h4><h5 id="优化一梯度同步通信与反向传播计算并行">优化一：梯度同步（通信）与反向传播（计算）并行</h5><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-5.png"> 一旦最后一层的反向传播计算完成，这些梯度可以立即被收集、求和；而反向传播计算会继续向左传播，计算更早层的梯度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">register_backward_hook</span>(<span class="params">self, hook</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Registers a backward hook for all parameters of the model that </span></span><br><span class="line"><span class="string">    require gradients.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> <span class="variable language_">self</span>.module.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.requires_grad <span class="keyword">is</span> <span class="literal">True</span>:</span><br><span class="line">            p.register_post_accumulate_grad_hook(hook)</span><br></pre></td></tr></table></figure><h5 id="优化二梯度分桶">优化二：梯度分桶</h5><p>GPU 操作通常在大tensor上执行时效率更高；通信操作亦然。因此，可以通过将梯度分组到多个桶中，并为每个桶内的所有梯度启动一个单独的 all-reduce 操作，（而不是为每个梯度执行独立的 all-reduce 操作）。显著减少通信开销，加速通信操作。 <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-6.png"></p><h5 id="优化三配合梯度累积">优化三：配合梯度累积</h5><p>何时同步梯度？</p><p>在一个简单版本中，每次反向传播后，自动触发一个 all-reduce 操作，这样效率较低：在最终步骤之后执行一次 reduce 操作能达到相同效果，同时减少开销。</p><blockquote><p>在 PyTorch 中，通常在不需要进行梯度同步的反向传播上添加 <code>model.no_sync()</code> 装饰器，来解决这个问题。</p></blockquote><p>加入DP和梯度累积参数后，global batch size更新如下： <span class="math display">\[ bs=gbs=mbs*grad_{acc}*dp \]</span> 其中，<span class="math inline">\(grad_{acc}\)</span>是梯度累积的步数，<span class="math inline">\(dp\)</span>是DP中并行实例的数量。</p><blockquote><p>实际上，一般倾向于最大化DP中并行节点的数量：因为DP是并行的，梯度累积是顺序的。在数据并行扩展不足时，再加上梯度累积，以达到目标的global batch size。</p></blockquote><h4 id="dp步骤">DP步骤</h4><p>总结一下采用DP进行训练的配置步骤：</p><ol type="1"><li>确定最佳的global batch size(in tokens)；</li><li>选择训练的序列长度（2~8k个tokens当前结果不错）；</li><li>寻找单个GPU上最大的local batch size(mbs)（不断增加，直到耗尽内存）；</li><li>确定DP使用的GPU数量：GBS 与 DP 的比值决定所需的梯度累积步数。</li></ol><blockquote><p>例子： 假设要训练一个global batch size=4M的模型，序列长度为4k；则批量大小为1024个样本。</p><p>假设观察到单个 GPU 只能容纳 MBS=2 的内存，并且有 128 个 GPU 可供训练。那么：通过4步梯度累积，将实现每个训练步骤 1024 个样本或 4M tokens 的目标。</p><p>如果突然有 512 个 GPU 可用，仍然可以保持 MBS=2，并将梯度累积步数设置为 1，从而实现更快的训练！</p><p>注意：在使用 512+ 个 GPU 的规模时，取决于所使用的网络，通信操作将开始受到环延迟的限制，这会降低计算效率，并影响吞吐量。</p></blockquote><p>虽然DP将梯度同步的 all-reduce 操作与反向传播计算重叠以节省时间，但这种好处在大规模下开始失效。为什么？因为随着添加更多的 GPU（成百上千个），它们之间的协调开销会显著增加，导致网络需求变得过大，抵消了带来的好处。随着每个新 GPU 加入，设置DP的效率将越来越低。</p><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-7.png"></p><h4 id="deepspeed-zero零冗余优化器">DeepSpeed ZeRO（零冗余优化器）</h4><p>在每个DP rank上对优化状态、梯度、参数进行赋值，将导致大量内存冗余。<strong>ZeRO通过在数据并行维度上，对优化器状态、梯度和参数进行分区来消除内存冗余</strong>，同时仍然允许使用完整的参数集进行计算。</p><blockquote><p>activations不参与分区：每个DP replica接收不同的micro-batch，因此每个DP节点上的activations也不同，不参与复制。</p></blockquote><p>考虑如下场景：使用混合精度训练和Adam优化器时，假设模型参数量为 <span class="math inline">\(\psi\)</span>​，那么每张GPU中的显存内容分为两类：</p><ol type="1"><li>模型状态：<ul><li>模型参数（半精度，bf16/fp16）：<span class="math inline">\(2\psi\)</span></li><li>模型梯度（半精度，bf16/fp16）：<span class="math inline">\(2\psi\)</span></li><li>Adam优化器状态（FP32格式的模型参数备份、FP32的momentum和FP32的variance）：<span class="math inline">\(4\psi+4\psi+4\psi\)</span> Adam状态占比75%。</li></ul></li><li>剩余状态： 除了模型状态之外的显存占用，包括activation、各种buffer以及无法使用的显存碎片（fragmentation）。</li></ol><blockquote><p>混合精度训练：同时存在fp16和fp32两种格式的数值，其中模型参数、模型梯度都是fp16，此外还有fp32的模型参数，如果优化器是Adam，则还有fp32的momentum和variance。、 <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-8.png"></p></blockquote><p>假设显卡数量为<span class="math inline">\(N\)</span>，提出以下三种ZeRO算法： <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-9.png"></p><ul><li>ZeRO-1：<strong>只对优化器状态进行分片</strong>，每张卡保存<span class="math inline">\(\frac{1}{N}\)</span>​的状态量。此时，每张卡所需显存是<span class="math inline">\(4\psi+\frac{12\psi}{N}\)</span>字节，当<span class="math inline">\(N\)</span>较大时，趋向于<span class="math inline">\(4\psi\)</span>，记为<span class="math inline">\(P_{os}\)</span>；</li><li>ZeRO-2：<strong>对优化器状态和梯度进行分片</strong>，此时，每张卡所需显存是<span class="math inline">\(2\psi+\frac{2\psi+12\psi}{N}\)</span>字节，当<span class="math inline">\(N\)</span>较大时，趋向于<span class="math inline">\(2\psi\)</span>，记为<span class="math inline">\(P_{os+g}\)</span>；</li><li>ZeRO-3：<strong>将模型参数、梯度、优化器状态三者都进行分片</strong>，此时，每张卡所需显存是<span class="math inline">\(\frac{16\psi}{N}\)</span>字节，当<span class="math inline">\(N\)</span>较大时，趋向于<span class="math inline">\(0\)</span>，记为<span class="math inline">\(P_{os+g+p}\)</span>；<ul><li>ZeRO-3对应Pytorch FSDP</li></ul></li></ul><h5 id="zero-1zero-2zero-3通信量分析">ZeRO-1，ZeRO-2，ZeRO-3通信量分析</h5><blockquote><p>集群通信：</p><p>reduce-scatter: <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-10.png"> all-gather: <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-11.png"> Ring all-reduce:由reduce-scatter，all-gather两个步骤组成： <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-12.png"></p></blockquote><p>传统的DP在每一步计算梯度后，需要一次all-reduce操作计算梯度均值，当前常用Ring all-reduce，分为reduce-scatter和all-gather两步。</p><ul><li><p>ZeRO-1，ZeRO-2将all-reduce梯度通信改为：reduce-scatter操作，并在优化器步骤之后，增加了对所有参数的all-scatter操作。 <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-13.png"></p><ul><li><span class="math inline">\(P_{os}\)</span>，<span class="math inline">\(P_{os+g}\)</span>和传统DP的通信量相同</li></ul></li><li><p>ZeRO-3：</p><ul><li>前向传播：依次通过各个layer，按需获取必要的参数；在参数不再需要时，立即从显存中清除。 <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-14.png"></li><li>反向传播：生成梯度分片。 <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-15.png"></li></ul><p>需要在前向/反向传播中，持续执行all-gathers操作，那么与ZeRO-2相比，需要额外执行<span class="math inline">\(2*numLayers-1\)</span>次all-gather，每次操作都会带来一个小的基础延迟开销。</p><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-16.png"></p><p>在前向传播时，需要参数时执行all-gather操作，产生一个<span class="math inline">\(\psi\)</span>的通信开销，立即清除不需要的参数，因此反向传播时还需要一次all-gather操作；最后，与ZeRO-2相同，进行reduce-scatter操作处理梯度，产生<span class="math inline">\(\psi\)</span>的通信开销。总通信开销为：<span class="math inline">\(3\psi\)</span>（ZeRO-2的通信开销为<span class="math inline">\(2\psi\)</span>）</p></li></ul><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-17.png"></p><h5 id="zero-r">ZeRO-R</h5><p>在进行tensor并行时，前向传播中的activations会在各个GPU中重复存储，因此：ZeRO-R将所有的中间activations分片存储，即只对activation checkpoints分片（其他activations已被抛弃）</p><p>见：<a href="####activation重计算（gradient%20checkpoints）">重计算</a></p><blockquote><p>正常情况：保存前向传播中，每一个activations，用于反向传播时计算梯度；每一个前向中的activation，到计算完对应梯度节点后，才能释放。</p><ul><li>缺点：需要保存大量的中间激活值，导致占用了大量显存，并且所需的显存是随着层数n线性增长的。</li></ul><p>优化一：将所有的中间激活值全部丢弃，反向传播需要时，再重新计算；</p><ul><li>缺点：训练速度慢，每个前向节点原本只需要计算一次，现在最多需要计算n次！</li></ul><p>折中做法：选取一些前向节点作为checkpoint，训练时，这些checkpoint节点的激活值会一直保存在显存中，而其他节点的激活值会被丢弃。</p><ul><li>优点：计算反向梯度节点时，只需要从离它最近的checkpoint节点开始计算，而不用把每个节点都重新计算一遍。</li></ul></blockquote><h5 id="zero-offload">ZeRO-Offload</h5><p>GPU显存不够用，则：<strong>将一部分计算和存储下放到CPU和内存</strong>，并且不让CPU和GPU之间的通信成为瓶颈，也不让CPU参与过多计算，避免CPU计算成为瓶颈。</p><p>Adma优化器中，每一层迭代如下： <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-19.png"></p><p>将数据流图切分成CPU和GPU两部分。ZeRO-Offload策略如下：它将<strong>计算复杂度较高的前向FWD和反向BWD放在GPU上</strong>；而<strong>参数更新和float2half这两个计算操作放在CPU上</strong>。因此，优化器状态也放在内存中，</p><p>述方法仅仅针对单卡场景。在多卡场景下，ZeRO-Offload利用ZeRO-2方法。ZeRO-2将优化器状态和梯度分片，每张卡只存储<span class="math inline">\(\frac{1}{N}\)</span>​，而ZeRO-Offload将这<span class="math inline">\(\frac{1}{N}\)</span><strong>个​优化器状态和梯度都下放到内存，只在CPU上进行参数更新</strong>.</p><p>更多内容参考：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/700525463">大模型并行训练技术（一）—— ZeRO系列</a></p><h3 id="张量并行tensor-parallelism">张量并行（Tensor Parallelism）</h3><h4 id="数学原理">数学原理</h4><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-20.png"></p><p>在神经网络中，矩阵乘法常用以下方式表示：<span class="math inline">\(X\times W\)</span>，其中：</p><ul><li><span class="math inline">\(X\)</span>为activation的输入；</li><li><span class="math inline">\(W\)</span>为<code>nn.Linear</code>的权重。 <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-21.png"></li></ul><p>在TP中，tensors将被沿着一个特定维度分为N个shards，并分布在N个GPU上。矩阵可以按行/按列切分，分别对应行并行、列并行。</p><h5 id="column-linear">column-linear</h5><ol type="1"><li>运用<strong>broadcast</strong>操作，将输入矩阵复制到每个worker；</li><li>将每个权重矩阵切分为若干个列，分别与输入矩阵相乘，最后通过<strong>all-gather</strong>操作结合。</li></ol><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-22.png"></p><h5 id="row-linear">row-linear</h5><ol type="1"><li>运用<strong>scatter</strong>操作，将输入矩阵切分为若干个列；</li><li>将每个权重矩阵切分为若干行，分别与输入矩阵的各列相乘，最后通过<strong>all-reduce</strong>操作相加。</li></ol><h4 id="transformer-block内部的张量并行">Transformer Block内部的张量并行</h4><p>一个Transformer由两个主要的block组成：前向反馈层（Feedbackforward layers，MLP）和多头注意力层（Multi-Head Attention，MHA），可以同时运用TP。</p><h5 id="前向反馈层mlp">前向反馈层（MLP）</h5><p>MLP：先使用column-linear，再使用row-linear（现实训练中不需要broadcast操作，因为可以确保输入已经在TP ranks之间同步）</p><blockquote><p>比先row-linear后column-linear更快，省去了中间的all-reduce操作。 <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-23.png"></p></blockquote><h5 id="多头注意力层mha">多头注意力层（MHA）</h5><p>将 Q、K 和 V 矩阵按列并行拆分，输出投影则沿着行维度拆分。在多头注意力的情况下，按列并行的方法有一个非常自然的解释：<strong>每个worker计算单个或一部分head的注意力</strong>。这种方法同样适用于多查询（MQA）或分组查询注意力（GQA），其中，<strong>keys和values在queries之间共享</strong>。 <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-24.png"></p><blockquote><p>值得注意的是，<strong>张量并行度（TP degree）不应超过Q/K/V 头的数量</strong>，因为需要保证每个 TP rank的head是完整的（否则无法在每个 GPU 上独立计算注意力，需要额外的通信操作）。</p><p>如果使用 GQA，TP 幅度应该实际小于 K/V 头的数量。例如，LLaMA-3 8B 模型有 8 个Key/Value heads，因此TP degree最好不要超过 8；如果我们为这个模型使用 TP=16，那么我们需要在每个 GPU 上复制 K/V 头，并确保它们保持同步。</p></blockquote><h4 id="tp性能">TP性能</h4><p>然而，TP也不是万全之策。需要在模型的计算路径中直接添加了多个分布式通信原语，因此这些通信操作很难完全隐藏或与计算重叠（就像在 ZeRO 中做的那样）。最终的性能将是计算和内存增益与额外通信开销之间的折中结果。举个例子：</p><p>MLP的操作流程图如下： <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-25.png"> 在每个decoder layer的前向传播中，会遇到一个同步点（即all-reduce操作，无法与计算重叠）；该通信开销是必需的，在应用 LayerNorm 之前，合并tensor-parallel ranks的结果。</p><ul><li>TP优点：TP有助于减少矩阵乘法的activation memory，因为过程中间的activations被分片存储在不同的 GPU 上；</li><li>TP缺点：<ul><li>需要收集完整的activations以执行类似 LayerNorm 的操作，这并没有完全利用内存的优势；</li><li>引入大量通信需求，严重依赖于网络基础设施。由于无法完全将这个特定的 AllReduce 操作与计算重叠，它直接延长了前向传播的关键路径。</li></ul></li></ul><p>下图展示分布式训练中，计算效率和内存可用性之间的trade-off：<strong>随着TP degree增加，虽然每个GPU的吞吐量减少（左图），但它能够处理更大的批量大小（右图）。</strong> <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-26.png"></p><blockquote><p>实际上，正如左图所示：TP的通信开销在超越 <strong>8 个 GPU</strong> 时变得尤为显著：虽然在单节点内使用TP时，可以利用快速的 NVLink 互连，但跨节点通信则依赖于较慢的网络连接；当从 TP=8 增加到 TP=16 时，性能有显著下降；而从 TP=16 增加到 TP=32 时，下降更加明显。在更高的并行度下，通信开销变得如此之高，以至于它迅速主导了计算时间。</p></blockquote><h5 id="b大模型的内存使用量">70B大模型的内存使用量</h5><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-27.png"></p><p>是否有办法从TP中获得更多的好处呢？可以看到，层归一化（Layer Normalization）和Dropout仍然需要在每个 GPU 上收集完整的activations，这在一定程度上抵消了内存节省。可以通过寻找方法将这些剩余操作也并行化，从而做得更好。</p><blockquote><ul><li>TP中的层归一化：由于每个 TP rank 在 all-gather 之后看到的是相同的activations，因此层归一化的权重实际上不需要 all-reduce 来同步它们的梯度。在反向传播之后，它们自然会在各个 rank 上保持同步；</li><li>TP中的Dropout：必须确保跨 TP rank 同步随机种子，以保持行为的确定性。</li></ul></blockquote><h4 id="一个小扩展序列并行sequence-parallelism">一个小扩展：序列并行（Sequence Parallelism）</h4><p>SP切分TP未处理的activations和computations（例如：<strong>LayerNorm和Dropout</strong>），但是沿着input sequence的维度（不是跨隐藏层的维度）。</p><p>上述操作<strong>需要访问完整的隐藏维度</strong>才能正确计算。例如：LayerNorm需要完整的隐藏维度，以计算均值和方差： <span class="math display">\[ LayerNorm(x)=\gamma\cdot\frac{x-\mu}{\sqrt(\sigma^{2}+\epsilon)}+\beta \]</span> 其中，<span class="math inline">\(\mu=mean(x)\)</span>，<span class="math inline">\(\sigma^{2}=var(x)\)</span>是沿着隐藏层<span class="math inline">\(h\)</span>计算的。</p><p>尽管这些操作在计算上比较简单，但它们仍然需要大量的激活内存，因为它们需要完整的隐藏维度。SP允许我们沿着sequence维度拆分，来将这一内存负担分摊到多个GPU上。</p><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-28.png"></p><ul><li>前向传播：<ul><li>"f" 是一个无操作（no-op），因为activations已经在不同的rank之间进行了复制；</li><li>"f*" 是一次全归约（all-reduce），用于同步activations，并确保正确性。</li></ul></li><li>反向传播：<ul><li>"f*" 是一个无操作（no-op），因为gradient已经在不同的rank之间进行了复制；</li><li>"f" 是一次全归约（all-reduce），用于同步gradient。</li></ul></li></ul><p>这些操作“f”和“f*”被称为共轭对，因为它们是互补的——在前向传播中，当一个是no-op时，另一个在反向传播中是all-reduce，反之亦然。 &gt; 在SP中，避免使用all-reduce（需要收集完整的激活值）</p><p>事实上发生了什么呢？ * <strong>初始层归一化（SP区域）</strong> * 输入张量 X1 和 X2（形状为 b, s/2, h）进入LayerNorm，已经沿sequence维度进行拆分 每个GPU独立计算它们各自序列块的LayerNorm，得到 Y1 和 Y2； * <strong>第一次转换（SP → TP）</strong> * “g”操作（all-gather）将 Y1 和 Y2 合并回完整的序列长度 恢复 Y（形状为 b, s, h） * <strong>第一次线性变换（TP区域）</strong> * A1层 是column-linear，所以它沿隐藏维度拆分 Y；GeLU 激活函数在每个GPU上独立应用 Z1，形状为 (b, s, h/2) * <strong>第二次线性变换（TP区域）</strong> * B1层 是row-linear，它恢复隐藏维度 W1 形状为 (b, s, h) * <strong>最后转换（TP → SP）</strong> * “g*”操作（reduce-scatter），在前一个row-linear层的进行dropout，同时在sequence维度上进行分散；W1形状为 (b, s/2, h)</p><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-29.png"></p><p>SP：的一大优势是：它减小了需要存储的最大的activation size。在仅使用TP时，需要在不同的点存储形状为（b, s, h）的activations。然而，使用SP后，最大的activation size减小到<span class="math inline">\(\frac{b\cdot s\cdot h}{tp}\)</span>（<span class="math inline">\(tp\)</span>是分割数），因为总是沿着序列维度或隐藏维度进行拆分。</p><p>以下表格描述：前向传播过程中，activations shape随着隐藏维度<span class="math inline">\(h\)</span>和序列维度<span class="math inline">\(s\)</span>的变化：</p><table><colgroup><col style="width:21%"><col style="width:39%"><col style="width:38%"></colgroup><thead><tr><th>Region</th><th>TP only</th><th>TP with SP</th></tr></thead><tbody><tr><td>Enter TP (Column Linear)</td><td>h: sharded (weight_out is sharded)</td><td>h: sharded (weight_out is sharded)<br>s: <strong>all-gather</strong> to full</td></tr><tr><td>TP Region</td><td>h: sharded<br>s: full</td><td>h: sharded<br>s: full</td></tr><tr><td>Exit TP (Row Linear)</td><td>h: full (weight_out is full + all-reduce for correctness)<br>s: full</td><td>h: full (weight_out is full + <strong>reduce-scatter</strong> for correctness)<br>s: <strong>reduce-scatter</strong> to sharded</td></tr><tr><td>SP Region</td><td>h: full<br>s: full</td><td>h: full<br>s: sharded</td></tr></tbody></table><p>对于嵌入层：</p><table><colgroup><col style="width:28%"><col style="width:36%"><col style="width:35%"></colgroup><thead><tr><th>Region</th><th>Vanilla TP</th><th>TP with SP</th></tr></thead><tbody><tr><td>Embedding Layer (Row Linear sharded on vocab)</td><td>h: full (weight_out is full + <strong>all-reduce</strong> for correctness)<br>s: full</td><td>h: full (weight_out is full + <strong>reduce-scatter</strong> for correctness)<br>s: <strong>reduce-scatter</strong> to sharded</td></tr></tbody></table><h5 id="b大模型的内存使用量-1">70B大模型的内存使用量</h5><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-30.png"></p><p>如上图，通过TP/SP=16，使得处理16k tokens成为可能。</p><blockquote><p>使用TP+SP是否会比传统的TP引入更多的通信开销？是和否都有可能。</p><p>在传统TP的前向传播过程中，每个Transformer块中有两个all-reduce操作；而在SP中，每个Transformer块中有两个all-gather和两个reduce-scatter操作。因此，SP的通信操作数量是TP的两倍。但由于all-reduce操作可以分解为all-gather + reduce-scatter。因此它们在通信开销上是等效的。同样的推理适用于反向传播，因为我们只需使用每个操作的共轭（no-op ↔︎ all-reduce，all-gather ↔︎ reduce-scatter）。</p></blockquote><p>在每个layer中，讨论了4个通信操作（2个用于Attention，2个用于MLP）。以下为TP+SP时，MLP的性能分析情况： <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-31.png"></p><p>和TP相似，TP+SP难以与计算重叠，这使得吞吐量在很大程度上依赖于通信带宽。在这一点上，和传统TP一样，TP+SP通常只在单个节点内执行（将TP degree保持在每个节点的GPU数量之下，例如TP≤8）。</p><h5 id="tpsp性能">TP+SP性能</h5><p>以下为使用TP+SP扩展时，对于一个3B模型和4096序列长度，吞吐量和内存利用率的变化： <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-32.png"> 再次观察到，计算效率（左图）和内存容量（右图）之间的trade-off。虽然较高的并行度通过减少activations的内存，能够处理<strong>更大的批次大小</strong>；但它们也<strong>降低了每个GPU的吞吐量</strong>，特别是当并行度超过节点内GPU数量的阈值时。</p><p>总结观察结果：</p><ul><li>对于这两种方法，在<strong>从TP=8到TP=16时，性能出现最大降幅</strong>：因为这是我们从仅在单个节点（NVLink）内进行通信，转变为跨节点通信（EFA）的时刻；</li><li>使用TP+SP时，相较于TP，帮助处理<strong>更大的批次</strong>；</li></ul><p>TP通过沿着<strong>隐藏维度</strong>拆分注意力和前馈操作，将activations分布在多个GPU上；而SP沿着<strong>序列维度</strong>拆分剩余的操作，进一步提高了activations的并行程度。</p><blockquote><p>由于SP区域中的LayerNorm操作在不同的序列部分上进行，因此它们的梯度会在不同TP ranks之间有所不同。为了确保权重保持同步，我们需要在反向传播过程中，对它们的梯度进行all-reduce操作，类似于数据并行（DP）确保权重同步。然而，这只是一个小的通信开销，因为LayerNorm的参数相对较少。</p></blockquote><p>然而，TP和SP也有两个限制： 1. 如果扩展序列长度，TP区域的activations内存仍然会膨胀； 2. 如果模型太大，无法适应TP=8，那么由于节点间连接的瓶颈，将出现巨大的性能下降。</p><p>我们可以通过上下文并行（Context Parallelism）来解决问题1），通过流水线并行（Pipeline Parallelism）来解决问题2）。接下来，先来看看上下文并行！</p><h3 id="上下文并行context-parallelism解决长序列的activations爆炸">上下文并行（Context Parallelism）：解决长序列的activations爆炸</h3><p>通过张量并行（Tensor Parallelism）和序列并行（Sequence Parallelism），我们可以显著减少每个GPU的内存需求，因为模型的weights和activations被分布到多个GPU上。然而，当我们训练更长的序列时（例如，当序列长度扩展到128k或更多tokens时），仍然可能超出单个节点的内存容量，因为在TP区域内，我们仍然需要处理完整的序列长度。</p><p>此外，即使我们完全重新计算activations（带来大约30%的计算开销），仍然需要在layer boundaries处保留一些activations，其内存需求也会随着序列长度的增加而线性增长。接下来，看看CP如何帮助我们解决这个问题：</p><p>SP沿着序列维度拆分输入；但现在，将<strong>这种拆分应用到整个模型</strong>，而不是仅仅应用于模型中的SP区域。</p><p><strong>拆分序列</strong>不会影响大多数模块，如MLP和LayerNorm（每个token都是独立处理的）。它也不需要像TP那样的昂贵通信（因为仅拆分输入，而不是权重矩阵）。就像DP一样，在计算梯度后，会<strong>启动一个all-reduce操作，来同步CP组中的梯度</strong>。</p><p>然而，一个重要的例外是：<strong>注意力模块</strong>。在注意力模块中，<strong>每个token需要访问所有其他序列token的键/值对</strong>（即使在causal attention中，也至少需要关注之前的所有token）；因此，注意力模块需要在GPU之间进行完全通信，以交换必要的keys/values。</p><p>这个想法是intuitively expensive的。现在引入一个高效的key/value通信机制：<strong>Ring Attention</strong>。</p><h4 id="ring-attention">Ring attention</h4><p>首先，每个GPU启动一个异步通信操作，将其key/value pair发送至其他GPU；在等待其他GPU数据时，计算已存储在内存中数据的注意力分数。理想情况下，在计算完成之前，从另一个GPU接收到下一个key/value pair，这样GPU就可以在完成第一轮计算后，立即开始下一轮计算。（等待与计算重叠）。</p><p>假设当前有4个GPU和一个4个token的输入。最初，输入序列沿着序列维度均匀拆分，因此每个GPU将只拥有一个token及其对应的Q/K/V值。假设Q1、K1和V1表示第一个token的query、key和value，这些数据位于第一个GPU上。注意力计算需要4个step才能完成。在每个step内，每个GPU执行以下三个连续操作：</p><ol type="1"><li>将当前key/value以非阻塞的方式发送到下一个机器（除了在最后一个step）；</li><li>本地计算注意力分数：运用当前的key/value值： <span class="math display">\[ softmax(\frac{QK^{T}}{\sqrt{d}})*V \]</span></li><li>等待接收来自前一个GPU的key/value；返回第1步，更新当前的key/value为：刚从前一个GPU接收到的key/value。 <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-33.png"></li></ol><p>然而，Ring Attention的简单实现，会导致一个问题：由causal attention矩阵形状引起的<strong>GPU负载不均</strong>。来看看casual attention mask的计算： <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-34.png"> <strong>SoftMax是按行计算</strong>的，这说明：<strong>每当一个GPU接收到某行的所有token时，它就可以开始计算</strong>。可以看到，GPU1可以立即计算它，因为它从token 1到token 4开始，且GPU1实际上不需要从其他GPU接收任何信息；然而，GPU2需要等待第二轮，才能接收到token 1-4，从而拥有token 1-8的所有值。此外，GPU1似乎执行的工作远少于其他所有GPU。</p><p>用什么方式平衡GPU负载呢？</p><h5 id="zig-zag-ring-attention">Zig-Zag Ring Attention</h5><p>我们需要更好的方法来分配输入序列。这可以通过不完全按顺序将token分配给GPU，而是<strong>稍微混合一下顺序，使得每个GPU都有早期和晚期的token</strong>。这个方法被称为Zig-Zag Attention，在这种新的安排中，attention mask将展现更均匀的计算分布，（数一数被标记的方块，发现计算已经在所有GPU之间平衡分配）</p><p>同时看到，为了完成所有行，每个GPU都需要从其他GPU获取信息。</p><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-35.png"></p><p>有两种常见方式来使计算和通信重叠，分别是： 1. 执行一般的all-gather，将每个GPU上的所有同时KV重新收集（类似ZeRO-3）； 2. 按需将每个GPU上的key/value pair，逐个从一个GPU收集到另一个GPU。</p><ul><li>all-gather实现：所有GPU同时收集来自其他所有GPU的完整KV对<ul><li>需要更多的临时内存，因为每个GPU必须同时存储完整的KV对；</li><li>通信一次性完成，但内存开销较大。 <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-37.png"></li></ul></li><li>All-to-All（Ring）实现：GPU以类似环形的模式交换KV对，一次交换一块<ul><li>更节省内存，因为每个GPU只需要临时存储一块额外的KV对；</li><li>通信被分散，与计算重叠 <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-36.png"></li></ul></li></ul><p>TP在跨节点时扩展性不好，如果模型权重无法轻松放入一个节点该怎么办呢？接下来，进入另一种并行方式：Pipeline Parallelism（流水线并行），来解决这个问题！</p><h3 id="流水线并行pipeline-parallelism">流水线并行（Pipeline Parallelism）</h3><p>在TP部分，我们看到，<strong>当尝试将TP扩展到超出单节点内GPU数量（通常为4或8）时，性能会受到一个低带宽网络——“节点间连接”的强烈影响</strong>。可以通过对集群中多个节点进行基准测试清晰地看到这一点（每个节点有8个GPU）： 节点间通信带宽测量，展示了不同节点数下的AllReduce、AllGather和ReduceScatter操作的中位数（线）和5th-95th百分位范围（阴影区域）。 <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-38.png"></p><p>SP和CP可以帮助处理长序列，但如果内存问题的根本原因不是序列长度，而是模型本身的大小，它们的帮助就不大了。<strong>对于大型模型（如70B+），仅仅模型权重的大小就足以超出单节点上4-8个GPU的限制</strong>。我们可以通过引入第四个（也是最后一个）并行维度：“流水线并行性（Pipeline Parallelism）”来解决这个问题。</p><p>PP是一种简单但强大的技术——我们<strong>将模型的层划分到多个GPU上</strong>！例如，如果我们有8个GPU，我们可以将第1-4层放在GPU 1上，将第5-8层放在GPU 2上，依此类推。这样，<strong>每个GPU只需要存储和处理模型的一部分层，显著减少了每个GPU的内存需求</strong>。让我们看看在一个8B模型上的流水线并行性如何影响内存使用情况：</p><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-39.png"></p><p>从上图可知：尽管模型参数在GPU之间得到了很好的分配，但<strong>每个GPU的activations内存仍然保持不变</strong>！（每个GPU仍然需要处理完整的批次数据，只不过它们处理的是不同的层）。<strong>一个GPU的层产生的activations会被传递到下一个GPU</strong>，以继续前向传播。</p><p>这引入了一种新的通信模式：与DP中通过ZeRO-3传递参数不同；现在通过Pipeline，在GPU之间顺序地传递activation tensors。</p><h4 id="在不同节点上拆分层">在不同节点上拆分层</h4><p>假设我们简单地将模型的层分布在多个设备上，例如：第一块GPU处理模型的前几层，第二块GPU处理模型的后几层，依此类推。这样，模型的前向传播过程转为：按顺序将数据批次传递给每个计算设备，从而依次使用每个设备进行计算。</p><blockquote><p>这种做法的直接优势之一是：所需的互联带宽较低，因为只在模型深度的几个位置传递中等大小的activations；（而TP中，通信发生在每一层的多个位置）</p></blockquote><p>然而，PP的主要挑战在于：如何高效地绕过PP的顺序性质，以确保GPU始终保持忙碌状态，即<strong>保持：计算与通信的重叠状态</strong>。 <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-40.png"> 上图中，空闲时间用灰色表示，命名为“bubble”。bubble造成的时间损失是多少呢？</p><blockquote><p>假设<span class="math inline">\(t_f\)</span>, <span class="math inline">\(t_b\)</span>分别是前向和反向传播的时间，针对一个microbatch和管道的一个阶段进行测量（（一个简单的假设是：<span class="math inline">\(t_b=2*t_f\)</span>），如果实现完美的并行化，理想时间为：<span class="math inline">\(t_{id}=t_b+t_f\)</span>.</p><p>但是，考虑到bubble的存在，额外的时间为： <span class="math display">\[ t_pb=(p-1)\times (t_b+t_f) \]</span> 其中，<span class="math inline">\(p\)</span>是pipeline并行度（GPU数量）。那么bubble时间和理想时间的比率为：<span class="math inline">\(p-1\)</span>。即：随着更多GPU的加入，bubble时间增加，GPU的时间利用率下降。</p></blockquote><p>有哪些减少bubble的方法呢？先看看第一个：all-forward-all-backward (AFAB) 调度。</p><h4 id="all-forward-all-backward-afab-调度改善activations的内存占用">all-forward-all-backward (AFAB) 调度：改善activations的内存占用</h4><p>将批次分成更小的部分，这些部分可以并行或几乎并行地处理（就像在DP中做的那样）。现在，当第二个GPU忙于处理microbatch 1时，第一个GPU可以开始处理microbatch 1。以下是使用8个microbatch的调度示例： <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-41.png"></p><p>首先执行所有的前向传播，然后仅执行所有的反向传播。其优点是：<strong>前向和反向步骤仍然是一般性的顺序操作</strong>，因此保留了模型训练代码的一般组织方式。AFAB是PP的最简单的实现之一。</p><blockquote><p>处理<span class="math inline">\(m\)</span>个microbatch的理想时间为：<span class="math inline">\(t_{id}=m\times(t_f+t_b)\)</span>；</p><p>bubble时间比率为：<span class="math inline">\(r_{bubble}=\frac{(p-1)\times(t_f+t_b)}{m\times(t_f+t_b)}=\frac{p-1}{m}.\)</span></p><p>通过增加更多的microbatch，可以减少bubble的大小，将其缩小<span class="math inline">\(m\)</span>倍。</p></blockquote><p>尽管bubble令人烦恼，但还有一个更大的问题：当前需要将所有的activations存储在内存中，直至到达反向传播阶段，这会导致在PP中快速出现内存爆炸。那么，我们能否做得更好？</p><h4 id="one-forward-one-backward1f1b调度-和-llama-3.1-schemes">One-forward-one-backward（1F1B）调度 和 LLama 3.1 schemes</h4><p>1F1B的中间稳定状态为：<strong>交替执行一次前向传播和一次反向传播</strong>。其理念是尽早开始执行反向传播。调度如下图： <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-42.png"> bubble的大小相同，因此训练效率没有显著提高。然而，<strong>只需要存储<span class="math inline">\(p\)</span>个microbatch的activations（<span class="math inline">\(p\)</span>为pipeline并行度）</strong>，无需存储<span class="math inline">\(m\)</span>个microbatch的activations。从而减小AFAB调度中的内存爆炸压力。 <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-43.png"> 在左图中：</p><ul><li><span class="math inline">\(m\leq p-1\)</span>时，bubble的存在导致性能较低（即使扩展<span class="math inline">\(p\)</span>，性能也下降）；</li><li><span class="math inline">\(m=32&gt;&gt;p-1\)</span>时，可以改善低pipeline并行度下的性能。</li></ul><p>实际上，由于最终受限于global batch size的影响，不能无限地增加microbatch的数量，以保持<span class="math inline">\(m&gt;&gt;p-1\)</span>的比例。</p><blockquote><p>可以观察到，在左图中（microbatch数量较少时），从一个节点（<span class="math inline">\(p=8\)</span>）扩展到两个节点（<span class="math inline">\(p=16\)</span>）时，性能仅下降14%；TP在类似跨节点场景下，性能下降约43%）。因此PP非常适合分布式训练。</p></blockquote><p>1F1B改善了activations的内存使用；但是由<span class="math inline">\(r_{bubble}=\frac{p-1}{m}\)</span>可知，bubble大小与<span class="math inline">\(p\)</span>成比例，GPU计算依然处于空闲状态，是否有更智能的调度策略呢？</p><h4 id="交错阶段interleaving-stages改善bubble大小">交错阶段（Interleaving Stages）：改善bubble大小</h4><p>到目前为止，我们是通过沿模型深度维度简单地切分模型，例如将第 1-4 层放在第一个 GPU 上，将第 5-8 层放在第二个 GPU 上。但其实还有其他方式可以切分我们的层，例如将奇数层（1、3、5、7）放在第一个 GPU 上，将偶数层（2、4、6、8）放在第二个 GPU 上。</p><p>这可以看作是一种“循环管道”的方式，其中microbatch将从一个 GPU 移动到下一个 GPU，在模型的前向传播过程中不断循环。 <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-44.png"></p><p>随着模型在每个 GPU 上多次经过，出现了额外的通信操作，这是因为之前只需要计算只需传递一次，现在需要传递多次。每个前向和反向传播过程被切分为<span class="math inline">\(v\)</span>个部分（<span class="math inline">\(v\)</span>是每个 GPU 上的stages或model chunks的数量）。则有： <span class="math display">\[ t_{pb}=\frac{(p-1)\times(t_f+t_b)}{v} \\ r_{bubble}=\frac{1}{v}\frac{(p-1)\times(t_f+t_b)}{m\times(t_f+t_b)}=\frac{p-1}{v\times m} \]</span> 现在，我们通过增加microbatch数量和交错阶段数量，来减小bubble大小（当然，通信量也会增加一个因子<span class="math inline">\(v\)</span>）。 下图为<span class="math inline">\(p=8\)</span>时的bubble大小。其中：</p><ul><li><span class="math inline">\(m=1, v=1\)</span>：普通的流水线并行；</li><li><span class="math inline">\(v=1\)</span>：AFAB或1F1B；</li><li><span class="math inline">\(v\neq 1\)</span>：交错阶段。 <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-45.png"></li></ul><p>GPU调度是一个值得深入探讨的问题。有两种方式：</p><ol type="1"><li><strong>深度优先</strong>：优先让较早的micro-batches通过更靠后的layers：尽快关闭前向和反向循环，即尽快让batches从模型中输出；</li><li><strong>深度优先</strong>：优先让较后的micro-batches通过更靠前的layers：尽可能填满pipeline。</li></ol><blockquote><p>Llama3.1中的PP策略：1F1B+交错阶段，depth-first和bread-first可选。 <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-46.png"></p></blockquote><p>然而，PP的策略优化依然进行中，在DeepSeek V3/R1中，提出了一种方法，将bubble大小降低至几乎为零！</p><h4 id="zero-bubble和双管道dualpipe">Zero Bubble和双管道（DualPipe）</h4><p>核心理念是：将涉及的操作划分得更精细，以最有效的方式进行交错。</p><p>Zero Bubble是DualPipe的前身。ZeroBubble 的基本观察是，矩阵乘法的反向传播，涉及两个独立的操作：输入的反向操作（B）和权重的反向操作（W）：</p><p>输入的反向传播，是进行执行更低层反向传播的必要条件；然而，权重的反向传播则不是（只要在优化步骤之前执行即可），如下图所示： <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-47.png"></p><p>因此，权重的反向操作（W），可安排在对应B之后的任何位置；这<strong>允许策略性地安排W，以填补pipeline中的buble</strong>。</p><blockquote><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-48.png"> * 1F1B调度：交替进行前向/反向传播，但反向传播为较粗粒度； * ZeroBubble的两个变种：<strong>将反向传播拆分为：B和W的细粒度操作</strong>（最后一个称为ZB-H2，是一个理论上的Zero Bubble调度）</p><p>DeepSeek 的 DualPipe 在其 V3 技术报告中，引入了这一分解方法的扩展，增加了<strong>两个流从 PP 维度的两端传播</strong>的情况，这些流被交替安排，以进一步最小化 GPU 的空闲时间。如下图： <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-49.png"></p></blockquote><p>最后来到专家并行（Expert Parallelism），其提供高效训练大模型的并行策略。</p><h3 id="专家并行expert-parallelism">专家并行（Expert Parallelism）</h3><p><a target="_blank" rel="noopener" href="https://huggingface.co/blog/moe">MoE框架介绍</a></p><p>MoE由两个关键部分组成：</p><ol type="1"><li>稀疏MoE层：替代Transformer中的前馈网络（FFN）层，包含若干个expert，每个expert本身是一个独立的神经网络。</li><li>gate network和router：决定哪个tokens发送到哪个expert（可以将一个token发送给多个expert）。roter由学习得到的参数组成，与网络的其他部分一同预训练。 <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-50.png"> &gt; MoE特点：（与稠密模型相比） &gt; * 训练：预训练速度更快；但在微调阶段常面临泛化能力不足，易引发过拟合； &gt; * 推理：由于只使用一部分参数，MoE推理速度快于相同参数量的稠密模型；但需要将所有expert加载至内存，对显存要求高。</li></ol><p>EP的理念是：运用MoE框架，实现expert维度上的并行。由于前馈层完全独立，可以<strong>将每个expert的前馈层，放在不同的worker上</strong>。（比TP更轻量，因为无需拆分矩阵乘法，只需将token的隐藏层通过router导引至相应expert）</p><p>现实中，EP通常与其他并行策略一起使用（例如DP），因为<strong>EP只影响MoE层，不拆分tokens</strong>（CP会沿着序列维度拆分tokens）。 <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-51.png"></p><h2 id="d并行">5D并行</h2><p>当前，已经学习了扩展模型训练的5种并行策略：</p><ol type="1"><li>数据并行（DP）：沿着batch维度</li><li>张量并行（TP）：沿着隐藏层维度</li><li>序列/上下文并行（SP/CP）：沿着sequence维度</li><li>流水线并行（PP）：沿着model layers维度</li><li>专家并行（EP）：沿着MoE experts维度</li></ol><p>以及三种可以与DP结合使用的 ZeRO 策略，用于节省内存： * ZeRO-1 – 在 DP 副本之间，对optimizer states分片 * ZeRO-2 – 在 DP 副本之间，对optimizer states, gradients分片 * ZeRO-3 – 在 DP 副本之间，对optimizer states, gradients, parameters分片</p><p>我们应该如何高效地组合这些策略，哪些策略应该分开使用？</p><h3 id="pp-vs.-zero-3">PP vs. ZeRO-3</h3><p>PP和ZeRO-3都是将模型weights划分到多个GPU上，沿着模型深度轴，执行计算/通信（例如在 ZeRO-3 中，在计算时预取下一层）。这说明：<strong>完整的层操作都在每个设备上进行</strong>（而不像 TP 或 EP ，在子层单元上执行计算）。</p><p>PP和ZeRO-3的区别如下：</p><table><colgroup><col style="width:31%"><col style="width:34%"><col style="width:33%"></colgroup><thead><tr><th><strong>Aspect</strong></th><th><strong>ZeRO-3</strong></th><th><strong>Pipeline Parallelism (PP)</strong></th></tr></thead><tbody><tr><td><strong>每个计算单元存储</strong></td><td>layer的一部分</td><td>整个layer</td></tr><tr><td><strong>使用通信传输的内容</strong></td><td>权重</td><td>激活值</td></tr><tr><td><strong>协调</strong></td><td>与模型无关</td><td>与模型无关</td></tr><tr><td><strong>实现挑战</strong></td><td>复杂的模型分区和通信处理</td><td>复杂的调度策略</td></tr><tr><td><strong>扩展性</strong></td><td>偏好大批量和长序列，以隐藏通信</td><td>偏好较大<code>grad_acc</code>以隐藏bubble</td></tr></tbody></table><p>ZeRO-3 和 PP 解决的是相同的挑战，但涉及不同的方法，其结合使用在实践中并不常见；ZeRO-1 和 ZeRO-2 主要关注优化器状态和梯度，很容易地与PP结合。</p><h3 id="tpsp">TP（+SP）</h3><p>TP与SP是天然互补的，可以与PP和ZeRO-3结合使用。因为它依赖于矩阵乘法的分配性质，这使得<strong>weights和activations可以被分割并独立计算，之后再进行合并</strong>。 <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-52.png"> 单独使用TP的两个限制：</p><ol type="1"><li><strong>通信开销</strong>：由于TP的通信操作是计算关键路径的一部分，因此它在某个点之后难以很好地扩展，此时通信开销开始占主导地位。</li><li>模型特定的分割要求： 与ZeRO和PP不同，TP需要确定activations的切分策略—有时是在隐藏维度（TP区域），有时是在序列维度（SP区域）—这使得实现变得更加繁琐。</li></ol><p>因此，TP通常用于<strong>节点内部通信</strong>；ZeRO-3或PP用于<strong>跨节点并行</strong>（需要更少的带宽（对于PP），或更容易与计算重叠（对于ZeRO-3））。</p><h3 id="cpep">CP/EP</h3><p>CP和EP均有利于activations的切片，可视作TP的互补。CP解决了长序列的训练问题；EP支持分布式MoE训练。</p><p>CP通过沿着序列维度切分activations，并使之分布在不同GPU上，解决长序列的训练问题。虽然大多数操作（如MLP和LayerNorm）可以独立处理这些分割的序列；但<strong>注意力层则需要通信，因为每个token都需要访问来自整个序列的keys/values</strong>（通过<strong>Ring Attention</strong>处理，使得计算和通信重叠）。 &gt; CP在扩展到极端序列长度（128k+ tokens）时尤为价值：此时即使使用完全的activations重计算，单个GPU的内存需求也难以承受。</p><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-53.png"></p><p>EP专门解决训练MoE的挑战：<strong>将experts分布在多个GPU上；在计算期间，动态地将tokens导向相关的expert</strong>。关键通信操作是：all-to-all操作，即将token通过router导向分配的expert，并将结果收集回来。尽管当前操作引入通信开销，但它使得模型容量显著扩展：因为<strong>每个token在推理（和训练）过程中，只由总参数的一个较小部分处理</strong>。 <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-54.png"> &gt; EP和DP在输入处理上具备相似性：因此通常将EP视为DP的一个子方法；区别在于：EP使用专门的expert routing处理输入；而DP让所有GPU通过相同的模型副本处理输入。</p><h3 id="总结">总结</h3><p>总结以上并行策略对模型各个子部分的影响：</p><ul><li>TP（+SP）：通过切分weights和activations，影响<strong>整个模型</strong>的计算；</li><li>CP：主要影响<strong>注意力层</strong>（需要跨序列通信），其他层可以独立处理被分割序列；</li><li>EP：主要影响<strong>MoE层</strong>（替代了标准的MLP块），其他注意力层和组件保持不变；</li><li>PP和ZeRO：不特别针对任何子模块或组件（唯一的例外是：PP中modules和layers需要平衡，因此第一层和最后一层通常会因为附加的嵌入层而被特别处理）</li></ul><table><colgroup><col style="width:43%"><col style="width:28%"><col style="width:27%"></colgroup><thead><tr><th><strong>TP+SP</strong></th><th><strong>CP</strong></th><th><strong>EP</strong></th></tr></thead><tbody><tr><td>在hidden/seq维度上分割weights和activations</td><td>在seq维度上分割activations</td><td>分割expert的weights和activations</td></tr><tr><td>用于矩阵乘法操作（column/row linears）的通信</td><td>用于注意力key/values的通信</td><td>用于token路由到expert的通信</td></tr><tr><td>针对模型特定实现</td><td>除注意力外，与模型无关</td><td>除MoE层外，与模型无关</td></tr><tr><td>偏好高带宽的节点内通信</td><td>偏好长序列长度</td><td>需要MoE模型</td></tr></tbody></table><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-55.png"></p><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ca7e7de/image-56.png"></p><table><colgroup><col style="width:11%"><col style="width:40%"><col style="width:30%"><col style="width:17%"></colgroup><thead><tr><th><strong>方法</strong></th><th><strong>内存节省</strong></th><th><strong>并行/切分维度</strong></th><th><strong>缺点</strong></th></tr></thead><tbody><tr><td>DP</td><td>Activations (降低local batch size)</td><td>Batch</td><td>受限于max batch size</td></tr><tr><td>PP</td><td>Model parameters</td><td>Model layers</td><td>bubble和复杂调度</td></tr><tr><td>TP/SP</td><td>Model parameters和activations</td><td>Hidden dimension / Sequence length</td><td>高带宽通信</td></tr><tr><td>CP</td><td>Activations</td><td>Sequence length</td><td>注意力模块增添额外通信</td></tr><tr><td>EP</td><td>Experts parameters</td><td>Expert dimension</td><td>需要MoE layers, 增添额外routing通信</td></tr><tr><td>ZeRO-1</td><td>Optimizer states</td><td>在DP副本之间切分</td><td>额外的参数通信</td></tr><tr><td>ZeRO-2</td><td>Optimizer states and gradients</td><td>在DP副本之间切分</td><td>额外的参数通信</td></tr><tr><td>ZeRO-3</td><td>Optimizer states, gradients, and model parameters</td><td>在DP副本之间切分</td><td>额外的参数通信</td></tr></tbody></table></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://wenliuyi.github.io">Liuyi Wen</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://wenliuyi.github.io/posts/ca7e7de.html">http://wenliuyi.github.io/posts/ca7e7de.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://wenliuyi.github.io" target="_blank">Liuyi Wen's Blog</a>！</span></div></div><div class="tag_share"><div class="post-share"><div class="social-share" data-image="/img/WechatIMG105.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/posts/44ad5109.html" title="C++泛型算法"><div class="cover" style="background:var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">C++泛型算法</div></div><div class="info-2"><div class="info-item-1">本篇隶属C++ Primer中C++标准库专题，当前关注范型算法。 标准库容器只定义了很少的操作；因此提供了一组范型算法，其中大多数独立于特定的容器，具备通用性。 范型算法 概述 大多数算法在头文件algorithm中；头文件numeric中定义了一组数值范型算法。 范型算法本身不会执行容器的操作；只会运行于迭代器之上，执行迭代器的操作。 只读算法 只读取输入范围的元素，从不改变元素 find 12int val=42;auto result=find(vec.cbegin(), vec.cend(), val); find前两个参数：表示元素范围的迭代器；第三个参数：一个值。将范围中每个元素与给定值比较： 返回指向第一个等于给定值元素的迭代器； 若范围内无匹配元素，返回第二个参数，表示搜索失败。 accumulate 1int sum=accumulate(vec.cbegin(), vec.cend(), 0);	// 将sum设置为：vec中元素之和 第三个参数指定保存和的对象类型，在该类型上必须定义了"+"运算符 12// 错误：const...</div></div></div></a><a class="pagination-related" href="/posts/a9bcb957.html" title="BUAA-OS Lab3 实验笔记"><div class="cover" style="background:var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">BUAA-OS Lab3 实验笔记</div></div><div class="info-2"><div class="info-item-1">...</div></div></div></a></nav><hr class="custom-hr"><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/img/WechatIMG105.jpg" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info-name">Liuyi Wen</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">43</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">5</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/WenLiuyi"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">The Journey Is the Reward.</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9C%A8%E5%8D%95%E4%B8%AAgpu%E4%B8%8A%E8%AE%AD%E7%BB%83"><span class="toc-number">1.</span> <span class="toc-text">在单个GPU上训练</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#batch-size%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">1.1.</span> <span class="toc-text">batch size的影响</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#transformer%E4%B8%8A%E7%9A%84%E5%86%85%E5%AD%98%E4%BD%BF%E7%94%A8"><span class="toc-number">1.2.</span> <span class="toc-text">Transformer上的内存使用</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#weightsgradsoptimizer-state%E7%9A%84%E5%86%85%E5%AD%98"><span class="toc-number">1.2.1.</span> <span class="toc-text">weights&#x2F;grads&#x2F;optimizer state的内存</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#activations%E7%9A%84%E5%86%85%E5%AD%98"><span class="toc-number">1.2.2.</span> <span class="toc-text">activations的内存</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%A7%E5%88%B6activation%E5%A2%9E%E9%95%BF%E7%9A%84%E7%AD%96%E7%95%A5"><span class="toc-number">1.3.</span> <span class="toc-text">控制activation增长的策略</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#activation%E9%87%8D%E8%AE%A1%E7%AE%97gradient-checkpoints"><span class="toc-number">1.3.1.</span> <span class="toc-text">activation重计算（gradient checkpoints）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E7%B4%AF%E7%A7%AFgradient-accumulation"><span class="toc-number">1.3.2.</span> <span class="toc-text">梯度累积（gradient accumulation）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5"><span class="toc-number">2.</span> <span class="toc-text">并行策略</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8Cdata-parallelism"><span class="toc-number">2.1.</span> <span class="toc-text">数据并行（Data Parallelism）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5"><span class="toc-number">2.1.1.</span> <span class="toc-text">优化策略</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E4%B8%80%E6%A2%AF%E5%BA%A6%E5%90%8C%E6%AD%A5%E9%80%9A%E4%BF%A1%E4%B8%8E%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%A1%E7%AE%97%E5%B9%B6%E8%A1%8C"><span class="toc-number">2.1.1.1.</span> <span class="toc-text">优化一：梯度同步（通信）与反向传播（计算）并行</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E4%BA%8C%E6%A2%AF%E5%BA%A6%E5%88%86%E6%A1%B6"><span class="toc-number">2.1.1.2.</span> <span class="toc-text">优化二：梯度分桶</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E4%B8%89%E9%85%8D%E5%90%88%E6%A2%AF%E5%BA%A6%E7%B4%AF%E7%A7%AF"><span class="toc-number">2.1.1.3.</span> <span class="toc-text">优化三：配合梯度累积</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#dp%E6%AD%A5%E9%AA%A4"><span class="toc-number">2.1.2.</span> <span class="toc-text">DP步骤</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#deepspeed-zero%E9%9B%B6%E5%86%97%E4%BD%99%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">2.1.3.</span> <span class="toc-text">DeepSpeed ZeRO（零冗余优化器）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#zero-1zero-2zero-3%E9%80%9A%E4%BF%A1%E9%87%8F%E5%88%86%E6%9E%90"><span class="toc-number">2.1.3.1.</span> <span class="toc-text">ZeRO-1，ZeRO-2，ZeRO-3通信量分析</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#zero-r"><span class="toc-number">2.1.3.2.</span> <span class="toc-text">ZeRO-R</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#zero-offload"><span class="toc-number">2.1.3.3.</span> <span class="toc-text">ZeRO-Offload</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E5%B9%B6%E8%A1%8Ctensor-parallelism"><span class="toc-number">2.2.</span> <span class="toc-text">张量并行（Tensor Parallelism）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86"><span class="toc-number">2.2.1.</span> <span class="toc-text">数学原理</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#column-linear"><span class="toc-number">2.2.1.1.</span> <span class="toc-text">column-linear</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#row-linear"><span class="toc-number">2.2.1.2.</span> <span class="toc-text">row-linear</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#transformer-block%E5%86%85%E9%83%A8%E7%9A%84%E5%BC%A0%E9%87%8F%E5%B9%B6%E8%A1%8C"><span class="toc-number">2.2.2.</span> <span class="toc-text">Transformer Block内部的张量并行</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%89%8D%E5%90%91%E5%8F%8D%E9%A6%88%E5%B1%82mlp"><span class="toc-number">2.2.2.1.</span> <span class="toc-text">前向反馈层（MLP）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%B1%82mha"><span class="toc-number">2.2.2.2.</span> <span class="toc-text">多头注意力层（MHA）</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#tp%E6%80%A7%E8%83%BD"><span class="toc-number">2.2.3.</span> <span class="toc-text">TP性能</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#b%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%86%85%E5%AD%98%E4%BD%BF%E7%94%A8%E9%87%8F"><span class="toc-number">2.2.3.1.</span> <span class="toc-text">70B大模型的内存使用量</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80%E4%B8%AA%E5%B0%8F%E6%89%A9%E5%B1%95%E5%BA%8F%E5%88%97%E5%B9%B6%E8%A1%8Csequence-parallelism"><span class="toc-number">2.2.4.</span> <span class="toc-text">一个小扩展：序列并行（Sequence Parallelism）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#b%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%86%85%E5%AD%98%E4%BD%BF%E7%94%A8%E9%87%8F-1"><span class="toc-number">2.2.4.1.</span> <span class="toc-text">70B大模型的内存使用量</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#tpsp%E6%80%A7%E8%83%BD"><span class="toc-number">2.2.4.2.</span> <span class="toc-text">TP+SP性能</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B9%B6%E8%A1%8Ccontext-parallelism%E8%A7%A3%E5%86%B3%E9%95%BF%E5%BA%8F%E5%88%97%E7%9A%84activations%E7%88%86%E7%82%B8"><span class="toc-number">2.3.</span> <span class="toc-text">上下文并行（Context Parallelism）：解决长序列的activations爆炸</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#ring-attention"><span class="toc-number">2.3.1.</span> <span class="toc-text">Ring attention</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#zig-zag-ring-attention"><span class="toc-number">2.3.1.1.</span> <span class="toc-text">Zig-Zag Ring Attention</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B5%81%E6%B0%B4%E7%BA%BF%E5%B9%B6%E8%A1%8Cpipeline-parallelism"><span class="toc-number">2.4.</span> <span class="toc-text">流水线并行（Pipeline Parallelism）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9C%A8%E4%B8%8D%E5%90%8C%E8%8A%82%E7%82%B9%E4%B8%8A%E6%8B%86%E5%88%86%E5%B1%82"><span class="toc-number">2.4.1.</span> <span class="toc-text">在不同节点上拆分层</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#all-forward-all-backward-afab-%E8%B0%83%E5%BA%A6%E6%94%B9%E5%96%84activations%E7%9A%84%E5%86%85%E5%AD%98%E5%8D%A0%E7%94%A8"><span class="toc-number">2.4.2.</span> <span class="toc-text">all-forward-all-backward (AFAB) 调度：改善activations的内存占用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#one-forward-one-backward1f1b%E8%B0%83%E5%BA%A6-%E5%92%8C-llama-3.1-schemes"><span class="toc-number">2.4.3.</span> <span class="toc-text">One-forward-one-backward（1F1B）调度 和 LLama 3.1 schemes</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%A4%E9%94%99%E9%98%B6%E6%AE%B5interleaving-stages%E6%94%B9%E5%96%84bubble%E5%A4%A7%E5%B0%8F"><span class="toc-number">2.4.4.</span> <span class="toc-text">交错阶段（Interleaving Stages）：改善bubble大小</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#zero-bubble%E5%92%8C%E5%8F%8C%E7%AE%A1%E9%81%93dualpipe"><span class="toc-number">2.4.5.</span> <span class="toc-text">Zero Bubble和双管道（DualPipe）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%93%E5%AE%B6%E5%B9%B6%E8%A1%8Cexpert-parallelism"><span class="toc-number">2.5.</span> <span class="toc-text">专家并行（Expert Parallelism）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#d%E5%B9%B6%E8%A1%8C"><span class="toc-number">3.</span> <span class="toc-text">5D并行</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#pp-vs.-zero-3"><span class="toc-number">3.1.</span> <span class="toc-text">PP vs. ZeRO-3</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tpsp"><span class="toc-number">3.2.</span> <span class="toc-text">TP（+SP）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cpep"><span class="toc-number">3.3.</span> <span class="toc-text">CP&#x2F;EP</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">3.4.</span> <span class="toc-text">总结</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/2d0d7608.html" title="Transformer 系列：3. Encoder 和 Decoder 的架构">Transformer 系列：3. Encoder 和 Decoder 的架构</a><time datetime="2025-09-29T09:08:01.000Z" title="发表于 2025-09-29 17:08:01">2025-09-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/d4ce6176.html" title="幂等性设计">幂等性设计</a><time datetime="2025-09-28T11:21:49.000Z" title="发表于 2025-09-28 19:21:49">2025-09-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/7824989e.html" title="从 TCP 粘包到分帧">从 TCP 粘包到分帧</a><time datetime="2025-09-26T10:04:55.000Z" title="发表于 2025-09-26 18:04:55">2025-09-26</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/eecc19a2.html" title="并行训练系列：5. Megatron 之分布式环境初始化">并行训练系列：5. Megatron 之分布式环境初始化</a><time datetime="2025-09-25T09:41:56.000Z" title="发表于 2025-09-25 17:41:56">2025-09-25</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/d7906c2a.html" title="并行训练系列：4. 张量并行（TP）">并行训练系列：4. 张量并行（TP）</a><time datetime="2025-09-18T15:53:50.000Z" title="发表于 2025-09-18 23:53:50">2025-09-18</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By Liuyi Wen</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(()=>{const t=()=>{if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"all"},chtml:{scale:1.1},options:{enableMenu:!0,renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const n=!!e.type.match(/; *mode=display/),a=new t.options.MathItem(e.textContent,t.inputJax[0],n),d=document.createTextNode("");e.parentNode.replaceChild(d,e),a.start={node:d,delim:"",n:0},a.end={node:d,delim:"",n:0},t.math.push(a)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}};btf.addGlobalFn("encrypt",t,"mathjax"),window.pjax?t():window.addEventListener("load",t)})()</script><script>(()=>{const n="shuoshuo"===GLOBAL_CONFIG_SITE.pageType,e=(e,o)=>{n&&(window.shuoshuoComment.destroyValine=()=>{e.children.length&&(e.innerHTML="",e.classList.add("no-comment"))});const t={el:"#vcomment",appId:"bsxtUJWr1muoPS1pmoXLOPZ2-gzGzoHsz",appKey:"wm2wUYvKLEySwyRnFn7xAbJI",avatar:"monsterid",serverURLs:"",emojiMaps:"",visitor:!1,path:n?o:window.location.pathname};new Valine(t)},o=async(n,o)=>{"function"==typeof Valine||await btf.getScript("https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"),e(n,o)};n?window.shuoshuoComment={loadComment:o}:btf.loadComment(document.getElementById("vcomment"),o)})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-info"><div class="algolia-stats"></div><div class="algolia-poweredBy"></div></div></div></div></div><div id="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/algoliasearch/dist/lite/builds/browser.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instantsearch.js/dist/instantsearch.production.min.js"></script><script src="/js/search/algolia.js"></script></div></div>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body></html>