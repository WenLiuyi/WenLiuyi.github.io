<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>verl框架：2. 对比OpenRLHF+colocate思路解析 | Liuyi Wen's Blog</title><meta name="author" content="Liuyi Wen"><meta name="copyright" content="Liuyi Wen"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="SPMD-&gt;MPMD SPMD设计范式：单程序多数据，所有进程&#x2F;线程执行同一个程序的拷贝，通过环境变量差异自主确定行为模式，无需中心调度节点。主流并行框架（DDP&#x2F;DeepSpeed&#x2F;Megatron）均基于SPMD范式。 优点：SPMD由于没有controller，完全由worker自驱，在运行时更为高效； 缺点：由于各个worker上需要运行相同程序，灵活性不如single-contro"><meta property="og:type" content="article"><meta property="og:title" content="verl框架：2. 对比OpenRLHF+colocate思路解析"><meta property="og:url" content="http://wenliuyi.github.io/posts/2ac460b1.html"><meta property="og:site_name" content="Liuyi Wen&#39;s Blog"><meta property="og:description" content="SPMD-&gt;MPMD SPMD设计范式：单程序多数据，所有进程&#x2F;线程执行同一个程序的拷贝，通过环境变量差异自主确定行为模式，无需中心调度节点。主流并行框架（DDP&#x2F;DeepSpeed&#x2F;Megatron）均基于SPMD范式。 优点：SPMD由于没有controller，完全由worker自驱，在运行时更为高效； 缺点：由于各个worker上需要运行相同程序，灵活性不如single-contro"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://wenliuyi.github.io/img/WechatIMG105.jpg"><meta property="article:published_time" content="2025-05-06T01:49:06.000Z"><meta property="article:modified_time" content="2025-09-08T09:28:42.440Z"><meta property="article:author" content="Liuyi Wen"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="http://wenliuyi.github.io/img/WechatIMG105.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "verl框架：2. 对比OpenRLHF+colocate思路解析",
  "url": "http://wenliuyi.github.io/posts/2ac460b1.html",
  "image": "http://wenliuyi.github.io/img/WechatIMG105.jpg",
  "datePublished": "2025-05-06T01:49:06.000Z",
  "dateModified": "2025-09-08T09:28:42.440Z",
  "author": [
    {
      "@type": "Person",
      "name": "Liuyi Wen",
      "url": "http://wenliuyi.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://wenliuyi.github.io/posts/2ac460b1.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>(()=>{const e={set:(e,t,o)=>{if(!o)return;const a=Date.now()+864e5*o;localStorage.setItem(e,JSON.stringify({value:t,expiry:a}))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const{value:o,expiry:a}=JSON.parse(t);if(!(Date.now()>a))return o;localStorage.removeItem(e)}};window.btf={saveToLocal:e,getScript:(e,t={})=>new Promise((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,Object.entries(t).forEach(([e,t])=>n.setAttribute(e,t)),n.onload=n.onreadystatechange=()=>{n.readyState&&!/loaded|complete/.test(n.readyState)||o()},n.onerror=a,document.head.appendChild(n)}),getCSS:(e,t)=>new Promise((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onload=n.onreadystatechange=()=>{n.readyState&&!/loaded|complete/.test(n.readyState)||o()},n.onerror=a,document.head.appendChild(n)}),addGlobalFn:(e,t,o=!1,a=window)=>{if(e.startsWith("pjax"))return;const n=a.globalFn||{};n[e]=n[e]||{},n[e][o||Object.keys(n[e]).length]=t,a.globalFn=n}};const t=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},o=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};btf.activateDarkMode=t,btf.activateLightMode=o;const a=e.get("theme");"dark"===a?t():"light"===a&&o();const n=e.get("aside-status");void 0!==n&&document.documentElement.classList.toggle("hide-aside","hide"===n);/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})()</script><script>const GLOBAL_CONFIG={root:"/",algolia:{appId:"VE36MEFVE6",apiKey:"f9b9ca5a3cdb9455658600dba6ae7706",indexName:"hexo-algolia indexing key",hitsPerPage:6,languages:{input_placeholder:"搜索文章",hits_empty:"未找到符合您查询的内容：${query}",hits_stats:"找到 ${hits} 条结果，耗时 ${time} 毫秒"}},localSearch:void 0,translate:void 0,highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:!1,highlightFullpage:!1,highlightMacStyle:!1},copy:{success:"复制成功",error:"复制失败",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"null",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!1,islazyloadPlugin:!1,isAnchor:!1,percent:{toc:!0,rightside:!1},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"verl框架：2. 对比OpenRLHF+colocate思路解析",isHighlightShrink:!1,isToc:!0,pageType:"post"}</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Liuyi Wen's Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">verl框架：2. 对比OpenRLHF+colocate思路解析</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div></div></nav><div id="post-info"><h1 class="post-title">verl框架：2. 对比OpenRLHF+colocate思路解析</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-05-06T01:49:06.000Z" title="发表于 2025-05-06 09:49:06">2025-05-06</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-09-08T09:28:42.440Z" title="更新于 2025-09-08 17:28:42">2025-09-08</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/verl/">verl</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h2 id="spmd-mpmd">SPMD-&gt;MPMD</h2><p>SPMD设计范式：单程序多数据，<strong>所有进程/线程执行同一个程序的拷贝</strong>，通过环境变量差异自主确定行为模式，<strong>无需中心调度节点</strong>。主流并行框架（DDP/DeepSpeed/Megatron）均基于SPMD范式。</p><p>优点：SPMD由于没有controller，完全由worker自驱，在运行时更为高效；</p><p>缺点：由于各个worker上需要运行相同程序，灵活性不如single-controller模式；需要考虑各个rank之间的通信，增加编程复杂度。</p><p>经典代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(os.environ[<span class="string">&#x27;RANK&#x27;</span>], os.environ[<span class="string">&#x27;WORLD_SIZE&#x27;</span>], os.environ[<span class="string">&#x27;MASTER_ADDR&#x27;</span>], os.environ[<span class="string">&#x27;MASTER_PORT&#x27;</span>])</span><br><span class="line">torch.distributed.init_process_group(backend=<span class="string">&quot;nccl&quot;</span>)</span><br><span class="line">torch.cuda.set_device(torch.distributed.get_rank())</span><br></pre></td></tr></table></figure><ol type="1"><li><strong><code>torchrun</code>执行以上脚本，启动多个进程</strong>；每个进程不同的环境变量，标识其所属的机器号和端口号，以及进程号和进程总数。</li><li><strong><code>torch.distributed.init_process_group</code>根据环境变量构建通信组</strong>（一个阻塞操作，所有进程必须完成后才开始执行）</li><li><code>set_device</code>将当前进程绑定在一块GPU上。</li></ol><p>OpenRLHF SPMD ppo的系统架构如下：<strong>PPOTrainer负责整个PPO算法的控制逻辑</strong>。此时，<strong>不同的模型在同一组卡和同一组进程上，按照不同的时间片运行SPMD</strong>。这些共享同一组计算资源并按时间交替使用的模型被称为<strong>colocate models</strong>。</p><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/2ac460b1/image-18.png"></p><p>然而，SPMD要求不同的模型串行执行，即使没有数据依赖的模型也难以实现并发。如果模型不需要占用全部计算卡，就会导致部分计算资源的闲置；此外，SPMD需要将多个模型的参数同时加载到一张计算卡上，如果不结合offload等技术，很容易引发显存OOM问题。</p><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/2ac460b1/image-19.png"></p><p>那么如何实现这一点呢？OpenRLHF的方案是：<strong>使用ray拉起</strong>。在 Ray 的抽象下，各个模块都可以看成是独立的 multi-process training / generate，通过配置不同的placement group，从而使模块绑定到不同的卡上；模块之间的交互通过 Object Store 和 Object Ref 做数据收发来实现。</p><h2 id="openrlhf的ray流程">OpenRLHF的Ray流程</h2><p>OpenRLHF与Ray相关的架构图如下：</p><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/2ac460b1/image-20.png"></p><h3 id="driver-process">Driver Process</h3><ol type="1"><li>在<strong>Driver process中实例化多个<a target="_blank" rel="noopener" href="https://github.com/OpenRLHF/OpenRLHF/blob/v0.5.9.post1/openrlhf/trainer/ray/launcher.py#L143">PPORayActorGroup</a></strong>：<strong>每一个 Group 实例代表着一个PPO模块，包含1个Master Ray-Actor，多个Worker Ray-Actor；每个 Worker Ray-Actor是这个完整模型的 DP 分片。</strong></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PPORayActorGroup</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        num_nodes,					<span class="comment"># 节点数量（物理机器）</span></span></span><br><span class="line"><span class="params">        num_gpus_per_node,	<span class="comment"># 每个节点上的GPU数量</span></span></span><br><span class="line"><span class="params">        ray_actor_type: <span class="type">Type</span>[BasePPORole],	<span class="comment"># 每个Actor类型</span></span></span><br><span class="line"><span class="params">        pg: PlacementGroup = <span class="literal">None</span>,		<span class="comment"># Ray 的 PlacementGroup，控制 Actor 的调度策略</span></span></span><br><span class="line"><span class="params">        num_gpus_per_actor=<span class="number">1</span>,					<span class="comment"># 每个 Actor 分配的 GPU 数量</span></span></span><br><span class="line"><span class="params">        resources: <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="built_in">float</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        num_resources_per_node: <span class="built_in">int</span> = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="literal">None</span>:</span><br><span class="line"> 				......</span><br><span class="line">        <span class="variable language_">self</span>._initiate_actors(pg, num_gpus_per_actor)</span><br><span class="line">解释</span><br></pre></td></tr></table></figure><p>创建<code>PPORayActorGroup</code>实例时，其<code>__init__</code>函数包括创建Ray Actor集群，过程如下：</p><blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_initiate_actors</span>(<span class="params">self, pg, num_gpus_per_actor</span>):</span><br><span class="line">world_size = <span class="variable language_">self</span>._num_nodes * <span class="variable language_">self</span>._num_gpus_per_node	<span class="comment"># 总GPU数量</span></span><br><span class="line"><span class="comment"># 1. 创建 Placement Group（每个节点上GPU数量多于1个）：默认每个Actor需要1GPU+1CPU</span></span><br><span class="line"> <span class="keyword">if</span> <span class="variable language_">self</span>._num_gpus_per_node &gt; <span class="number">1</span> <span class="keyword">and</span> pg <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">     bundles = [&#123;<span class="string">&quot;GPU&quot;</span>: <span class="number">1</span>, <span class="string">&quot;CPU&quot;</span>: <span class="number">1</span>&#125; <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>._num_nodes * <span class="variable language_">self</span>._num_gpus_per_node)]</span><br><span class="line">     <span class="keyword">if</span> <span class="variable language_">self</span>._resources:</span><br><span class="line">         ......</span><br><span class="line">     pg = placement_group(bundles, strategy=<span class="string">&quot;PACK&quot;</span>)</span><br><span class="line">     ray.get(pg.ready())</span><br><span class="line"> <span class="comment"># 2. 创建Master Ray-Actor（rank=0）：</span></span><br><span class="line"> <span class="keyword">if</span> pg:</span><br><span class="line">     master_actor = <span class="variable language_">self</span>.ray_actor_type.options(</span><br><span class="line">         ......</span><br><span class="line">     ).remote(world_size, <span class="number">0</span>, <span class="literal">None</span>, <span class="literal">None</span>)	<span class="comment"># None, None：Master 的地址和端口</span></span><br><span class="line"> <span class="keyword">else</span>:</span><br><span class="line">     master_actor = <span class="variable language_">self</span>.ray_actor_type.options(</span><br><span class="line">         ......</span><br><span class="line">     ).remote(world_size, <span class="number">0</span>, <span class="literal">None</span>, <span class="literal">None</span>)</span><br><span class="line"> <span class="variable language_">self</span>._actor_handlers = [master_actor]</span><br><span class="line"> <span class="comment"># 3. 创建 Worker Ray-Actor（rank=1 到 world_size-1）</span></span><br><span class="line"> <span class="keyword">if</span> world_size &gt; <span class="number">1</span>:</span><br><span class="line"> master_addr,master_port=ray.get(master_actor.get_master_addr_port.remote())</span><br><span class="line"> 	<span class="keyword">for</span> rank <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, world_size):</span><br><span class="line">     <span class="keyword">if</span> pg:</span><br><span class="line">         worker_actor = <span class="variable language_">self</span>.ray_actor_type.options(</span><br><span class="line">            ......</span><br><span class="line">             ),</span><br><span class="line">         ).remote(world_size, rank, master_addr, master_port)</span><br><span class="line">     <span class="keyword">else</span>:</span><br><span class="line">         worker_actor = <span class="variable language_">self</span>.ray_actor_type.options(</span><br><span class="line">            ......</span><br><span class="line">         ).remote(world_size, rank, master_addr, master_port)</span><br><span class="line">     <span class="variable language_">self</span>._actor_handlers.append(worker_actor)</span><br></pre></td></tr></table></figure></blockquote><ul><li><p><code>PPORayActorGroup</code>中维护列表<code>self._actor_handlers</code>，是一个<code>List[ray.actor.ActorHandle]</code>，列表中每个元素表示<strong>某个远端Ray-Actor的引用</strong>（对应PPO-Actor/Ref/Critic/RM实例）。可以在Ray集群中的任何位置调用这个handler，来对相应的远端Ray-Actor执行操作。</p></li><li><p><code>ActorModelRayActor</code>：<strong>创建在远端worker进程上，是Ray-Actor</strong>。它包含了设置ds_zero分布式环境、加载模型权重、数据集准备、optimizer/scheduler准备、训练等一系列操作。</p></li></ul><p>注意：<code>PPORayActorGroup</code>在Driver Process中完成实例化，但主进程中并不包括控制逻辑；算法逻辑在模块对应的<code>PPORayActorGroup</code>中，通过远程调用<code>ActorPPOTrainer</code>实现。</p><h3 id="ray远程调用fit开始训练">Ray远程调用fit，开始训练</h3><p>完成参数初始化、各个模块建立和模型初始化后，控制逻辑交给了隶属于 Actor 的 Group，调用<a href="https://link.zhihu.com/?target=https%3A//github.com/OpenRLHF/OpenRLHF/blob/273422305ea17362319f5569c6f9ef5a16b49cb0/openrlhf/trainer/ray/launcher.py%23L242">async_fit_actor_model</a>，这个方法内会调用所有 Actor worker 的 <code>fit</code>方法，本质上是调用了<a target="_blank" rel="noopener" href="https://github.com/OpenRLHF/OpenRLHF/blob/c438a86ab5981e40f12299c7da4e64468deb7a28/openrlhf/trainer/ppo_trainer.py#L125">PPOTrainer.fit</a>。使得所有worker同时开始训练。</p><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/2ac460b1/image-29.png"></p><blockquote><p>异步调用的实现方式：<code>async_run_method</code>函数通过<code>self._actor_handlers</code>，实现在相应的远端Ray-Actor上异步调用任意指定的方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">async_run_method</span>(<span class="params">self, method_name, *args, **kwargs</span>):</span><br><span class="line"> refs = []</span><br><span class="line"> <span class="keyword">for</span> actor <span class="keyword">in</span> <span class="variable language_">self</span>._actor_handlers:</span><br><span class="line">     method = <span class="built_in">getattr</span>(actor, method_name)</span><br><span class="line">     refs.append(method.remote(*args, **kwargs))</span><br><span class="line"> <span class="keyword">return</span> refs</span><br></pre></td></tr></table></figure></blockquote><ol type="1"><li><p><strong>初始化阶段</strong>：初始化训练参数，设置评估和模型保存频率。若未指定，默认每个epoch评估一次，且不自动保存检查点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        args = <span class="variable language_">self</span>.args</span><br><span class="line">        <span class="comment"># 加载数据集</span></span><br><span class="line">        num_rollouts_per_episodes = <span class="built_in">len</span>(<span class="variable language_">self</span>.prompts_dataloader)</span><br></pre></td></tr></table></figure></li><li><p><strong>检查点加载与vLLM引擎唤醒</strong>：</p><ul><li>检查检查点路径是否存在，若存在则加载；</li><li>若使用vLLM引擎且启用睡眠模式：先唤醒引擎再广播参数，完成后恢复睡眠以节省资源。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 广播初始检查点到vLLM引擎</span></span><br><span class="line">ckpt_path = os.path.join(args.ckpt_path, <span class="string">&quot;_actor&quot;</span>)</span><br><span class="line"><span class="keyword">if</span> args.load_checkpoint <span class="keyword">and</span> os.path.exists(ckpt_path) <span class="keyword">and</span> <span class="keyword">not</span> <span class="variable language_">self</span>.vllm_engines <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="comment"># 若启用vLLM睡眠模式，先唤醒引擎</span></span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.strategy.args.vllm_enable_sleep:</span><br><span class="line">        <span class="keyword">from</span> openrlhf.trainer.ray.vllm_engine <span class="keyword">import</span> batch_vllm_engine_call</span><br><span class="line">        batch_vllm_engine_call(<span class="variable language_">self</span>.vllm_engines, <span class="string">&quot;wake_up&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 异步广播模型参数到vLLM</span></span><br><span class="line">    ref = <span class="variable language_">self</span>.actor_model_group.async_run_method(method_name=<span class="string">&quot;broadcast_to_vllm&quot;</span>)</span><br><span class="line">    ray.get(ref)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 广播完成后重新进入睡眠模式</span></span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.strategy.args.vllm_enable_sleep:</span><br><span class="line">        batch_vllm_engine_call(<span class="variable language_">self</span>.vllm_engines, <span class="string">&quot;sleep&quot;</span>)</span><br></pre></td></tr></table></figure></li><li><p><strong>恢复训练状态</strong>：从断点恢复训练进度，计算当前所处的episode和step</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取已消耗的样本数和当前步数</span></span><br><span class="line">consumed_samples = ray.get(<span class="variable language_">self</span>.actor_model_group.async_run_method(method_name=<span class="string">&quot;get_consumed_samples&quot;</span>))[<span class="number">0</span>]</span><br><span class="line">steps = consumed_samples // args.rollout_batch_size + <span class="number">1</span></span><br><span class="line">start_episode = consumed_samples // args.rollout_batch_size // num_rollouts_per_episodes</span><br><span class="line">consumed_samples = consumed_samples % (num_rollouts_per_episodes * args.rollout_batch_size)</span><br></pre></td></tr></table></figure></li><li><p><strong>训练主循环</strong>：</p><ol type="1"><li>设置数据加载器的epoch和样本偏移，使用<code>tqdm</code>显示当前episode进度条；</li><li>核心步骤：<ul><li><strong>经验生成</strong>：根据输入prompts生成交互经验；</li><li><strong>数据分发</strong>：将经验数据异步发送给Actor和Critic模型；</li><li><strong>PPO训练</strong>：调用<code>ppo_train</code>更新模型参数；</li></ul></li><li>KL控制与日志记录：<ul><li><strong>KL控制</strong>：动态调整KL散度惩罚系数；</li><li><strong>保存日志/检查点内容</strong>：日志包括生成样本、奖励值、训练状态等；检查点按配置频率保存模型和训练状态。</li></ul></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(start_episode, args.num_episodes):</span><br><span class="line">    <span class="comment"># 设置数据加载器的epoch和样本偏移</span></span><br><span class="line">    <span class="variable language_">self</span>.prompts_dataloader.sampler.set_epoch(</span><br><span class="line">        episode, consumed_samples=<span class="number">0</span> <span class="keyword">if</span> episode &gt; start_episode <span class="keyword">else</span> consumed_samples</span><br><span class="line">    )</span><br><span class="line">    pbar = tqdm(<span class="built_in">range</span>(<span class="variable language_">self</span>.prompts_dataloader.__len__()), desc=<span class="string">f&quot;Episode [<span class="subst">&#123;episode + <span class="number">1</span>&#125;</span>/<span class="subst">&#123;args.num_episodes&#125;</span>]&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> _, rand_prompts, labels <span class="keyword">in</span> <span class="variable language_">self</span>.prompts_dataloader:</span><br><span class="line">    <span class="comment"># 生成经验数据（状态-动作-奖励序列）</span></span><br><span class="line">    experiences = <span class="variable language_">self</span>.experience_maker.make_experience_list(rand_prompts, labels, **<span class="variable language_">self</span>.generate_kwargs)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 解码示例样本（用于日志）</span></span><br><span class="line">    sample0 = <span class="variable language_">self</span>.tokenizer.batch_decode(experiences[<span class="number">0</span>].sequences[<span class="number">0</span>].unsqueeze(<span class="number">0</span>), skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span>(sample0)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 异步将经验数据分发到Actor和Critic模型组</span></span><br><span class="line">    refs = <span class="variable language_">self</span>.actor_model_group.async_run_method_batch(method_name=<span class="string">&quot;append&quot;</span>, experience=experiences)</span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.critic_model_group <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        refs.extend(<span class="variable language_">self</span>.critic_model_group.async_run_method_batch(method_name=<span class="string">&quot;append&quot;</span>, experience=experiences))</span><br><span class="line">    ray.get(refs)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 执行PPO训练步骤</span></span><br><span class="line">    status = <span class="variable language_">self</span>.ppo_train(steps)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 更新KL散度控制器</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">&quot;kl&quot;</span> <span class="keyword">in</span> status:</span><br><span class="line">        <span class="variable language_">self</span>.kl_ctl.update(status[<span class="string">&quot;kl&quot;</span>], args.rollout_batch_size * args.n_samples_per_prompt)</span><br><span class="line">    pbar.set_postfix(status)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#记录生成样本和奖励</span></span><br><span class="line">    status[<span class="string">&quot;generated_samples&quot;</span>] = [sample0[<span class="number">0</span>], experiences[<span class="number">0</span>].info[<span class="string">&quot;reward&quot;</span>][<span class="number">0</span>]]</span><br><span class="line">    <span class="comment"># 保存日志和检查点</span></span><br><span class="line">    client_states = &#123;<span class="string">&quot;consumed_samples&quot;</span>: steps * args.rollout_batch_size&#125;</span><br><span class="line">    <span class="variable language_">self</span>.save_logs_and_checkpoints(args, steps, pbar, status, client_states)</span><br><span class="line"></span><br><span class="line">    pbar.update()</span><br><span class="line">    steps = steps + <span class="number">1</span></span><br></pre></td></tr></table></figure></li><li><p><strong>训练终止</strong>：训练结束后关闭WandB/TensorBoard日志连接。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>._wandb <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="variable language_">self</span>._wandb.finish()</span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>._tensorboard <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="variable language_">self</span>._tensorboard.close()</span><br></pre></td></tr></table></figure></li></ol><h4 id="step1.-样本生成将这个-batch-的-prompts-输入给-actorrollout-得到-responses">Step1. <strong>样本生成</strong>：将这个 batch 的 prompts 输入给 Actor，rollout 得到 responses</h4><p>从<code>make_experience_list</code>进入，调用链为：<a target="_blank" rel="noopener" href="https://github.com/OpenRLHF/OpenRLHF/blob/0b530f1119147ba1241632b123032e228ad2636b/openrlhf/trainer/ppo_utils/experience_maker.py#L223"><code>make_experience_list</code></a>-&gt;<a target="_blank" rel="noopener" href="https://github.com/OpenRLHF/OpenRLHF/blob/0b530f1119147ba1241632b123032e228ad2636b/openrlhf/trainer/ppo_utils/experience_maker.py#L625"><code>generate_samples</code></a>-&gt;<a target="_blank" rel="noopener" href="https://github.com/OpenRLHF/OpenRLHF/blob/bb46342711a203c457df2fbca5967fd0549557e0/openrlhf/trainer/ppo_utils/experience_maker.py#L627"><code>_generate_vllm</code></a></p><ol type="1"><li><p><strong>初始化采样参数</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sampling_params = SamplingParams(</span><br><span class="line">    temperature=kwargs.get(<span class="string">&quot;temperature&quot;</span>, <span class="number">1.0</span>),</span><br><span class="line">    top_p=kwargs.get(<span class="string">&quot;top_p&quot;</span>, <span class="number">1.0</span>),</span><br><span class="line">    max_tokens=kwargs.get(<span class="string">&quot;max_new_tokens&quot;</span>, <span class="number">1024</span>),</span><br><span class="line">    ...</span><br><span class="line">)</span><br></pre></td></tr></table></figure></li><li><p><strong>扩展prompts</strong>：采用数据增强，每个提示生成多份样本</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">n_samples_per_prompt = kwargs.pop(<span class="string">&quot;n_samples_per_prompt&quot;</span>, args.n_samples_per_prompt)</span><br><span class="line">all_prompts = <span class="built_in">sum</span>([[prompt] * n_samples_per_prompt <span class="keyword">for</span> prompt <span class="keyword">in</span> all_prompts], [])</span><br></pre></td></tr></table></figure></li><li><p><strong>分布式请求分发</strong>：异步调用每个vllm引擎上的<code>add_requests</code>方法</p><blockquote><p>Vllm engine上的<a target="_blank" rel="noopener" href="https://github.com/OpenRLHF/OpenRLHF/blob/0b530f1119147ba1241632b123032e228ad2636b/openrlhf/trainer/ray/vllm_engine.py#L83"><code>add_requests</code></a>：收集所有Ray-Actor上的请求，统一生成responses</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">add_requests</span>(<span class="params">self, actor_rank, *, sampling_params, prompt_token_ids</span>):</span><br><span class="line">		......</span><br><span class="line">    <span class="comment"># 批量生成</span></span><br><span class="line">    responses = <span class="variable language_">self</span>.llm.generate(prompts=requests, sampling_params=sampling_params)</span><br><span class="line">    <span class="comment"># 结果分发</span></span><br><span class="line">    offset = <span class="number">0</span></span><br><span class="line">    <span class="variable language_">self</span>.responses = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> actor_rank, num <span class="keyword">in</span> num_requests:</span><br><span class="line">        <span class="variable language_">self</span>.response_queues[actor_rank].put(responses[offset : offset + num])</span><br><span class="line">        offset += num</span><br><span class="line">    <span class="variable language_">self</span>.requests = &#123;&#125;	<span class="comment"># 状态重置</span></span><br></pre></td></tr></table></figure><p>调用<a target="_blank" rel="noopener" href="https://github.com/OpenRLHF/OpenRLHF/blob/0b530f1119147ba1241632b123032e228ad2636b/openrlhf/models/actor.py#L136"><code>generate</code></a>函数，完成基于PyTorch的文本生成：调用Pytorch的<a target="_blank" rel="noopener" href="https://docs.pytorch.org/torchtune/0.3/generated/torchtune.generation.generate.html"><code>generate</code></a>函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate</span>(<span class="params">self, input_ids: torch.Tensor, **kwargs</span>) -&gt; <span class="type">Union</span>[</span><br><span class="line">    <span class="type">Tuple</span>[torch.LongTensor, torch.LongTensor],</span><br><span class="line">    <span class="type">Tuple</span>[torch.LongTensor, torch.LongTensor, torch.BoolTensor],</span><br><span class="line">]:</span><br><span class="line">    ......</span><br><span class="line">    <span class="comment"># Call generate</span></span><br><span class="line">    sequences = <span class="variable language_">self</span>.model.generate(**generate_args)</span><br><span class="line">    <span class="comment"># Prepare mask tensor</span></span><br><span class="line">    eos_token_id = generate_args[<span class="string">&quot;eos_token_id&quot;</span>]</span><br><span class="line">    pad_token_id = generate_args[<span class="string">&quot;pad_token_id&quot;</span>]</span><br><span class="line">    <span class="keyword">return</span> process_sequences(sequences, input_ids.size(<span class="number">1</span>), eos_token_id, pad_token_id)</span><br></pre></td></tr></table></figure></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">batch_size = (<span class="built_in">len</span>(all_prompt_token_ids) + <span class="built_in">len</span>(llms) - <span class="number">1</span>) // <span class="built_in">len</span>(llms)</span><br><span class="line"><span class="keyword">for</span> i, llm <span class="keyword">in</span> <span class="built_in">enumerate</span>(llms):</span><br><span class="line">    prompt_token_ids = all_prompt_token_ids[i*batch_size : (i+<span class="number">1</span>)*batch_size]</span><br><span class="line">    refs.append(llm.add_requests.remote(...))</span><br><span class="line">ray.get(refs)</span><br></pre></td></tr></table></figure></li><li><p><strong>收集结果，进行批处理和标准化</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">all_outputs = <span class="built_in">sum</span>(ray.get(all_output_refs), [])</span><br><span class="line">sequences, attention_mask, action_mask = process_sequences(</span><br><span class="line">    sequences, batch_max_input_len, eos_token_id, pad_token_id</span><br><span class="line">)</span><br></pre></td></tr></table></figure></li></ol><p>最后封装样本的数据结构如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Samples(</span><br><span class="line">    sequences: Tensor        <span class="comment"># [batch, seq_len] 完整序列</span></span><br><span class="line">    attention_mask: Tensor   <span class="comment"># [batch, seq_len] 有效token位置</span></span><br><span class="line">    action_mask: Tensor      <span class="comment"># [batch, seq_len] 需优化的token位置</span></span><br><span class="line">    response_length: Tensor  <span class="comment"># [batch] 每个响应的实际长度</span></span><br><span class="line">    prompts: <span class="type">List</span>[<span class="built_in">str</span>]       <span class="comment"># 原始提示</span></span><br><span class="line">    labels: <span class="type">List</span>             <span class="comment"># 对应标签</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="step2.-收集experiences从refrewardcritic上收集并处理exps">Step2. 收集experiences：从Ref/Reward/Critic上收集并处理exps</h4><p>从<code>make_experience_list</code>进入，调用链为：<a target="_blank" rel="noopener" href="https://github.com/OpenRLHF/OpenRLHF/blob/0b530f1119147ba1241632b123032e228ad2636b/openrlhf/trainer/ppo_utils/experience_maker.py#L223"><code>make_experience_list</code></a>-&gt;<a target="_blank" rel="noopener" href="https://github.com/OpenRLHF/OpenRLHF/blob/bb46342711a203c457df2fbca5967fd0549557e0/openrlhf/trainer/ppo_utils/experience_maker.py#L492"><code>make_experience</code></a></p><ol type="1"><li><p><strong>数据准备</strong>：在一遍inference后，收集所有样本信息，为批处理做准备</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sequences_list = [s.sequences <span class="keyword">for</span> s <span class="keyword">in</span> samples_list]</span><br><span class="line">attention_mask_list = [s.attention_mask <span class="keyword">for</span> s <span class="keyword">in</span> samples_list]</span><br><span class="line">......</span><br></pre></td></tr></table></figure></li><li><p><strong>计算Reward模型</strong>：</p><ul><li><strong>本地模式</strong>：直接调用模型组进行计算</li><li><strong>远程模式</strong>：通过Ray分发到远程服务</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">r_refs = <span class="literal">None</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>.remote_rm_url:		<span class="comment"># 本地奖励模型：</span></span><br><span class="line">    r_refs = <span class="variable language_">self</span>.reward_model_group.async_run_method_batch(</span><br><span class="line">        ......</span><br><span class="line">    )</span><br><span class="line"><span class="keyword">else</span>:		<span class="comment"># 远程奖励服务</span></span><br><span class="line">    queries_list = <span class="built_in">sum</span>(</span><br><span class="line">        [<span class="variable language_">self</span>.tokenizer.batch_decode(seq, skip_special_tokens=<span class="literal">False</span>) <span class="keyword">for</span> seq <span class="keyword">in</span> sequences_list], []</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.custom_reward_func:		<span class="comment"># 自定义奖励模型</span></span><br><span class="line">        ......</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_chunks):</span><br><span class="line">           ......</span><br><span class="line">            r = <span class="variable language_">self</span>.custom_reward_func.remote(</span><br><span class="line">                queries_list[start_idx:end_idx],</span><br><span class="line">                prompts_list[start_idx:end_idx],</span><br><span class="line">                labels_list[start_idx:end_idx],</span><br><span class="line">            )</span><br><span class="line">            r_refs.append(r)</span><br><span class="line">    <span class="keyword">else</span>:	<span class="comment"># 将数据分布在不同的远程奖励模型服务器上</span></span><br><span class="line">        num_servers = <span class="built_in">len</span>(<span class="variable language_">self</span>.remote_rm_url)</span><br><span class="line">        batch_size = (<span class="built_in">len</span>(queries_list) + num_servers - <span class="number">1</span>) // num_servers</span><br><span class="line">        r_refs = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_servers):</span><br><span class="line">            ......</span><br><span class="line">            r = remote_rm_fn_ray.remote(</span><br><span class="line">                rm,</span><br><span class="line">                queries=queries_list[start_idx:end_idx],</span><br><span class="line">                prompts=prompts_list[start_idx:end_idx],</span><br><span class="line">                labels=labels_list[start_idx:end_idx],</span><br><span class="line">            )</span><br><span class="line">            r_refs.append(r)</span><br></pre></td></tr></table></figure></li><li><p><strong>从Ref/Critic上收集exps</strong>：</p><ul><li><code>action_log_probs</code>: 当前策略的动作概率</li><li><code>value</code>: 状态价值估计</li><li><code>base_action_log_probs</code>: 参考策略的动作概率（用于KL散度计算）</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Actor模型（当前策略）</span></span><br><span class="line">action_log_probs_ref = <span class="variable language_">self</span>.actor_model_group.async_run_method_batch(...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Critic模型（价值函数）</span></span><br><span class="line">value_ref = <span class="variable language_">self</span>.critic_model_group.async_run_method_batch(...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始模型（参考策略）</span></span><br><span class="line">base_action_log_probs_ref = <span class="variable language_">self</span>.initial_model_group.async_run_method_batch(...)</span><br></pre></td></tr></table></figure></li><li><p><strong>结果整合</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">experience = Experience(</span><br><span class="line">    sequences,                <span class="comment"># 输入序列</span></span><br><span class="line">    action_log_probs,         <span class="comment"># 当前策略logprobs</span></span><br><span class="line">    base_action_log_probs,    <span class="comment"># 参考策略logprobs</span></span><br><span class="line">    value,                    <span class="comment"># 价值函数输出</span></span><br><span class="line">    <span class="literal">None</span>,                     <span class="comment"># 初始化为空的advantage</span></span><br><span class="line">    <span class="literal">None</span>,                     <span class="comment"># 初始化为空的return</span></span><br><span class="line">    attention_mask,           <span class="comment"># 注意力掩码</span></span><br><span class="line">    samples.action_mask,      <span class="comment"># 动作掩码</span></span><br><span class="line">    info,                     <span class="comment"># 元信息</span></span><br><span class="line">    kl                        <span class="comment"># KL散度</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure></li><li><p><strong>计算KL散度</strong>：衡量当前策略与参考策略的差异</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kl = compute_approx_kl(action_log_probs, base_action_log_probs)</span><br><span class="line">kl_mean = masked_mean(kl, samples.action_mask)</span><br></pre></td></tr></table></figure></li></ol><h4 id="step3.-确保将处理后的exps传送给critic并行执行actor和critic的训练">Step3. 确保将处理后的exps传送给Critic，并行执行Actor和Critic的训练</h4><p>从<code>PPOTrainer.ppo_train</code>开始：这里<code>critic_model_group</code>，<code>actor_model_group</code>均为<code>PPORayActorGroup</code>，通过<code>async_run_method</code>远程调用Ray-Actor上的<code>fit</code>方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">ppo_train</span>(<span class="params">self, global_steps</span>):</span><br><span class="line">    status = &#123;&#125;</span><br><span class="line">    <span class="comment"># 1. Critic模型训练</span></span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.critic_model_group <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        ......</span><br><span class="line">        <span class="comment"># 异步启动Critic模型训练(fit方法)</span></span><br><span class="line">        critic_status_ref = <span class="variable language_">self</span>.critic_model_group.async_run_method(method_name=<span class="string">&quot;fit&quot;</span>)</span><br><span class="line">				......</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 2. Actor模型训练</span></span><br><span class="line">    <span class="keyword">if</span> global_steps &gt; <span class="variable language_">self</span>.freezing_actor_steps:</span><br><span class="line">      <span class="comment"># 当全局步数超过冻结步数时才执行：异步启动actor模型训练</span></span><br><span class="line">      	......</span><br><span class="line">        actor_status_ref = <span class="variable language_">self</span>.actor_model_group.async_run_method(method_name=<span class="string">&quot;fit&quot;</span>, kl_ctl=<span class="variable language_">self</span>.kl_ctl.value)</span><br><span class="line">        status.update(ray.get(actor_status_ref)[<span class="number">0</span>])	<span class="comment"># 获取并记录actor训练状态</span></span><br><span class="line">				...... </span><br><span class="line">        <span class="comment"># 2.1 如果有vLLM引擎：广播actor模型权重到vLLM引擎</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.vllm_engines <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">              ......</span><br><span class="line">  ray.get(<span class="variable language_">self</span>.actor_model_group.async_run_method(method_name=<span class="string">&quot;broadcast_to_vllm&quot;</span>))</span><br><span class="line">				......</span><br><span class="line">    <span class="comment"># 3. 等待Critic训练完成，更新其状态</span></span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.critic_model_group <span class="keyword">and</span> <span class="keyword">not</span> <span class="variable language_">self</span>.strategy.args.colocate_all_models:</span><br><span class="line">        status.update(ray.get(critic_status_ref)[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> status</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><code>ppo_critic</code>，<code>ppo_actor</code>在Ray上分别有各自的<code>ppo_train</code>函数：</p><ol type="1"><li><p><a target="_blank" rel="noopener" href="https://github.com/OpenRLHF/OpenRLHF/blob/bb46342711a203c457df2fbca5967fd0549557e0/openrlhf/trainer/ppo_utils/experience_maker.py#L470"><strong>将exps传送给Critic</strong></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.critic <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="keyword">for</span> experience <span class="keyword">in</span> experiences:</span><br><span class="line">        <span class="comment"># send experience to critic</span></span><br><span class="line">        experience_cpu = deepcopy(experience)</span><br><span class="line">        experience_cpu.to_device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">        <span class="variable language_">self</span>._ref = <span class="variable language_">self</span>.critic.append.remote(experience_cpu)</span><br></pre></td></tr></table></figure></li><li><p><strong>Actor训练</strong>：调用链为：<a target="_blank" rel="noopener" href="https://github.com/OpenRLHF/OpenRLHF/blob/ebe9b6fdd0753c248e51593186c7420fc751e44d/openrlhf/trainer/ppo_trainer.py#L204"><code>PPOTrainer.ppo_train</code></a>-&gt;<a target="_blank" rel="noopener" href="https://github.com/OpenRLHF/OpenRLHF/blob/ebe9b6fdd0753c248e51593186c7420fc751e44d/openrlhf/trainer/ray/ppo_actor.py#L454">‎<code>ActorModelRayActor.fit‎</code></a>-&gt;<a target="_blank" rel="noopener" href="https://github.com/OpenRLHF/OpenRLHF/blob/ebe9b6fdd0753c248e51593186c7420fc751e44d/openrlhf/trainer/ray/ppo_actor.py#L143">`<code>ActorModelRayActor.ppo_train</code></a></p></li><li><p><strong>Critic训练</strong>：调用链为：<a target="_blank" rel="noopener" href="https://github.com/OpenRLHF/OpenRLHF/blob/ebe9b6fdd0753c248e51593186c7420fc751e44d/openrlhf/trainer/ppo_trainer.py#L204"><code>PPOTrainer.ppo_train</code></a>-&gt;<a target="_blank" rel="noopener" href="https://github.com/OpenRLHF/OpenRLHF/blob/ebe9b6fdd0753c248e51593186c7420fc751e44d/openrlhf/trainer/ray/ppo_critic.py#L246">‎<code>CriticModelRayActor.fit‎</code></a>-&gt;<a target="_blank" rel="noopener" href="https://github.com/OpenRLHF/OpenRLHF/blob/ebe9b6fdd0753c248e51593186c7420fc751e44d/openrlhf/trainer/ray/ppo_critic.py#L59">``<code>CriticModelRayActor.ppo_train</code></a></p></li></ol><h4 id="step4.-vllm_engine权重更新">Step4. vllm_engine权重更新</h4><h3 id="总流程">总流程</h3><p>最后看看总流程：<a target="_blank" rel="noopener" href="https://github.com/OpenRLHF/OpenRLHF/tree/main">OpenRLHF</a>/<a target="_blank" rel="noopener" href="https://github.com/OpenRLHF/OpenRLHF/tree/main/openrlhf">openrlhf</a>/<a target="_blank" rel="noopener" href="https://github.com/OpenRLHF/OpenRLHF/tree/main/openrlhf/cli">cli</a>/<a target="_blank" rel="noopener" href="https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/cli/train_ppo_ray.py">train_ppo_ray.py</a></p><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/2ac460b1/image-21.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">args</span>):</span><br><span class="line">  	<span class="string">&#x27;&#x27;&#x27;1. 初始化阶段&#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 1.1 分布式策略配置</span></span><br><span class="line">    strategy = get_strategy(args)</span><br><span class="line">    strategy.<span class="built_in">print</span>(args)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1.2 Placement Group初始化</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;1.2 如果采用colocate_actor_ref或colocate_all_models策略：创建Placement Group</span></span><br><span class="line"><span class="string">    	将 Actor 和 Reference 模型部署在相同的 GPU 上，减少跨节点通信&#x27;&#x27;&#x27;</span></span><br><span class="line">    pg = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> args.colocate_actor_ref <span class="keyword">or</span> args.colocate_all_models:</span><br><span class="line">        <span class="keyword">if</span> args.init_kl_coef &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">assert</span> (</span><br><span class="line">                args.actor_num_nodes == args.ref_num_nodes</span><br><span class="line">                <span class="keyword">and</span> args.actor_num_gpus_per_node == args.ref_num_gpus_per_node</span><br><span class="line">            ), <span class="string">f&quot;num_nodes and num_gpus_per_node must be the same when colocate actor and ref model.&quot;</span></span><br><span class="line"></span><br><span class="line">        bundles = [&#123;<span class="string">&quot;GPU&quot;</span>: <span class="number">1</span>, <span class="string">&quot;CPU&quot;</span>: <span class="number">1</span>&#125; <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(args.actor_num_nodes * args.actor_num_gpus_per_node)]</span><br><span class="line">        pg = placement_group(bundles, strategy=<span class="string">&quot;PACK&quot;</span>)</span><br><span class="line">        ray.get(pg.ready())</span><br><span class="line">		</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;2. 核心组件初始化&#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 2.1 初始化vLLM引擎（用于文本生成）</span></span><br><span class="line">    vllm_engines = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> args.vllm_num_engines <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> args.vllm_num_engines &gt; <span class="number">0</span>:</span><br><span class="line">        max_len = args.max_len <span class="keyword">if</span> args.max_len <span class="keyword">else</span> args.prompt_max_len + args.generate_max_len</span><br><span class="line">        <span class="keyword">if</span> args.colocate_all_models:</span><br><span class="line">            <span class="keyword">assert</span> (</span><br><span class="line">                args.actor_num_nodes * args.actor_num_gpus_per_node</span><br><span class="line">                == args.vllm_num_engines * args.vllm_tensor_parallel_size</span><br><span class="line">            ), (</span><br><span class="line">                <span class="string">f&quot;actor_num_nodes * actor_num_gpus_per_node must be equal to &quot;</span></span><br><span class="line">                <span class="string">f&quot;vllm_num_engines * vllm_tensor_parallel_size, got <span class="subst">&#123;args.actor_num_nodes * args.actor_num_gpus_per_node&#125;</span> &quot;</span></span><br><span class="line">                <span class="string">f&quot;and <span class="subst">&#123;args.vllm_num_engines * args.vllm_tensor_parallel_size&#125;</span>&quot;</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        vllm_engines = create_vllm_engines(	</span><br><span class="line">            args.vllm_num_engines,</span><br><span class="line">            args.vllm_tensor_parallel_size,	<span class="comment"># Tensor 并行度（GPU 数量）</span></span><br><span class="line">            args.pretrain,</span><br><span class="line">            args.seed,</span><br><span class="line">            args.full_determinism,</span><br><span class="line">            args.enable_prefix_caching,			<span class="comment"># 启用 KV Cache 复用</span></span><br><span class="line">            args.enforce_eager,</span><br><span class="line">            max_len,</span><br><span class="line">            pg <span class="keyword">if</span> args.colocate_all_models <span class="keyword">else</span> <span class="literal">None</span>,</span><br><span class="line">            args.vllm_gpu_memory_utilization,</span><br><span class="line">            args.vllm_enable_sleep,</span><br><span class="line">        )</span><br><span class="line">    <span class="comment"># 2.2 Actor / Critic / Reward / Reference 模型初始化</span></span><br><span class="line">		<span class="string">&#x27;&#x27;&#x27;num_gpus_per_actor：</span></span><br><span class="line"><span class="string">				如果使用PlacementGroup，则允许5个Actor共享1个GPU；否则每个Actor独占1个GPU&#x27;&#x27;&#x27;</span></span><br><span class="line">    actor_model = PPORayActorGroup(</span><br><span class="line">        args.actor_num_nodes,</span><br><span class="line">        args.actor_num_gpus_per_node,</span><br><span class="line">        ActorModelRayActor,</span><br><span class="line">        pg=pg,</span><br><span class="line">        num_gpus_per_actor=<span class="number">0.2</span> <span class="keyword">if</span> pg <span class="keyword">else</span> <span class="number">1</span>,</span><br><span class="line">        duplicate_actors=args.ring_attn_size * args.ds_tensor_parallel_size,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">if</span> args.init_kl_coef &lt;= <span class="number">0</span>:</span><br><span class="line">        ref_model = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        ref_model = PPORayActorGroup(</span><br><span class="line">            args.ref_num_nodes,</span><br><span class="line">            args.ref_num_gpus_per_node,</span><br><span class="line">            ReferenceModelRayActor,</span><br><span class="line">            pg=pg,</span><br><span class="line">            num_gpus_per_actor=<span class="number">0.2</span> <span class="keyword">if</span> pg <span class="keyword">else</span> <span class="number">1</span>,</span><br><span class="line">            duplicate_actors=args.ring_attn_size * args.ds_tensor_parallel_size,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> args.colocate_all_models:</span><br><span class="line">        pg = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> args.critic_pretrain <span class="keyword">and</span> args.colocate_critic_reward:</span><br><span class="line">        <span class="keyword">assert</span> (</span><br><span class="line">            args.critic_num_nodes == args.reward_num_nodes</span><br><span class="line">            <span class="keyword">and</span> args.critic_num_gpus_per_node == args.reward_num_gpus_per_node</span><br><span class="line">        ), <span class="string">f&quot;num_nodes and num_gpus_per_node must be the same when colocate critic and reward model.&quot;</span></span><br><span class="line"></span><br><span class="line">        bundles = [&#123;<span class="string">&quot;GPU&quot;</span>: <span class="number">1</span>, <span class="string">&quot;CPU&quot;</span>: <span class="number">1</span>&#125; <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(args.critic_num_nodes * args.critic_num_gpus_per_node)]</span><br><span class="line">        pg = placement_group(bundles, strategy=<span class="string">&quot;PACK&quot;</span>)</span><br><span class="line">        ray.get(pg.ready())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.critic_pretrain:</span><br><span class="line">        critic_model = PPORayActorGroup(</span><br><span class="line">            args.critic_num_nodes,</span><br><span class="line">            args.critic_num_gpus_per_node,</span><br><span class="line">            CriticModelRayActor,</span><br><span class="line">            pg=pg,</span><br><span class="line">            num_gpus_per_actor=<span class="number">0.2</span> <span class="keyword">if</span> pg <span class="keyword">else</span> <span class="number">1</span>,</span><br><span class="line">            duplicate_actors=args.ring_attn_size * args.ds_tensor_parallel_size,</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        critic_model = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> args.remote_rm_url:</span><br><span class="line">        reward_pretrain = args.reward_pretrain</span><br><span class="line">        reward_model = PPORayActorGroup(</span><br><span class="line">            args.reward_num_nodes,</span><br><span class="line">            args.reward_num_gpus_per_node,</span><br><span class="line">            RewardModelRayActor,</span><br><span class="line">            pg=pg,</span><br><span class="line">            num_gpus_per_actor=<span class="number">0.2</span> <span class="keyword">if</span> pg <span class="keyword">else</span> <span class="number">1</span>,</span><br><span class="line">            duplicate_actors=args.ring_attn_size * args.ds_tensor_parallel_size,</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        reward_model = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;3. 训练流程&#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 3.1 初始化训练控制器PPOTrainer</span></span><br><span class="line">    ppo_trainer = PPOTrainer.remote(</span><br><span class="line">        args.pretrain,</span><br><span class="line">        strategy,</span><br><span class="line">        actor_model,</span><br><span class="line">        critic_model,</span><br><span class="line">        reward_model,</span><br><span class="line">        ref_model,</span><br><span class="line">        vllm_engines,</span><br><span class="line">        prompt_split=args.prompt_split,</span><br><span class="line">        eval_split=args.eval_split,</span><br><span class="line">        <span class="comment"># generate kwargs</span></span><br><span class="line">        do_sample=<span class="literal">True</span>,</span><br><span class="line">        prompt_max_len=args.prompt_max_len,</span><br><span class="line">        max_new_tokens=args.generate_max_len,</span><br><span class="line">        max_length=args.max_len,</span><br><span class="line">        temperature=args.temperature,</span><br><span class="line">        top_p=args.top_p,</span><br><span class="line">    )</span><br><span class="line">    max_steps = ray.get(ppo_trainer.get_max_steps.remote())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3.2 分阶段初始化</span></span><br><span class="line">    <span class="comment"># 第一阶段：初始化Actor/Ref/Reward</span></span><br><span class="line">    refs = []</span><br><span class="line">    <span class="keyword">if</span> ref_model <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        refs.extend(ref_model.async_init_model_from_pretrained(strategy, args.pretrain))</span><br><span class="line">    refs.extend(actor_model.async_init_model_from_pretrained(strategy, args.pretrain, max_steps, vllm_engines))</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> args.remote_rm_url:</span><br><span class="line">        refs.extend(reward_model.async_init_model_from_pretrained(strategy, reward_pretrain))</span><br><span class="line">    ray.get(refs)		<span class="comment"># 同步等待</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 第二阶段：初始化Critic（Critic需要等Actor确定max_steps后才能初始化）</span></span><br><span class="line">    <span class="keyword">if</span> args.critic_pretrain:</span><br><span class="line">        refs.extend(critic_model.async_init_model_from_pretrained(strategy, args.critic_pretrain, max_steps))</span><br><span class="line">        ray.get(refs)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3.3 训练执行</span></span><br><span class="line">    ray.get(ppo_trainer.fit.remote())</span><br><span class="line"></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;4. 收尾阶段&#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 4.1 模型保存：Actor必保存；Critic可选保存</span></span><br><span class="line">    ray.get(actor_model.async_save_model())</span><br><span class="line">    <span class="keyword">if</span> args.critic_pretrain <span class="keyword">and</span> args.save_value_network:</span><br><span class="line">        ray.get(critic_model.async_save_model())</span><br></pre></td></tr></table></figure><h2 id="openrlhf的colocate策略">OpenRLHF的Colocate策略</h2><h3 id="原理">原理</h3><p>在进入colocate策略的分析前，先回顾一下PPO的工作流：</p><ol type="1"><li><p>准备一个batch的prompts；</p></li><li><p>将这个 batch 的 prompts 输入给 Actor，rollout 得到 responses；</p></li><li><p>将 prompt + responses 输入给 Critic/Reward/Reference，进行 inference，分别计算得得到 values、reward 和 log probs，将这些整合称为 experiences；</p><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/2ac460b1/image-22.png"></p></li><li><p>根据 experiences 多轮计算 actor loss 和 critic loss 并更新 Actor 和 Critic。</p><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/2ac460b1/image-23.png"></p></li></ol><p>再纵向整理一次各个模块的工作流：</p><ol type="1"><li><p>Actor：需要 training engine 和 rollout engine。前者使用现代 training engine，比如 Megatron 或者 FSDP，后者得用现代推理引擎，比如 SGLang 或者 vllm 作为 rollout engine。有一个小问题，为什么不能拿着 training engine 得到的 logits 做 sampling 然后 decode，貌似也可以用去 rollout？简单来说，太慢了，用训练引擎做 decode 的效果自然不如专用的推理引擎。</p></li><li><p>Critic：需要 training engine 和 inference engine。前者还是是现代的训练引擎，但是后者，可以用现代的推理引擎的高效 prefill 来得到 value 么？其实不能，critic model 的 inference 会直接复用 training engine 的 forward 来得到 value，所以 critic 的 inference engine 和 training engine 其实是同一个。</p></li><li><p>Reference 和 Reward：只需要 inference，因为二者不需要训练，但是用现代推理引擎得到的 log probs 和 reward 的精度不如用现代训练引擎得到的精度，所以这里选择用 training engine 的 forward 来做 inference，得到 log probs 和 reward。</p></li></ol><p>collocate 策略：</p><ol type="1"><li>将 actor 的 training engine 和 reference 的 inference engine 放置在同一个资源组上；</li><li>将 critic 的 training/inference engine 和 reward 的 inference engine 放置在同一个资源组上；</li><li>最后单独放置 actor 的 rollout engine。</li></ol><h3 id="部署actorrefcriticrm实例">部署Actor/Ref/Critic/RM实例</h3><h4 id="非共同部署">非共同部署</h4><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/2ac460b1/image-24.png"></p><p>一个部署示例如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">actor_model = PPORayActorGroup(</span><br><span class="line">    args.actor_num_nodes,		<span class="comment"># 部署想用的节点数</span></span><br><span class="line">    args.actor_num_gpus_per_node,	<span class="comment"># 部署后每个节点上想用的gpu数</span></span><br><span class="line">    ActorModelRayActor,			<span class="comment"># Actor/Critic/Reward/ReferenceRayActor</span></span><br><span class="line">    pg=pg,</span><br><span class="line">    num_gpus_per_actor=<span class="number">0.2</span> <span class="keyword">if</span> pg <span class="keyword">else</span> <span class="number">1</span>,	<span class="comment"># </span></span><br><span class="line">    duplicate_actors=args.ring_attn_size * args.ds_tensor_parallel_size,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>其中，<code>num_gpus_per_actor</code>等于1说明每个实例占满一张gpu，即“非共同部署”；小于1说明每个实例只占部分gpu，即“共同部署”。</p><h4 id="共同部署">共同部署</h4><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/2ac460b1/image-25.png"></p><p>这里展示PPO-Actor和PPO-Reference的colocate策略：</p><ol type="1"><li><p>创建一个PlacementGroup，接下来Actor和Reference实例均使用这个配置方案：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bundles = [&#123;<span class="string">&quot;GPU&quot;</span>: <span class="number">1</span>, <span class="string">&quot;CPU&quot;</span>: <span class="number">1</span>&#125; <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(args.actor_num_nodes * args.actor_num_gpus_per_node)]</span><br><span class="line">pg = placement_group(bundles, strategy=<span class="string">&quot;PACK&quot;</span>)</span><br></pre></td></tr></table></figure></li><li><p>PPO-Actor和PPO-Reference分别创建一个PPORayActorGroup。<strong>为了实现模块之间的colocate，往两个Group中传入同一个pg</strong>：</p><p><strong>在Group内部，通过<code>num_gpus_per_actor</code>分配每个worker的bundle</strong></p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">......</span><br><span class="line">actor_model = PPORayActorGroup(</span><br><span class="line">    args.actor_num_nodes,</span><br><span class="line">    args.actor_num_gpus_per_node,</span><br><span class="line">    ActorModelRayActor,</span><br><span class="line">    pg=pg,</span><br><span class="line">    num_gpus_per_actor=<span class="number">0.2</span> <span class="keyword">if</span> pg <span class="keyword">else</span> <span class="number">1</span>,</span><br><span class="line">    duplicate_actors=args.ring_attn_size * args.ds_tensor_parallel_size,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">if</span> args.init_kl_coef &lt;= <span class="number">0</span>:</span><br><span class="line">    ref_model = <span class="literal">None</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    ref_model = PPORayActorGroup(</span><br><span class="line">        args.ref_num_nodes,</span><br><span class="line">        args.ref_num_gpus_per_node,</span><br><span class="line">        ReferenceModelRayActor,</span><br><span class="line">        pg=pg,</span><br><span class="line">        num_gpus_per_actor=<span class="number">0.2</span> <span class="keyword">if</span> pg <span class="keyword">else</span> <span class="number">1</span>,</span><br><span class="line">        duplicate_actors=args.ring_attn_size * args.ds_tensor_parallel_size,</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><h3 id="部署vllm_engines实例">部署vllm_engines实例</h3><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/2ac460b1/image-26.png"></p><p>对于<strong>Rollout模块</strong>：</p><ol type="1"><li>Driver process中创建一个或多个<a target="_blank" rel="noopener" href="https://github.com/OpenRLHF/OpenRLHF/blob/c438a86ab5981e40f12299c7da4e64468deb7a28/openrlhf/trainer/ray/vllm_engine.py#L26">LLMRayActor</a>（worker端的Ray-Actor），每个代表一个 vLLM engine（一个完整的 DP 模型），由<a target="_blank" rel="noopener" href="https://github.com/OpenRLHF/OpenRLHF/blob/c438a86ab5981e40f12299c7da4e64468deb7a28/openrlhf/trainer/ray/vllm_engine.py#L111">create_vllm_engines</a>创建。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create_vllm_engines函数部分代码：</span></span><br><span class="line">vllm_engines.append(</span><br><span class="line">    LLMRayActor.options(</span><br><span class="line">        num_cpus=num_gpus,</span><br><span class="line">        num_gpus=num_gpus,</span><br><span class="line">        scheduling_strategy=scheduling_strategy,</span><br><span class="line">    ).remote(</span><br><span class="line">        model=pretrain,</span><br><span class="line"> 				......</span><br><span class="line">    )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>每个 engine 内部通过 Ray 启动 TP Ray-Actor（这个 Ray-Actor 会 attach 到已有的 cluster，不会新建一个）。</p><h3 id="ds_rank0与vllm_ranks之间的通信">ds_rank0与vllm_ranks之间的通信</h3><p>假设DP分组如下：</p><ul><li>Actor0 / Ref0 / RM0 / Critic0 / vllm_engine0为一组</li><li>Actor1 / Ref1 / RM1 / Critic1 / vllm_engine1为一组</li><li>Actor2 / Ref2 / RM2 / Critic2 / vllm_engine2为一组</li><li>Actor3 / Ref3 / RM3 / Critic3 / vllm_engine3为一组</li></ul><p>每一组负责一个micro-batch的训练（一个DP分片）。</p><p>在OpenRLHF中，<strong>Actor和Rollout是两个独立的模块，前者放在deepseed训练引擎，后者放在vLLM中，需要保持权重同步</strong>。因此，当PPO-Actor更新时，ds_rank0需要和all_vllm_ranks进行通讯，最新的权重broadcast给所有vllm_ranks：</p><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/2ac460b1/image-27.png"></p><p>分成以下几个步骤：</p><h4 id="创建通信组">创建通信组</h4><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/2ac460b1/image-28.png"></p><ol type="1"><li><p><strong>PPO-Actor0（ds_rank0）所在的worker进程</strong>：<strong>通过handler引用，触发远端每个vllm_engine上的init_process_group操作，并将ds_rank0纳入通讯组</strong>。<a target="_blank" rel="noopener" href="https://github.com/OpenRLHF/OpenRLHF/blob/bb46342711a203c457df2fbca5967fd0549557e0/openrlhf/trainer/ray/ppo_actor.py#L58">code</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create torch group with deepspeed rank 0 and all vllm ranks</span></span><br><span class="line"><span class="comment"># to update vllm engine&#x27;s weights after each training stage.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Say we have 3 vllm engines and eache of them has 4 GPUs,</span></span><br><span class="line"><span class="comment"># then the torch group is:</span></span><br><span class="line"><span class="comment"># [    0,      1, 2, 3, 4,  5, 6, 7, 8,  9, 10, 11, 12]</span></span><br><span class="line"><span class="comment"># |ds rank 0 |  engine-0  |  engine-1  |   engine-2   |</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># For ZeRO-1/2:</span></span><br><span class="line"><span class="comment">#   1. Broadcast parameters from rank 0 to all vllm engines</span></span><br><span class="line"><span class="comment"># For ZeRO-3:</span></span><br><span class="line"><span class="comment">#   1. AllGather paramters to rank 0</span></span><br><span class="line"><span class="comment">#   2. Broadcast parameters from rank 0 to all vllm engines</span></span><br><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.vllm_engines <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> torch.distributed.get_rank() == <span class="number">0</span>:</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment"># world_size = num_of_all_vllm_ranks + 1 ds_rank0</span></span><br><span class="line">    world_size = vllm_num_engines * vllm_tensor_parallel_size + <span class="number">1</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="comment"># =====================================================================</span></span><br><span class="line">    <span class="comment"># 遍历每个vllm_engines，将其下的每个vllm_rank添加进通讯组中，这里又分成两步：</span></span><br><span class="line">    <span class="comment"># 1. engine.init_process_group.remote(...)：</span></span><br><span class="line">    <span class="comment">#    首先，触发远程vllm_engine的init_process_group方法</span></span><br><span class="line">    <span class="comment"># 2. 远程vllm_engine是一个包装过的vllm实例，它的init_process_group</span></span><br><span class="line">    <span class="comment">#    方法将进一步触发这个vllm实例下的各个worker进程（见4.4图例），</span></span><br><span class="line">    <span class="comment">#    最终是在这些worker进程上执行“将每个vllm_rank&quot;添加进ds_rank0通讯组的工作</span></span><br><span class="line">    <span class="comment"># =====================================================================</span></span><br><span class="line">    refs = [</span><br><span class="line">        engine.init_process_group.remote(</span><br><span class="line">            <span class="comment"># ds_rank0所在node addr</span></span><br><span class="line">            master_address, </span><br><span class="line">            <span class="comment"># ds_rank0所在node port</span></span><br><span class="line">            master_port,</span><br><span class="line">            <span class="comment"># 该vllm_engine的第一个rank在&quot;ds_rank0 + all_vllm_ranks“中的global_rank，</span></span><br><span class="line">            <span class="comment"># 该值将作为一个offset，以该值为起点，可以推算出该vllm_engine中其余vllm_rank的global_rank</span></span><br><span class="line">            i * vllm_tensor_parallel_size + <span class="number">1</span>, </span><br><span class="line">            world_size,</span><br><span class="line">            <span class="string">&quot;openrlhf&quot;</span>,</span><br><span class="line">            backend=backend,</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">for</span> i, engine <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="variable language_">self</span>.vllm_engines)</span><br><span class="line">    ]</span><br><span class="line">    <span class="comment"># =====================================================================</span></span><br><span class="line">    <span class="comment"># 将ds_rank0添加进通讯组中</span></span><br><span class="line">    <span class="comment"># =====================================================================</span></span><br><span class="line">    <span class="variable language_">self</span>._model_update_group = init_process_group(</span><br><span class="line">        backend=backend,</span><br><span class="line">        init_method=<span class="string">f&quot;tcp://<span class="subst">&#123;master_address&#125;</span>:<span class="subst">&#123;master_port&#125;</span>&quot;</span>,</span><br><span class="line">        world_size=world_size,</span><br><span class="line">        rank=<span class="number">0</span>,</span><br><span class="line">        group_name=<span class="string">&quot;openrlhf&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># =====================================================================</span></span><br><span class="line">    <span class="comment"># 确保all_vllm_ranks都已添加进通讯组中</span></span><br><span class="line">    <span class="comment"># =====================================================================</span></span><br><span class="line">    ray.get(refs)</span><br></pre></td></tr></table></figure></li><li><p><strong>每个vllm_engine（即每个包装后的vllm实例）下的worker进程</strong>：<a target="_blank" rel="noopener" href="https://github.com/OpenRLHF/OpenRLHF/blob/bb46342711a203c457df2fbca5967fd0549557e0/openrlhf/trainer/ray/vllm_worker_wrap.py#L11">code</a></p><ul><li><strong>例如tp_size=2，那么每个vllm实例下就有2个worker进程，这两个worker进程都会运行这段代码。</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">WorkerWrap</span>(<span class="title class_ inherited__">Worker</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_process_group</span>(<span class="params">self, master_address, master_port, rank_offset, world_size, group_name, backend=<span class="string">&quot;nccl&quot;</span></span>):</span><br><span class="line">        <span class="keyword">assert</span> torch.distributed.is_initialized(), <span class="string">f&quot;default torch process group must be initialized&quot;</span></span><br><span class="line">        <span class="keyword">assert</span> group_name != <span class="string">&quot;&quot;</span>, <span class="string">f&quot;group name must not be empty&quot;</span></span><br><span class="line">        <span class="comment"># =====================================================================</span></span><br><span class="line">        <span class="comment"># torch.distributed.get_rank(): 在当前vllm_engine内部的rank，</span></span><br><span class="line">        <span class="comment">#                               例如在tp_size = 2时，这个值要么是0，要么是1</span></span><br><span class="line">        <span class="comment"># rank_offset：当前vllm_engine中的第一个rank在“ds_rank0 + all_vllm_ranks&quot;中的global_rank</span></span><br><span class="line">        <span class="comment"># 两者相加：最终得到当前rank在“ds_rank0 + all_vllm_ranks&quot;中的global_rank</span></span><br><span class="line">        <span class="comment"># =====================================================================</span></span><br><span class="line">        rank = torch.distributed.get_rank() + rank_offset</span><br><span class="line">        <span class="variable language_">self</span>._model_update_group = init_process_group(</span><br><span class="line">            backend=backend,</span><br><span class="line">            init_method=<span class="string">f&quot;tcp://<span class="subst">&#123;master_address&#125;</span>:<span class="subst">&#123;master_port&#125;</span>&quot;</span>,</span><br><span class="line">            world_size=world_size,</span><br><span class="line">            rank=rank,</span><br><span class="line">            group_name=group_name,</span><br><span class="line">        )</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure></li></ol><h4 id="广播ppo-actor权重到all_vllm_ranks">广播PPO-Actor权重到all_vllm_ranks</h4><p>分成以下两步：</p><ol type="1"><li><p><strong>ds_rank0对应的worker进程中</strong>：<strong>PPO-Actor ds_rank0发送权重</strong> <a target="_blank" rel="noopener" href="https://github.com/OpenRLHF/OpenRLHF/blob/bb46342711a203c457df2fbca5967fd0549557e0/openrlhf/trainer/ray/ppo_actor.py#L146">code</a></p><p>准备工作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_broadcast_to_vllm</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="comment"># 1. 前缀缓存清理：清空 vLLM 的 KV Cache，避免旧参数生成的缓存影响新结果</span></span><br><span class="line">  	use_prefix_cache = <span class="built_in">getattr</span>(<span class="variable language_">self</span>.strategy.args, <span class="string">&quot;enable_prefix_caching&quot;</span>, <span class="literal">False</span>)</span><br><span class="line">    cache_reset_refs = []</span><br><span class="line">    <span class="keyword">if</span> use_prefix_cache <span class="keyword">and</span> torch.distributed.get_rank() == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># clear prefix cache</span></span><br><span class="line">        <span class="keyword">for</span> engine <span class="keyword">in</span> <span class="variable language_">self</span>.vllm_engines:</span><br><span class="line">            cache_reset_refs.append(engine.reset_prefix_cache.remote())</span><br><span class="line">    <span class="comment"># 2. 清理GPU缓存，避免OOM</span></span><br><span class="line">    torch.cuda.empty_cache()</span><br><span class="line">    <span class="comment"># 3. 遍历模型参数，记录当前参数序号（count）和总参数数（num_params）</span></span><br><span class="line">    model = <span class="variable language_">self</span>.actor.model.module	<span class="comment"># 获取底层模型（去掉 DP/DDP 包装）</span></span><br><span class="line">    count, num_params = <span class="number">0</span>, <span class="built_in">len</span>(<span class="built_in">list</span>(model.named_parameters()))</span><br><span class="line">    <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">        count += <span class="number">1</span>  </span><br></pre></td></tr></table></figure><p>广播模式分为两种，通过<code>self.use_cuda_ipc</code> 切换：之后详细阐述</p></li><li><p><strong>每个vllm_engine（即每个包装后的vllm实例）下的worker进程</strong>：<strong>各个vllm_ranks接收权重</strong> <a target="_blank" rel="noopener" href="https://github.com/OpenRLHF/OpenRLHF/blob/bb46342711a203c457df2fbca5967fd0549557e0/openrlhf/trainer/ray/vllm_worker_wrap.py#L29">code</a></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">update_weight</span>(<span class="params">self, name, dtype, shape, empty_cache=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="keyword">if</span> torch.distributed.get_rank() == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;update weight: <span class="subst">&#123;name&#125;</span>, dtype: <span class="subst">&#123;dtype&#125;</span>, shape: <span class="subst">&#123;shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> dtype == <span class="variable language_">self</span>.model_config.dtype, <span class="string">f&quot;mismatch dtype: src <span class="subst">&#123;dtype&#125;</span>, dst <span class="subst">&#123;self.model_config.dtype&#125;</span>&quot;</span></span><br><span class="line">    <span class="comment"># 创建同尺寸空张量用于接收ds_rank0广播来的权重</span></span><br><span class="line">    weight = torch.empty(shape, dtype=dtype, device=<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line">    <span class="comment"># 接收权重</span></span><br><span class="line">    torch.distributed.broadcast(weight, <span class="number">0</span>, group=<span class="variable language_">self</span>._model_update_group)</span><br><span class="line">		<span class="comment"># 使用接收到的权重进行更新</span></span><br><span class="line">    <span class="variable language_">self</span>.model_runner.model.load_weights(weights=[(name, weight)])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">del</span> weight</span><br></pre></td></tr></table></figure></li></ol><h5 id="默认常规广播ray-collective-或-pytorch-ddp">默认：常规广播（Ray Collective 或 PyTorch DDP）</h5><p>条件：<code>self.use_cuda_ipc = False</code>（初始化）</p><ul><li><code>use_ray=True</code>：使用 <code>ray.util.collective</code> 进行跨节点广播</li><li><code>use_ray=False</code>：使用 PyTorch 原生 <code>torch.distributed.broadcast</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>.use_cuda_ipc:</span><br><span class="line">    use_ray = <span class="built_in">getattr</span>(<span class="variable language_">self</span>.strategy.args, <span class="string">&quot;vllm_sync_with_ray&quot;</span>, <span class="literal">False</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Rank 0 准备vLLM更新请求</span></span><br><span class="line">    <span class="keyword">if</span> torch.distributed.get_rank() == <span class="number">0</span>:</span><br><span class="line">        shape = param.shape <span class="keyword">if</span> <span class="variable language_">self</span>.strategy.args.zero_stage != <span class="number">3</span> <span class="keyword">else</span> param.ds_shape</span><br><span class="line">        refs = [engine.update_weight.remote(name, dtype, shape, count==num_params) </span><br><span class="line">               <span class="keyword">for</span> engine <span class="keyword">in</span> <span class="variable language_">self</span>.vllm_engines]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ZeRO-3 参数全收集 + 广播</span></span><br><span class="line">    <span class="keyword">with</span> deepspeed.zero.GatheredParameters([param], enabled=<span class="variable language_">self</span>.strategy.args.zero_stage == <span class="number">3</span>):</span><br><span class="line">        <span class="keyword">if</span> torch.distributed.get_rank() == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">if</span> use_ray:</span><br><span class="line">                collective.broadcast(param.data, <span class="number">0</span>, group_name=<span class="variable language_">self</span>._model_update_group)  <span class="comment"># Ray Collective</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                torch.distributed.broadcast(param.data, <span class="number">0</span>, group=<span class="variable language_">self</span>._model_update_group)  <span class="comment"># PyTorch DDP</span></span><br><span class="line">            ray.get(refs)  <span class="comment"># 等待vLLM更新完成</span></span><br></pre></td></tr></table></figure><h5 id="cuda-ipc-高速通信">CUDA IPC 高速通信</h5><p>条件：<strong>Actor和Rollout colocate</strong>，即采用<code>colocate_all_models</code>策略</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> backend == <span class="string">&quot;nccl&quot;</span> <span class="keyword">and</span> <span class="variable language_">self</span>.strategy.args.colocate_all_models:</span><br><span class="line">        <span class="variable language_">self</span>.use_cuda_ipc = <span class="literal">True</span></span><br></pre></td></tr></table></figure><p>当两个模块时 colocate 到一张卡上时，NCCL 无法做同一张卡上两个进程的通信，所以需要<a href="https://link.zhihu.com/?target=https%3A//github.com/OpenRLHF/OpenRLHF/blob/17bbb313551a3af3cdd213d8b9e7522fe9c6271b/openrlhf/trainer/ray/ppo_actor.py%23L223-L232">用 CUDA IPC 做进程间通信</a>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">with</span> deepspeed.zero.GatheredParameters([param], enabled=<span class="variable language_">self</span>.strategy.args.zero_stage == <span class="number">3</span>):</span><br><span class="line">        weight = param.data.clone()</span><br><span class="line">        ipc_handle = reduce_tensor(weight)  <span class="comment"># 生成IPC内存句柄</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 全收集所有Rank的IPC句柄</span></span><br><span class="line">        ipc_handle = &#123;get_physical_gpu_id(): ipc_handle&#125;</span><br><span class="line">        ipc_handle_list = [<span class="literal">None</span>] * torch.distributed.get_world_size()</span><br><span class="line">        torch.distributed.all_gather_object(ipc_handle_list, ipc_handle)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Rank 0 通过IPC更新vLLM</span></span><br><span class="line">        <span class="keyword">if</span> torch.distributed.get_rank() == <span class="number">0</span>:</span><br><span class="line">            ipc_handles = &#123;&#125;</span><br><span class="line">            <span class="keyword">for</span> d <span class="keyword">in</span> ipc_handle_list: ipc_handles.update(d)  <span class="comment"># 合并所有GPU的句柄</span></span><br><span class="line">            </span><br><span class="line">            refs = [engine.update_weight_cuda_ipc.remote(name, dtype, shape, ipc_handles, count==num_params)</span><br><span class="line">                   <span class="keyword">for</span> engine <span class="keyword">in</span> <span class="variable language_">self</span>.vllm_engines]</span><br><span class="line">            ray.get(refs)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 同步点</span></span><br><span class="line">        torch.distributed.barrier()</span><br><span class="line">        torch.cuda.synchronize()</span><br></pre></td></tr></table></figure><blockquote><p>ZeRO-3并行：</p><ol type="1"><li>先在 Actor workers 内部 all_gather 权重；</li><li>再由 rank0 代表 Actor 向所有 Rollout 实例 broadcast 权重。</li></ol></blockquote><h2 id="参考">参考</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/html/2405.11143v4">OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework</a></p><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.19256">HybridFlow: A Flexible and Efficient RLHF Framework</a></p><p><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Single_program,_multiple_data">Single program, multiple data</a></p><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/29046833667">品鉴一下OpenRLHF和verl的系统设计</a></p><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/26833089345">基于 Ray 的分离式架构：veRL、OpenRLHF 工程设计</a></p><p><a target="_blank" rel="noopener" href="https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/readme.md">HybridFlow veRL 原文浅析</a></p><p><a target="_blank" rel="noopener" href="https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/OpenRLHF/readme.md#更新流程">浅析以 OpenRLHF 为代表的 post-training 系统的计算流程</a></p><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/12871616401">图解OpenRLHF中基于Ray的分布式训练流程</a></p><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/1902472584827732882">分布式RLHF武庙十哲下 - 手抓饼熊的文章 - 知乎</a></p></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://wenliuyi.github.io">Liuyi Wen</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://wenliuyi.github.io/posts/2ac460b1.html">http://wenliuyi.github.io/posts/2ac460b1.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://wenliuyi.github.io" target="_blank">Liuyi Wen's Blog</a>！</span></div></div><div class="tag_share"><div class="post-share"><div class="social-share" data-image="/img/WechatIMG105.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/posts/5d9f220e.html" title="verl框架：1. Ray集群介绍+verl中基于Ray的执行流程解析"><div class="cover" style="background:var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">verl框架：1. Ray集群介绍+verl中基于Ray的执行流程解析</div></div><div class="info-2"><div class="info-item-1">现代计算机体系结构 现代计算机体系结构如下： 多核：一台计算机上有多颗CPU，每个 CPU 有多个计算核心。CPU内部有缓存结构，外部有主存。 集群：多台计算机通过高速网络互联，每台计算机上配有至少一块高速网卡。使得不同节点之间互相访问数据就像在单个节点一样。 异构计算：CPU 和主存通常被称为主机（Host），各类专用的加速器被称为设备（Device）。当前基于 GPU 的异构计算是主流，GPU 有区别于 CPU 的芯片微架构和编译软件栈。 软件层面：GPU 提供了 CUDA编程接口； 硬件层面：GPU 有很多个专用计算核心，和 GPU...</div></div></div></a><a class="pagination-related" href="/posts/76aa9d6e.html" title="Transformer的KV Cache"><div class="cover" style="background:var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">Transformer的KV Cache</div></div><div class="info-2"><div class="info-item-1">Problems 对于LLMs，每次矩阵乘法都由若干个浮点运算组成，因此其性能受限于GPU的FLOPS；随着输入的token长度增加，Transformer的自注意力机制与输入序列长度呈平方关系增长，产生最大的延迟开销。 为了解决推理延迟和吞吐量问题，当前的大模型服务系统通常采用KV Cache：通过缓存已计算的Key和Value矩阵，以避免在解码阶段重复计算键和值的投影（空间换时间）。然而在以下场景中KV Cache占用内存较大，影响推理性能： 处理长序列或多轮对话； 对于多个客户端请求，每个请求分别保留各自的KV Cache。 KV Cache的核心问题在于：占用大量内存和访存带宽；在生成阶段引入大量重复计算。本篇博客探讨KV Cache压缩技术。 Backgrounds 推理加速的衡量指标如下： 吞吐量：每生成一个token，服务商需要支付的算力成本。可以通过tokens per second(tps)衡量，即推理服务器单位时间内能处理针对所有用户和请求生成的输出token数。 延迟：包括两个指标： TTFT（Time To...</div></div></div></a></nav><hr class="custom-hr"><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/img/WechatIMG105.jpg" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info-name">Liuyi Wen</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">37</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">5</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/WenLiuyi"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">The Journey Is the Reward.</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#spmd-mpmd"><span class="toc-number">1.</span> <span class="toc-text">SPMD-&gt;MPMD</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#openrlhf%E7%9A%84ray%E6%B5%81%E7%A8%8B"><span class="toc-number">2.</span> <span class="toc-text">OpenRLHF的Ray流程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#driver-process"><span class="toc-number">2.1.</span> <span class="toc-text">Driver Process</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ray%E8%BF%9C%E7%A8%8B%E8%B0%83%E7%94%A8fit%E5%BC%80%E5%A7%8B%E8%AE%AD%E7%BB%83"><span class="toc-number">2.2.</span> <span class="toc-text">Ray远程调用fit，开始训练</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#step1.-%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90%E5%B0%86%E8%BF%99%E4%B8%AA-batch-%E7%9A%84-prompts-%E8%BE%93%E5%85%A5%E7%BB%99-actorrollout-%E5%BE%97%E5%88%B0-responses"><span class="toc-number">2.2.1.</span> <span class="toc-text">Step1. 样本生成：将这个 batch 的 prompts 输入给 Actor，rollout 得到 responses</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#step2.-%E6%94%B6%E9%9B%86experiences%E4%BB%8Erefrewardcritic%E4%B8%8A%E6%94%B6%E9%9B%86%E5%B9%B6%E5%A4%84%E7%90%86exps"><span class="toc-number">2.2.2.</span> <span class="toc-text">Step2. 收集experiences：从Ref&#x2F;Reward&#x2F;Critic上收集并处理exps</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#step3.-%E7%A1%AE%E4%BF%9D%E5%B0%86%E5%A4%84%E7%90%86%E5%90%8E%E7%9A%84exps%E4%BC%A0%E9%80%81%E7%BB%99critic%E5%B9%B6%E8%A1%8C%E6%89%A7%E8%A1%8Cactor%E5%92%8Ccritic%E7%9A%84%E8%AE%AD%E7%BB%83"><span class="toc-number">2.2.3.</span> <span class="toc-text">Step3. 确保将处理后的exps传送给Critic，并行执行Actor和Critic的训练</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#step4.-vllm_engine%E6%9D%83%E9%87%8D%E6%9B%B4%E6%96%B0"><span class="toc-number">2.2.4.</span> <span class="toc-text">Step4. vllm_engine权重更新</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E6%B5%81%E7%A8%8B"><span class="toc-number">2.3.</span> <span class="toc-text">总流程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#openrlhf%E7%9A%84colocate%E7%AD%96%E7%95%A5"><span class="toc-number">3.</span> <span class="toc-text">OpenRLHF的Colocate策略</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86"><span class="toc-number">3.1.</span> <span class="toc-text">原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%83%A8%E7%BD%B2actorrefcriticrm%E5%AE%9E%E4%BE%8B"><span class="toc-number">3.2.</span> <span class="toc-text">部署Actor&#x2F;Ref&#x2F;Critic&#x2F;RM实例</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9D%9E%E5%85%B1%E5%90%8C%E9%83%A8%E7%BD%B2"><span class="toc-number">3.2.1.</span> <span class="toc-text">非共同部署</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B1%E5%90%8C%E9%83%A8%E7%BD%B2"><span class="toc-number">3.2.2.</span> <span class="toc-text">共同部署</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%83%A8%E7%BD%B2vllm_engines%E5%AE%9E%E4%BE%8B"><span class="toc-number">3.3.</span> <span class="toc-text">部署vllm_engines实例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ds_rank0%E4%B8%8Evllm_ranks%E4%B9%8B%E9%97%B4%E7%9A%84%E9%80%9A%E4%BF%A1"><span class="toc-number">3.4.</span> <span class="toc-text">ds_rank0与vllm_ranks之间的通信</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E9%80%9A%E4%BF%A1%E7%BB%84"><span class="toc-number">3.4.1.</span> <span class="toc-text">创建通信组</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B9%BF%E6%92%ADppo-actor%E6%9D%83%E9%87%8D%E5%88%B0all_vllm_ranks"><span class="toc-number">3.4.2.</span> <span class="toc-text">广播PPO-Actor权重到all_vllm_ranks</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E9%BB%98%E8%AE%A4%E5%B8%B8%E8%A7%84%E5%B9%BF%E6%92%ADray-collective-%E6%88%96-pytorch-ddp"><span class="toc-number">3.4.2.1.</span> <span class="toc-text">默认：常规广播（Ray Collective 或 PyTorch DDP）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#cuda-ipc-%E9%AB%98%E9%80%9F%E9%80%9A%E4%BF%A1"><span class="toc-number">3.4.2.2.</span> <span class="toc-text">CUDA IPC 高速通信</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83"><span class="toc-number">4.</span> <span class="toc-text">参考</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/3858f068.html" title="并行训练系列：2. 数据并行上篇（DP，DDP）">并行训练系列：2. 数据并行上篇（DP，DDP）</a><time datetime="2025-09-16T07:43:28.000Z" title="发表于 2025-09-16 15:43:28">2025-09-16</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/ab0f7bb9.html" title="RL 系列：2. 从 Bellman 算子的角度解释策略迭代/价值迭代">RL 系列：2. 从 Bellman 算子的角度解释策略迭代/价值迭代</a><time datetime="2025-09-15T08:22:53.000Z" title="发表于 2025-09-15 16:22:53">2025-09-15</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/ba1e1693.html" title="RL 系列：1. Markov 决策过程">RL 系列：1. Markov 决策过程</a><time datetime="2025-09-08T09:29:49.000Z" title="发表于 2025-09-08 17:29:49">2025-09-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/b1ea32a.html" title="Go 的长连接">Go 的长连接</a><time datetime="2025-09-03T03:20:39.000Z" title="发表于 2025-09-03 11:20:39">2025-09-03</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/cda30c10.html" title="Go-内存分配器">Go-内存分配器</a><time datetime="2025-07-18T13:37:10.000Z" title="发表于 2025-07-18 21:37:10">2025-07-18</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By Liuyi Wen</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(()=>{const t=()=>{if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"all"},chtml:{scale:1.1},options:{enableMenu:!0,renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const n=!!e.type.match(/; *mode=display/),a=new t.options.MathItem(e.textContent,t.inputJax[0],n),d=document.createTextNode("");e.parentNode.replaceChild(d,e),a.start={node:d,delim:"",n:0},a.end={node:d,delim:"",n:0},t.math.push(a)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}};btf.addGlobalFn("encrypt",t,"mathjax"),window.pjax?t():window.addEventListener("load",t)})()</script><script>(()=>{const n="shuoshuo"===GLOBAL_CONFIG_SITE.pageType,e=(e,o)=>{n&&(window.shuoshuoComment.destroyValine=()=>{e.children.length&&(e.innerHTML="",e.classList.add("no-comment"))});const t={el:"#vcomment",appId:"bsxtUJWr1muoPS1pmoXLOPZ2-gzGzoHsz",appKey:"wm2wUYvKLEySwyRnFn7xAbJI",avatar:"monsterid",serverURLs:"",emojiMaps:"",visitor:!1,path:n?o:window.location.pathname};new Valine(t)},o=async(n,o)=>{"function"==typeof Valine||await btf.getScript("https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"),e(n,o)};n?window.shuoshuoComment={loadComment:o}:btf.loadComment(document.getElementById("vcomment"),o)})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-info"><div class="algolia-stats"></div><div class="algolia-poweredBy"></div></div></div></div></div><div id="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/algoliasearch/dist/lite/builds/browser.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instantsearch.js/dist/instantsearch.production.min.js"></script><script src="/js/search/algolia.js"></script></div></div>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body></html>