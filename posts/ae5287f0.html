<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>VLM-R1源码解析 | Liuyi Wen's Blog</title><meta name="author" content="Liuyi Wen"><meta name="copyright" content="Liuyi Wen"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="VLM-R1基于TRL框架。 GROP方法 GRPO 是一种在线学习算法，它通过使用训练模型本身在训练期间生成的数据进行迭代改进。其理念是：最大程度利用生成补全，同时确保模型始终接近参考策略。 分为4个步骤：生成补全、计算奖励、估计KL散度、计算损失。  要点 引入critic：使用预测baseline改进奖励 使用绝对奖励，单纯比较大小，会导致：奖励波动大，以及对部分改进的激励不足；因此引入Cr"><meta property="og:type" content="article"><meta property="og:title" content="VLM-R1源码解析"><meta property="og:url" content="http://wenliuyi.github.io/posts/ae5287f0.html"><meta property="og:site_name" content="Liuyi Wen&#39;s Blog"><meta property="og:description" content="VLM-R1基于TRL框架。 GROP方法 GRPO 是一种在线学习算法，它通过使用训练模型本身在训练期间生成的数据进行迭代改进。其理念是：最大程度利用生成补全，同时确保模型始终接近参考策略。 分为4个步骤：生成补全、计算奖励、估计KL散度、计算损失。  要点 引入critic：使用预测baseline改进奖励 使用绝对奖励，单纯比较大小，会导致：奖励波动大，以及对部分改进的激励不足；因此引入Cr"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://wenliuyi.github.io/img/butterfly-icon.png"><meta property="article:published_time" content="2025-03-28T02:45:42.000Z"><meta property="article:modified_time" content="2025-05-20T03:46:28.000Z"><meta property="article:author" content="Liuyi Wen"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="http://wenliuyi.github.io/img/butterfly-icon.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "VLM-R1源码解析",
  "url": "http://wenliuyi.github.io/posts/ae5287f0.html",
  "image": "http://wenliuyi.github.io/img/butterfly-icon.png",
  "datePublished": "2025-03-28T02:45:42.000Z",
  "dateModified": "2025-05-20T03:46:28.000Z",
  "author": [
    {
      "@type": "Person",
      "name": "Liuyi Wen",
      "url": "http://wenliuyi.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://wenliuyi.github.io/posts/ae5287f0.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>(()=>{const e={set:(e,t,o)=>{if(!o)return;const a=Date.now()+864e5*o;localStorage.setItem(e,JSON.stringify({value:t,expiry:a}))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const{value:o,expiry:a}=JSON.parse(t);if(!(Date.now()>a))return o;localStorage.removeItem(e)}};window.btf={saveToLocal:e,getScript:(e,t={})=>new Promise((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,Object.entries(t).forEach(([e,t])=>n.setAttribute(e,t)),n.onload=n.onreadystatechange=()=>{n.readyState&&!/loaded|complete/.test(n.readyState)||o()},n.onerror=a,document.head.appendChild(n)}),getCSS:(e,t)=>new Promise((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onload=n.onreadystatechange=()=>{n.readyState&&!/loaded|complete/.test(n.readyState)||o()},n.onerror=a,document.head.appendChild(n)}),addGlobalFn:(e,t,o=!1,a=window)=>{if(e.startsWith("pjax"))return;const n=a.globalFn||{};n[e]=n[e]||{},n[e][o||Object.keys(n[e]).length]=t,a.globalFn=n}};const t=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},o=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};btf.activateDarkMode=t,btf.activateLightMode=o;const a=e.get("theme");"dark"===a?t():"light"===a&&o();const n=e.get("aside-status");void 0!==n&&document.documentElement.classList.toggle("hide-aside","hide"===n);/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})()</script><script>const GLOBAL_CONFIG={root:"/",algolia:{appId:"VE36MEFVE6",apiKey:"f9b9ca5a3cdb9455658600dba6ae7706",indexName:"hexo-algolia indexing key",hitsPerPage:6,languages:{input_placeholder:"搜索文章",hits_empty:"未找到符合您查询的内容：${query}",hits_stats:"找到 ${hits} 条结果，耗时 ${time} 毫秒"}},localSearch:void 0,translate:void 0,highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:!1,highlightFullpage:!1,highlightMacStyle:!1},copy:{success:"复制成功",error:"复制失败",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"null",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!1,islazyloadPlugin:!1,isAnchor:!1,percent:{toc:!0,rightside:!1},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"VLM-R1源码解析",isHighlightShrink:!1,isToc:!0,pageType:"post"}</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Liuyi Wen's Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">VLM-R1源码解析</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div></div></nav><div id="post-info"><h1 class="post-title">VLM-R1源码解析</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-03-28T02:45:42.000Z" title="发表于 2025-03-28 10:45:42">2025-03-28</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-05-20T03:46:28.000Z" title="更新于 2025-05-20 11:46:28">2025-05-20</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Introduction-to-AI/">Introduction to AI</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><p>VLM-R1基于TRL框架。</p><h2 id="grop方法">GROP方法</h2><p>GRPO 是一种在线学习算法，它通过使用训练模型本身在训练期间生成的数据进行迭代改进。其理念是：最大程度利用生成补全，同时确保模型始终接近参考策略。</p><p>分为4个步骤：生成补全、计算奖励、估计KL散度、计算损失。 <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ae5287f0/image.png"></p><h3 id="要点">要点</h3><h4 id="引入critic使用预测baseline改进奖励">引入critic：使用预测baseline改进奖励</h4><p>使用绝对奖励，单纯比较大小，会导致：奖励波动大，以及对部分改进的激励不足；因此<strong>引入Critic：使用“预测baseline”来改进奖励</strong></p><p>对于给定状态(<span class="math inline">\(s_t\)</span>)和动作(<span class="math inline">\(o_t\)</span>)，该baseline为<strong>价值函数<span class="math inline">\((V_\psi(s))\)</span></strong>；训练目标由单纯的reward，转为超过该baseline的程度，由优势函数表示为： <span class="math display">\[ A_t=r_t-V_\psi(s_t) \]</span> 即训练中优化内容为： <span class="math display">\[ \mathcal{J}_{adv}(\theta)=\mathbb{E}[A(o)] \\ where A(o)=r(o)-V_\psi(o) \]</span> 通过减去baseline，降低了训练中的方差，为超出预期的动作提供更高的梯度信号；并对未达标的动作进行惩罚。</p><h4 id="添加裁剪和最小值操作防止过度更新">添加裁剪和最小值操作：防止过度更新</h4><p>适度控制学习策略的更新程度：若单次更新太多，则可能走向极端值；若单次更新太少，则动力不足。应找到一个平衡点。</p><p>在Proximal Policy Optimization (PPO)中最大化以下目标函数： <span class="math display">\[ \mathcal{J}_{adv}(\theta)=\mathbb{E}[q\sim P(Q), o\sim\pi_{\theta_{old}}(O|q)]\frac{1}{|o|}\sum_{t=1}^{|o|}\min[r_t(\theta)A_t, clip(r_t(\theta),1-\epsilon,1+\epsilon)A_t] \]</span> 其中： <span class="math display">\[ r_t(\theta)=\frac{\pi_{\theta}(o_t|q,o_{&lt;t})}{\pi_{\theta_{old}}(o_t|q,o_{&lt;t})} \]</span> <span class="math inline">\(\pi_{\theta}\)</span>和<span class="math inline">\(\pi_{\theta_{old}}\)</span>分别是：当前策略模型、旧策略模型；<span class="math inline">\(q\)</span>和<span class="math inline">\(o\)</span>是从问题数据集和旧策略<span class="math inline">\(\pi_{\theta_{old}}\)</span>中采样的问题和输出；超参数<span class="math inline">\(\epsilon\)</span>用于稳定训练过程；优势<span class="math inline">\(A_i\)</span>通过广义优势估计（GAE）计算。</p><h4 id="防止作弊和极端策略使用kl散度">防止作弊和极端策略：使用KL散度</h4><p>如果只专注于最大化目标函数，可能会采用可疑手段，即生成有害或虚假内容，以人为提高某些奖励指标。为了减轻对奖励模型的过度优化，标准方法是：在每个标记的奖励中，<strong>添加一个来自参考模型（初始策略）的每个标记的KL惩罚</strong>，即： <span class="math display">\[ r_t=r_\varphi(q,o_{\leq t})-\beta\log\frac{\pi_\theta(o_t|q,o_{&lt;t})}{\pi_{ref}(o_t|q,o_{&lt;t})} \]</span> 其中，<span class="math inline">\(r\)</span>是奖励模型，<span class="math inline">\(\pi_{ref}\)</span>是参考模型，通常是初始的监督微调（SFT）模型，而<span class="math inline">\(\beta\)</span>是KL惩罚项的系数。</p><p>然而，PPO 中的<strong>Critic（值函数）通常是一个与Actor（策略模型）大小相当的模型</strong>，这带来了显著的内存和计算负担；此外，在 LLMs 的上下文中，Critic在训练过程中被用作优势计算中的Baseline，但通常只有最后一个 token 会被奖励模型赋予奖励分数，这可能使得Critic的训练变得复杂。</p><p>为了解决这些问题，提出了 Group Relative Policy Optimization (GRPO)，不再需要像PPO那样加入额外的Critic近似，而是<strong>直接使用多个采样输出的平均奖励作为Baseline</strong>，显著减少了训练资源的使用。</p><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ae5287f0/image-2.png"></p><p>对于每个问题<span class="math inline">\(i\)</span>，GRPO从旧策略<span class="math inline">\(\pi_{\theta_{old}}\)</span>中，采样出一组输出{<span class="math inline">\(i_1\)</span>,...,<span class="math inline">\(i_A\)</span>}，通过最大化以下目标函数优化策略模型： <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ae5287f0/image-3.png"></p><p>其中，<span class="math inline">\(\epsilon\)</span> 和 <span class="math inline">\(\beta\)</span> 是超参数，<span class="math inline">\(\hat{A}_{i,t}\)</span>​ 是基于组内奖励的相对优势估计。</p><p>与 PPO 不同，GRPO： 1. 直接使用奖励模型的输出来估计baseline，避免了训练一个复杂的Critic； 2. 直接在损失函数中加入策略模型和参考模型之间的 KL 散度来正则化，而不是在奖励中加入 KL 惩罚项，从而简化了训练过程。</p><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ae5287f0/image-4.png"></p><p>使用下面的无偏估计来估计 KL 散度： <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/ae5287f0/image-5.png"></p><p>GRPO核心思想： 1. 无需为 Critic 设置单独的价值网络； 2. 对同一问题或状态，从旧策略中采样多个输出； 3. 将这些输出的平均奖励视为基准； 4. 任何高于平均值的都产生“正优势”，任何低于平均值的都产生“负优势”。</p><p>同时，GRPO保留了 PPO 的裁剪和 KL 机制，以确保稳定、合规的更新。</p><h2 id="自定义训练器vlmgrpotrainer">自定义训练器<code>VLMGRPOTrainer</code></h2><p>基于 <code>Trainer</code> 类扩展，实现了 Group Relative Policy Optimization (GRPO) 方法的训练，该方法首次在论文<a target="_blank" rel="noopener" href="https://huggingface.co/papers/2402.03300">DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models</a>中提出。</p><h3 id="初始化">初始化</h3><blockquote><p>默认：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">attn_implementation: <span class="built_in">str</span> = <span class="string">&quot;flash_attention_2&quot;</span>,</span><br><span class="line">torch_dtype: <span class="built_in">str</span> = <span class="string">&quot;bfloat16&quot;</span>,</span><br></pre></td></tr></table></figure><p></p></blockquote><ol type="1"><li><p><strong>加载预训练模型</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model_cls = <span class="variable language_">self</span>.vlm_module.get_model_class(model_id, model_init_kwargs)</span><br><span class="line">model = model_cls.from_pretrained(model_id, **model_init_kwargs)</span><br></pre></td></tr></table></figure><p></p></li><li><p><strong>PEFT配置</strong>:如果提供了 PEFT 配置，查找<strong>模型中的所有线性层（不包括视觉模块）</strong>，并<strong>将它们应用于 LoRA微调</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> peft_config <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">  target_modules = find_all_linear_names(model, <span class="variable language_">self</span>.vision_modules_keywords)</span><br><span class="line">  peft_config.target_modules = target_modules</span><br><span class="line">  model = get_peft_model(model, peft_config)</span><br></pre></td></tr></table></figure><p></p></li></ol><blockquote><p>寻找Pytorch中的线性层：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">find_all_linear_names</span>(<span class="params">model, multimodal_keywords</span>):</span><br><span class="line">   cls = torch.nn.Linear</span><br><span class="line">   lora_module_names = <span class="built_in">set</span>()</span><br><span class="line">   <span class="keyword">for</span> name, module <span class="keyword">in</span> model.named_modules():</span><br><span class="line">       <span class="comment"># 不在视觉模块上应用LoRA微调</span></span><br><span class="line">       <span class="keyword">if</span> <span class="built_in">any</span>(mm_keyword <span class="keyword">in</span> name <span class="keyword">for</span> mm_keyword <span class="keyword">in</span> multimodal_keywords):</span><br><span class="line">           <span class="keyword">continue</span></span><br><span class="line">       <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, cls):    <span class="comment"># 检查是否为线性层（全连接层）</span></span><br><span class="line">           lora_module_names.add(name)</span><br><span class="line">   <span class="keyword">for</span> m <span class="keyword">in</span> lora_module_names:  <span class="comment"># 移除嵌入层</span></span><br><span class="line">       <span class="keyword">if</span> <span class="string">&quot;embed_tokens&quot;</span> <span class="keyword">in</span> m:</span><br><span class="line">           lora_module_names.remove(m)</span><br><span class="line">   <span class="keyword">return</span> <span class="built_in">list</span>(lora_module_names)</span><br></pre></td></tr></table></figure><p></p></blockquote><ol start="3" type="1"><li><p><strong>冻结视觉模块</strong>：如果 <code>freeze_vision_modules</code> 为 <code>True</code>，冻结所有视觉模块的参数，<strong>不进行梯度更新</strong>；</p></li><li><p><strong>启用梯度检查点（如果需要）</strong>：<strong>在反向传播时，仅存储部分中间激活值，来减少训练过程中对内存的需求。</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.gradient_checkpointing:</span><br><span class="line">    model = <span class="variable language_">self</span>._enable_gradient_checkpointing(model, args)</span><br></pre></td></tr></table></figure><p></p></li></ol><p>看看启用梯度检查点的函数：<code>_enable_gradient_checkpointing</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_enable_gradient_checkpointing</span>(<span class="params">self, model: PreTrainedModel, args: GRPOConfig</span>) -&gt; PreTrainedModel:</span><br><span class="line">  <span class="comment"># 1. 禁用cache：禁用缓存中间激活值</span></span><br><span class="line">  model.config.use_cache = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 2. 对于 PEFT 模型启用梯度检查点（PEFT 模型通常通过添加适配器层进行微调，其他大部分参数保持冻结）</span></span><br><span class="line">  <span class="keyword">if</span> is_peft_model(model):</span><br><span class="line">      model.base_model.gradient_checkpointing_enable()</span><br><span class="line">  <span class="comment"># 3. 对于非 PEFT 模型启用梯度检查点：</span></span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">try</span>:</span><br><span class="line">          model.gradient_checkpointing_enable()</span><br><span class="line">      <span class="keyword">except</span>:</span><br><span class="line">          <span class="string">&#x27;&#x27;&#x27;首先尝试通过 model.gradient_checkpointing_enable() 启用梯度检查点。如果失败（例如，某些模型不支持该操作），则进行特定模型的特殊处理：</span></span><br><span class="line"><span class="string">            1. 启用 视觉模型 和其 编码器 的梯度检查点；</span></span><br><span class="line"><span class="string">            2. 在 InternVL 模型中，还需要禁用 gradient_checkpointing 参数，以避免出现不支持梯度检查点操作的错误。</span></span><br><span class="line"><span class="string">          &#x27;&#x27;&#x27;</span></span><br><span class="line">          model.language_model.config.use_cache = <span class="literal">False</span></span><br><span class="line">          model.vision_model.gradient_checkpointing = <span class="literal">True</span></span><br><span class="line">          model.vision_model.encoder.gradient_checkpointing = <span class="literal">True</span></span><br><span class="line">          model.language_model._set_gradient_checkpointing()</span><br><span class="line">          args.gradient_checkpointing = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 4. 启用梯度计算：若use_reentrant为True（默认为True），启用输入张量的梯度计算</span></span><br><span class="line">  gradient_checkpointing_kwargs = args.gradient_checkpointing_kwargs <span class="keyword">or</span> &#123;&#125;</span><br><span class="line">  use_reentrant = (</span><br><span class="line">      <span class="string">&quot;use_reentrant&quot;</span> <span class="keyword">not</span> <span class="keyword">in</span> gradient_checkpointing_kwargs <span class="keyword">or</span> gradient_checkpointing_kwargs[<span class="string">&quot;use_reentrant&quot;</span>]</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> use_reentrant:</span><br><span class="line">      model.enable_input_require_grads()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><p></p><ol start="5" type="1"><li><strong>加载参考模型</strong>：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> is_deepspeed_zero3_enabled():</span><br><span class="line">  <span class="variable language_">self</span>.ref_model = model_cls.from_pretrained(model_id, **model_init_kwargs)</span><br><span class="line"><span class="keyword">elif</span> peft_config <span class="keyword">is</span> <span class="literal">None</span>:   <span class="comment"># 不使用PEFT配置，从初始模型创建一个参考模型</span></span><br><span class="line">  <span class="comment"># If PEFT configuration is not provided, create a reference model based on the initial model.</span></span><br><span class="line">  <span class="variable language_">self</span>.ref_model = create_reference_model(model)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">  <span class="comment"># If PEFT is used, the reference model is not needed since the adapter can be disabled</span></span><br><span class="line">  <span class="comment"># to revert to the initial model.</span></span><br><span class="line">  <span class="variable language_">self</span>.ref_model = <span class="literal">None</span></span><br></pre></td></tr></table></figure></li></ol><p><code>create_reference_model</code>在<code>TRL</code>框架中定义：<strong>创建一个静态的参考模型（参考模型是原始模型的副本），其部分参数被冻结，以便在训练过程中不再更新</strong>。</p><blockquote><p>冻结参数： * <strong>冻结所有参数</strong>：如果没有指定共享层数（<code>num_shared_layers</code> 为 <code>None</code>），则函数会冻结所有的参数。这意味着参考模型的所有参数都不会在训练中被更新，通常这种方法用于构建一个固定的基准模型或用于知识蒸馏等场景。 * <strong>冻结部分参数（共享层）</strong>：如果指定了 <code>num_shared_layers</code>，则函数会根据该值冻结模型的前几个层（根据层数或指定的层名模式）。这意味着这些层的参数不会更新，从而让模型只对后面的层进行训练。</p></blockquote><p>我们一起看看：<a target="_blank" rel="noopener" href="https://github.com/huggingface/trl/blob/d625c5533a6b1c84d3565c8080857f6bb81c538a/trl/models/modeling_base.py#L605">trl/trl/models /modeling_base.py</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_reference_model</span>(<span class="params"></span></span><br><span class="line"><span class="params">    model: PreTrainedModelWrapper, num_shared_layers: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>, pattern: <span class="type">Optional</span>[<span class="built_in">str</span>] = <span class="literal">None</span></span></span><br><span class="line"><span class="params"></span>) -&gt; PreTrainedModelWrapper:</span><br><span class="line">    <span class="comment"># 1. 兼容性检查：不支持DeepSpeed 的 ZeRO-3 模式</span></span><br><span class="line">    <span class="keyword">if</span> is_deepspeed_zero3_enabled():</span><br><span class="line">        <span class="keyword">raise</span> ValueError(</span><br><span class="line">            <span class="string">&quot;DeepSpeed ZeRO-3 is enabled and is not compatible with `create_reference_model()`. Please instantiate your reference model directly with `AutoModelForCausalLM.from_pretrained()`.&quot;</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    parameter_names = [n <span class="keyword">for</span> n, _ <span class="keyword">in</span> model.named_parameters()]</span><br><span class="line">    <span class="comment"># 2. 复制模型（深拷贝）</span></span><br><span class="line">    ref_model = deepcopy(model)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. 如果没有共享层，则冻结所有参数并返回模型副本：（通常用于构建一个固定的基准模型，或用于知识蒸馏等场景）</span></span><br><span class="line">    <span class="keyword">if</span> num_shared_layers <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">for</span> param_name <span class="keyword">in</span> parameter_names:</span><br><span class="line">            param = ref_model.get_parameter(param_name)</span><br><span class="line">            param.requires_grad = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> ref_model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4. 如果有共享层，确定共享层的选择模式：</span></span><br><span class="line">    <span class="comment"># 如果指定了 pattern 参数，则使用该模式来选择共享层；否则，函数会遍历 LAYER_PATTERNS 中的模式，选择一个能够匹配参数名称的模式。</span></span><br><span class="line">    <span class="keyword">if</span> pattern <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        pattern = pattern.<span class="built_in">format</span>(layer=num_shared_layers)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">for</span> pattern_candidate <span class="keyword">in</span> LAYER_PATTERNS:</span><br><span class="line">            pattern_candidate = pattern_candidate.<span class="built_in">format</span>(layer=num_shared_layers)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">any</span>(pattern_candidate <span class="keyword">in</span> name <span class="keyword">for</span> name <span class="keyword">in</span> parameter_names):</span><br><span class="line">                pattern = pattern_candidate</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> pattern <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;Layer pattern could not be matched.&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5. 将参数分为共享和非共享两类：</span></span><br><span class="line">    shared_param_list = []</span><br><span class="line">    unshared_param_list = []</span><br><span class="line"></span><br><span class="line">    shared_parameter = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">for</span> name, _param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">        <span class="keyword">if</span> pattern <span class="keyword">in</span> name:</span><br><span class="line">            shared_parameter = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">if</span> shared_parameter:</span><br><span class="line">            shared_param_list.append(name)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            unshared_param_list.append(name)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 6. 冻结原始模型中所有属于共享层的参数：</span></span><br><span class="line">    <span class="keyword">for</span> param_name <span class="keyword">in</span> shared_param_list:</span><br><span class="line">        param = model.get_parameter(param_name)</span><br><span class="line">        param.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        _ref_param = ref_model.get_parameter(param_name)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 7. 冻结参考模型中所有属于非共享层的参数：</span></span><br><span class="line">    <span class="keyword">for</span> param_name <span class="keyword">in</span> unshared_param_list:</span><br><span class="line">        param = ref_model.get_parameter(param_name)</span><br><span class="line">        param.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> pattern <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> <span class="built_in">len</span>(unshared_param_list) == <span class="number">0</span>:</span><br><span class="line">        logging.warning(<span class="string">&quot;Pattern passed or found, but no layers matched in the model. Check for a typo.&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 8. 返回参考模型</span></span><br><span class="line">    <span class="keyword">return</span> ref_model.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure><ol start="6" type="1"><li><strong>处理类（<code>processing_class</code>）初始化</strong>：</li></ol><ul><li><p>如果<code>processing_class</code>参数为空：从<code>vlm_module</code>模块中获取相应的处理类，并从预训练模型中加载。</p></li><li><p>随后，处理类根据<code>kwargs</code>设置自定义的处理关键词：</p><ul><li>如果该处理类有 tokenizer，则设置其填充和结束标记 ID；</li><li>否则进行类型检查，确保该处理类是<code>PreTrainedTokenizerBase</code>类型。</li></ul></li></ul><ol start="7" type="1"><li><strong>对主模型、参考模型进行初始化</strong>：</li></ol><ul><li><code>Qwen2VLModule</code>无需处理；<code>InvernVLModule</code>需要从模型中提取并赋值到当前对象中，并且将图像上下文的 token ID 处理并存储到模型。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable language_">self</span>.vlm_module.post_model_init(model, processing_class)</span><br><span class="line"><span class="variable language_">self</span>.vlm_module.post_model_init(<span class="variable language_">self</span>.ref_model, processing_class)</span><br></pre></td></tr></table></figure></li></ul><ol start="8" type="1"><li><p><strong>奖励函数<code>reward_funcs</code>，奖励处理类<code>reward_processing_classes</code>初始化</strong></p></li><li><p><strong>处理奖励函数的 Tokenizer</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i, (reward_processing_class, reward_func) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(reward_processing_classes, reward_funcs)):</span><br><span class="line">  <span class="keyword">if</span> <span class="built_in">isinstance</span>(reward_func, PreTrainedModel):</span><br><span class="line">      <span class="keyword">if</span> reward_processing_class <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">          reward_processing_class = AutoTokenizer.from_pretrained(reward_func.config._name_or_path)</span><br><span class="line">      <span class="keyword">if</span> reward_processing_class.pad_token_id <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">          reward_processing_class.pad_token = reward_processing_class.eos_token</span><br><span class="line">      <span class="comment"># 计算输入序列中：最新non-padded token的奖励</span></span><br><span class="line">      reward_func.config.pad_token_id = reward_processing_class.pad_token_id</span><br><span class="line">      reward_processing_classes[i] = reward_processing_class</span><br><span class="line"><span class="variable language_">self</span>.reward_processing_classes = reward_processing_classes</span><br></pre></td></tr></table></figure><p></p></li><li><p><strong>训练参数的初始化</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable language_">self</span>.max_prompt_length = args.max_prompt_length</span><br><span class="line">  <span class="variable language_">self</span>.max_prompt_length = <span class="literal">None</span></span><br><span class="line">  <span class="keyword">if</span> args.max_prompt_length <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">      warnings.warn(<span class="string">&quot;Setting max_prompt_length is currently not supported, it has been set to None&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="variable language_">self</span>.max_completion_length = args.max_completion_length  <span class="comment"># = |o_i| in the GRPO paper</span></span><br><span class="line">  <span class="variable language_">self</span>.num_generations = args.num_generations  <span class="comment"># = G in the GRPO paper</span></span><br><span class="line">  <span class="variable language_">self</span>.generation_config = GenerationConfig(</span><br><span class="line">      max_new_tokens=<span class="variable language_">self</span>.max_completion_length,</span><br><span class="line">      do_sample=<span class="literal">True</span>,  </span><br><span class="line">      temperature=<span class="number">1</span>,</span><br><span class="line">      pad_token_id=pad_token_id,</span><br><span class="line">  )</span><br><span class="line">  <span class="keyword">if</span> <span class="built_in">hasattr</span>(<span class="variable language_">self</span>.vlm_module, <span class="string">&quot;get_eos_token_id&quot;</span>): <span class="comment"># For InternVL</span></span><br><span class="line">      <span class="variable language_">self</span>.generation_config.eos_token_id = <span class="variable language_">self</span>.vlm_module.get_eos_token_id(processing_class)</span><br><span class="line">      <span class="built_in">print</span>(<span class="number">222</span>, <span class="variable language_">self</span>.vlm_module.get_eos_token_id(processing_class))</span><br><span class="line">  <span class="variable language_">self</span>.beta = args.beta</span><br><span class="line">  <span class="variable language_">self</span>.epsilon = args.</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 多步</span></span><br><span class="line">  <span class="variable language_">self</span>.num_iterations = args.num_iterations  <span class="comment"># = 𝜇 in the GRPO paper</span></span><br><span class="line">  <span class="comment"># 迭代步数（前向+反向传播）</span></span><br><span class="line">  <span class="variable language_">self</span>._step = <span class="number">0</span>    </span><br><span class="line">  <span class="comment"># _buffered_inputs缓存批次中的输入，避免重复生成</span></span><br><span class="line">  <span class="variable language_">self</span>._buffered_inputs = [<span class="literal">None</span>] * args.gradient_accumulation_steps</span><br></pre></td></tr></table></figure><p></p></li><li><p><strong>设置随机种子</strong></p></li><li><p><strong>初始化参考模型<code>ref_model</code></strong></p></li><li><p><strong>准备奖励函数</strong></p></li></ol><h2 id="训练过程">训练过程</h2><p>从脚本<code>grpo_rec.sh</code>开始：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">torchrun --nproc_per_node=<span class="string">&quot;8&quot;</span> \     // 每个节点上运行的进程数：使用的 GPU 数量</span><br><span class="line">    --nnodes=<span class="string">&quot;1&quot;</span> \                  // 训练的节点数</span><br><span class="line">    --node_rank=<span class="string">&quot;0&quot;</span> \               // 当前节点的排名</span><br><span class="line">    --master_addr=<span class="string">&quot;127.0.0.1&quot;</span> \     // 主节点的 IP 地址</span><br><span class="line">    --master_port=<span class="string">&quot;12346&quot;</span> \         // 主节点监听的端口号</span><br><span class="line">    src/open_r1/grpo_rec.py \       // 实际运行的训练脚本路径</span><br><span class="line">    --deepspeed local_scripts/zero3.json \      // 使用 DeepSpeed 来优化训练</span><br><span class="line">    --output_dir output/<span class="variable">$RUN_NAME</span> \</span><br><span class="line">    --model_name_or_path Qwen/Qwen2.5-VL-3B-Instruct \</span><br><span class="line">    --dataset_name data_config/rec.yaml \</span><br><span class="line">    --image_root &lt;your_image_root&gt; \</span><br><span class="line">    --max_prompt_length 1024 \</span><br><span class="line">    --num_generations 8 \           // 每个输入生成的输出数量</span><br><span class="line">    --per_device_train_batch_size 1 \</span><br><span class="line">    --gradient_accumulation_steps 2 \</span><br><span class="line">    --logging_steps 1 \</span><br><span class="line">    --bf16 \                        // 使用 bfloat16 数据类型</span><br><span class="line">    --torch_dtype bfloat16 \        // 张量数据类型为 bfloat16</span><br><span class="line">    --data_seed 42 \</span><br><span class="line">    --report_to wandb \</span><br><span class="line">    --gradient_checkpointing <span class="literal">false</span> \</span><br><span class="line">    --attn_implementation flash_attention_2 \</span><br><span class="line">    --num_train_epochs 2 \</span><br><span class="line">    --run_name <span class="variable">$RUN_NAME</span> \</span><br><span class="line">    --save_steps 100 \</span><br><span class="line">    --save_only_model <span class="literal">true</span></span><br></pre></td></tr></table></figure><p></p><p><code>grpo_rec.py</code>入口函数：</p><ol type="1"><li><p><strong>加载VLM模型</strong>：<code>vlm_module_cls = get_vlm_module(model_args.model_name_or_path)</code>，支持：<code>Qwen2VLModule</code>和<code>InvernVLModule</code></p></li><li><p><strong>加载奖励函数</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">reward_funcs_registry = &#123;</span><br><span class="line">    <span class="string">&quot;accuracy&quot;</span>: vlm_module_cls.iou_reward,</span><br><span class="line">    <span class="string">&quot;format&quot;</span>: vlm_module_cls.format_reward_rec,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p></li></ol><blockquote><p>检查模型输出格式：<code>format_reward_rec</code> * 检查是否包含<code>think</code> 和 <code>answer</code> 标签； * <code>answer</code>标签中内容符合一个特定的边界框格式（如 <code>[x, y, w, h]</code>）； * 符合格式的输出奖励为 1.0，不符合格式的输出奖励为 0.0.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">format_reward_rec</span>(<span class="params">completions, **kwargs</span>):</span><br><span class="line">   <span class="keyword">import</span> re</span><br><span class="line">   pattern = <span class="string">r&quot;&lt;think&gt;.*?&lt;/think&gt;\s*&lt;answer&gt;.*?\&#123;.*\[\d+,\s*\d+,\s*\d+,\s*\d+\].*\&#125;.*?&lt;/answer&gt;&quot;</span></span><br><span class="line">   completion_contents = [completion[<span class="number">0</span>][<span class="string">&quot;content&quot;</span>] <span class="keyword">for</span> completion <span class="keyword">in</span> completions]</span><br><span class="line">   matches = [re.search(pattern, content, re.DOTALL) <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">for</span> content <span class="keyword">in</span> completion_contents]</span><br><span class="line">   <span class="keyword">return</span> [<span class="number">1.0</span> <span class="keyword">if</span> <span class="keyword">match</span> <span class="keyword">else</span> <span class="number">0.0</span> <span class="keyword">for</span> <span class="keyword">match</span> <span class="keyword">in</span> matches]</span><br></pre></td></tr></table></figure><p></p></blockquote><ol start="3" type="1"><li><strong>加载数据集</strong>：<code>LazySupervisedDataset</code>（符合<code>Pytorch</code>格式）</li></ol><p>数据应为<code>.yaml</code>文件，文件内容包含一个 <code>datasets</code> 字段，其中<strong>列出了多个数据源的路径以及相应的采样策略</strong>，示例如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">datasets:</span></span><br><span class="line">   <span class="bullet">-</span> <span class="attr">json_path:</span> <span class="string">xxxx1.json</span></span><br><span class="line">     <span class="attr">sampling_strategy:</span> <span class="string">first:1000</span></span><br><span class="line">   <span class="bullet">-</span> <span class="attr">json_path:</span> <span class="string">xxxx2.json</span></span><br><span class="line">     <span class="attr">sampling_strategy:</span> <span class="string">end:3000</span></span><br><span class="line">   <span class="bullet">-</span> <span class="attr">json_path:</span> <span class="string">xxxx3.json</span></span><br><span class="line">     <span class="attr">sampling_strategy:</span> <span class="string">random:999</span></span><br></pre></td></tr></table></figure><p></p><p>一共支持3种采样策略：</p><ul><li>"first"：取前 sampling_number 个样本；</li><li>"end"：取最后 sampling_number 个样本；</li><li>"random"：随机打乱数据并取前 sampling_number 个样本。</li></ul><h4 id="奖励函数iou_reward">奖励函数：<code>iou_reward</code></h4><h5 id="qwen2vlmodule的reward">Qwen2VLModule的reward</h5><p><strong>计算预测的边界框与真实边界框之间的IoU奖励</strong>:比较预测的边界框和真实边界框的重叠部分（交集）与它们的并集的比值</p><ol type="1"><li><p><code>IOU</code>函数定义：<code>inter</code>为交集面积；<code>union</code>为并集面积；比值为IOU</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">iou</span>(<span class="params">box1, box2</span>):</span><br><span class="line">    inter_x1 = <span class="built_in">max</span>(box1[<span class="number">0</span>], box2[<span class="number">0</span>])</span><br><span class="line">    inter_y1 = <span class="built_in">max</span>(box1[<span class="number">1</span>], box2[<span class="number">1</span>])</span><br><span class="line">    inter_x2 = <span class="built_in">min</span>(box1[<span class="number">2</span>]-<span class="number">1</span>, box2[<span class="number">2</span>]-<span class="number">1</span>)</span><br><span class="line">    inter_y2 = <span class="built_in">min</span>(box1[<span class="number">3</span>]-<span class="number">1</span>, box2[<span class="number">3</span>]-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> inter_x1 &lt; inter_x2 <span class="keyword">and</span> inter_y1 &lt; inter_y2:</span><br><span class="line">        inter = (inter_x2-inter_x1+<span class="number">1</span>)*(inter_y2-inter_y1+<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        inter = <span class="number">0</span></span><br><span class="line">    union = (box1[<span class="number">2</span>]-box1[<span class="number">0</span>])*(box1[<span class="number">3</span>]-box1[<span class="number">1</span>]) + (box2[<span class="number">2</span>]-box2[<span class="number">0</span>])*(box2[<span class="number">3</span>]-box2[<span class="number">1</span>]) - inter</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>(inter)/union</span><br></pre></td></tr></table></figure><p></p></li><li><p>主要逻辑：<strong>遍历每个预测值，提取出边界框，计算预测和真实边界框的 IoU 值作为奖励</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从 completions 中提取出每个预测结果的 content 字段</span></span><br><span class="line">contents = [completion[<span class="number">0</span>][<span class="string">&quot;content&quot;</span>] <span class="keyword">for</span> completion <span class="keyword">in</span> completions]</span><br><span class="line">rewards = []  <span class="comment"># 存储每个预测的奖励值（IoU 值）</span></span><br><span class="line">current_time = datetime.now().strftime(<span class="string">&quot;%d-%H-%M-%S-%f&quot;</span>)</span><br><span class="line">answer_tag_pattern = <span class="string">r&#x27;&lt;answer&gt;(.*?)&lt;/answer&gt;&#x27;</span></span><br><span class="line">bbox_pattern = <span class="string">r&#x27;\[(\d+),\s*(\d+),\s*(\d+),\s*(\d+)]&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> content, sol <span class="keyword">in</span> <span class="built_in">zip</span>(contents, solution):</span><br><span class="line">    reward = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">      <span class="comment"># content_answer_match：查找 &lt;answer&gt;...&lt;/answer&gt; 标签，并提取其中的答案</span></span><br><span class="line">        content_answer_match = re.search(answer_tag_pattern, content, re.DOTALL)</span><br><span class="line">        <span class="keyword">if</span> content_answer_match:</span><br><span class="line">            content_answer = content_answer_match.group(<span class="number">1</span>).strip()</span><br><span class="line">            <span class="comment"># bbox_match：在答案中提取边界框，使用正则表达式 [x1, y1, x2, y2]</span></span><br><span class="line">            bbox_match = re.search(bbox_pattern, content_answer)</span><br><span class="line">            <span class="keyword">if</span> bbox_match:</span><br><span class="line">                bbox = [<span class="built_in">int</span>(bbox_match.group(<span class="number">1</span>)), <span class="built_in">int</span>(bbox_match.group(<span class="number">2</span>)), <span class="built_in">int</span>(bbox_match.group(<span class="number">3</span>)), <span class="built_in">int</span>(bbox_match.group(<span class="number">4</span>))]</span><br><span class="line">                <span class="comment"># if iou(bbox, sol) &gt; 0.5:</span></span><br><span class="line">                <span class="comment">#     reward = 1.0</span></span><br><span class="line">                <span class="comment"># reward = iou(bbox, sol)：计算预测的边界框 bbox 和真实边界框 sol 之间的 IoU 值，作为奖励</span></span><br><span class="line">                reward = iou(bbox, sol)</span><br><span class="line">    <span class="keyword">except</span> Exception:</span><br><span class="line">        <span class="keyword">pass</span>  </span><br><span class="line">            </span><br><span class="line">    rewards.append(reward)</span><br><span class="line">    <span class="keyword">if</span> os.getenv(<span class="string">&quot;DEBUG_MODE&quot;</span>) == <span class="string">&quot;true&quot;</span>:</span><br><span class="line">        log_path = os.getenv(<span class="string">&quot;LOG_PATH&quot;</span>)</span><br><span class="line">        <span class="comment"># local_rank = int(os.getenv(&quot;LOCAL_RANK&quot;, 0))</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(log_path, <span class="string">&quot;a&quot;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(<span class="string">f&quot;------------- <span class="subst">&#123;current_time&#125;</span> Accuracy reward: <span class="subst">&#123;reward&#125;</span> -------------\n&quot;</span>)</span><br><span class="line">            f.write(<span class="string">f&quot;Content: <span class="subst">&#123;content&#125;</span>\n&quot;</span>)</span><br><span class="line">            f.write(<span class="string">f&quot;Solution: <span class="subst">&#123;sol&#125;</span>\n&quot;</span>)</span><br></pre></td></tr></table></figure><p></p></li></ol><h2 id="grpo-config参数">GRPO Config参数</h2><h3 id="数据预处理参数">数据预处理参数</h3><ul><li><code>remove_unused_columns (Optional[bool])</code>:如果设置为 <code>True</code>，则仅保留数据集中的 "prompt" 列，默认值为 <code>False</code>。用于计算自定义奖励函数；</li><li><code>max_prompt_length (Optional[int])</code>: prompt 的最大长度，默认值为 <code>512</code>（如果 prompt 长度超过此值，将会从左侧进行截断）</li><li><code>num_generations (Optional[int])</code>:每个 prompt 生成的样本数量。全局批次大小（num_processes * per_device_batch_size）必须能被此值整除，默认值为 <code>8</code>;</li><li><code>temperature (Optional[float])</code>：采样的温度值。温度越高，生成样本的随机性越高，默认值为<code>0.9</code>；</li><li><code>max_completion_length (Optional[int])</code>:生成的完成部分的最大长度，默认值为 <code>256</code>；</li><li><code>ds3_gather_for_generation (bool)</code>:针对 DeepSpeed ZeRO-3 的设置。启用时，<strong>生成时会收集策略模型的权重，提升生成速度</strong>；如果禁用此选项，能够训练超过单个 GPU VRAM 容量的模型，但生成速度会更慢，并且与 vLLM 生成不兼容。默认值为 <code>True</code>。</li></ul><h3 id="生成加速相关参数vllm">生成加速相关参数（vLLM）</h3><ul><li><code>use_vllm (Optional[bool])</code>:是否使用 vLLM（一个用于生成的加速库）进行生成，如果设置为 <code>True</code>，则需要确保有一个 GPU 用于生成，而不是训练；默认为<code>False</code></li><li><code>vllm_device (Optional[str])</code>:指定 vLLM 生成将运行的设备，例如 "cuda:1"。如果设置为 "auto"（默认值），系统会自动选择下一个可用的 GPU；</li><li><code>vllm_gpu_memory_utilization (float)</code>:该值指定要为 vLLM 生成预留的 GPU 内存比例。数值范围为 0 到 1，较高的数值会增加 KV 缓存大小，从而提高模型的吞吐量，但如果过高，可能会导致内存不足（OOM）错误。默认值为 <code>0.9</code>；</li><li><code>vllm_dtype (Optional[str])</code>:设置 vLLM 生成的数值类型。如果设置为 "auto"，将基于模型配置自动选择数据类型；</li><li><code>vllm_max_model_len (Optional[int])</code>:设置 vLLM 使用的最大模型长度。这对于减少 <code>vllm_gpu_memory_utilization</code> 的情况下，可能会减少 KV 缓存大小。如果未设置，vLLM 将使用模型的上下文大小；</li><li><code>vllm_enable_prefix_caching (Optional[bool])</code>:是否启用 vLLM 中的前缀缓存。若为 <code>True</code>（默认），确保模型和硬件支持此功能；</li><li><code>vllm_guided_decoding_regex (Optional[str])</code>:用于 vLLM 指导解码的正则表达式。如果为 <code>None</code>（默认），则禁用指导解码。</li></ul><h3 id="训练控制参数">训练控制参数</h3><ul><li><code>learning_rate (float)</code>:初始学习率，使用 AdamW 优化器。默认值为 <code>1e-6</code>，它替换了 <code>transformers.TrainingArguments</code> 中的默认学习率；</li><li><code>beta (float)</code>:KL 系数。值为 <code>0.0</code> 时，参考模型不会加载，从而减少内存使用和提高训练速度。默认值为 <code>0.04</code>；</li><li><code>num_iterations (int)</code>:每个批次的迭代次数（在算法中表示为<span class="math inline">\(\mu\)</span>）。默认值为 1；</li><li><code>epsilon (float)</code>:用于裁剪的 epsilon 值。默认值为 <code>0.2</code>；</li><li><code>reward_weights (Optional[list[float]])</code>:每个奖励函数的权重。如果为 <code>None</code>，则所有奖励函数的权重均为 <code>1.0</code>，默认值为 <code>None</code>；</li><li><code>sync_ref_model (bool)</code>:是否每 <code>ref_model_sync_steps</code> 步同步参考模型和活动模型，使用 <code>ref_model_mixup_alpha</code> 参数进行混合。默认值为 <code>False</code>；</li><li><code>ref_model_mixup_alpha (float)</code>:参考模型更新时的 <span class="math inline">\(\alpha\)</span> 参数，控制当前策略和前一个参考策略之间的混合。默认值为 <code>0.6</code>。此参数必须与 <code>sync_ref_model=True</code> 一起使用；</li><li><code>ref_model_sync_steps (int)</code>:确定当前策略与参考策略同步的频率，单位为步数。默认值为 <code>512</code>，此参数必须与 <code>sync_ref_model=True</code> 一起使用。</li></ul><h2 id="大规模grpo在多个节点上训练-70b-模型">大规模GRPO：在多个节点上训练 70B+ 模型</h2><p>在训练 Qwen2.5-72B 等大型模型时，需要进行几项关键优化，以提高训练效率并在多个 GPU 和节点之间扩展。包括： ### DeepSpeed ZeRO 第 3 阶段 利用<strong>数据并行</strong>性，在多个 GPU 和 CPU 之间分配模型状态（权重、梯度、优化器状态）</p></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://wenliuyi.github.io">Liuyi Wen</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://wenliuyi.github.io/posts/ae5287f0.html">http://wenliuyi.github.io/posts/ae5287f0.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://wenliuyi.github.io" target="_blank">Liuyi Wen's Blog</a>！</span></div></div><div class="tag_share"><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/posts/46623c6f.html" title="详解变分自编码器"><div class="cover" style="background:var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">详解变分自编码器</div></div><div class="info-2"><div class="info-item-1">VAE 信息论 信息量 \(I(x)=-\log{P(x)}\),描述事件x中包含的信息量。 信息熵 设随机变量X~p(X),则X的熵被定义为： \[ H(p)=\mathbb{E}_{X\sim p(X)}[-\log p(X)]. \] 当X为离散随机变量时， \[ H(p)=-\sum_{i=1}^{n}p(x_i)\log p(x_i) \] 熵的数学化理解： 编码随机变量所需的最短平均编码长度 即对于更大概率的事件，采用更短的编码(同Huffman编码思路一致)。 证明： 假设编码的字符集大小为\(D\)，若采用二进制编码，则\(D=2\). 假设存在需要编码的\(m\)个事件，每个事件的编码长度为\(l_i\). 根据编码理论中的Kraft–McMillan Inequality，在给定的码字字长下能够成功编码，当且仅当: \[ \sum_{i=1}^{m}D^{-l_i}\leq 1. \] 转为如下优化问题： \[ \min_{l_i}\sum_{i=1}^{m}p(x_i)l_i \] \[\sum_{i=1}^{m}D^{-l_i}\leq 1.\]...</div></div></div></a><a class="pagination-related" href="/posts/44ad5109.html" title="C++泛型算法"><div class="cover" style="background:var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">C++泛型算法</div></div><div class="info-2"><div class="info-item-1">本篇隶属C++ Primer中C++标准库专题，当前关注范型算法。 标准库容器只定义了很少的操作；因此提供了一组范型算法，其中大多数独立于特定的容器，具备通用性。 范型算法 概述 大多数算法在头文件algorithm中；头文件numeric中定义了一组数值范型算法。 范型算法本身不会执行容器的操作；只会运行于迭代器之上，执行迭代器的操作。 只读算法 只读取输入范围的元素，从不改变元素 find 12int val=42;auto result=find(vec.cbegin(), vec.cend(), val); find前两个参数：表示元素范围的迭代器；第三个参数：一个值。将范围中每个元素与给定值比较： 返回指向第一个等于给定值元素的迭代器； 若范围内无匹配元素，返回第二个参数，表示搜索失败。 accumulate 1int sum=accumulate(vec.cbegin(), vec.cend(), 0);	// 将sum设置为：vec中元素之和 第三个参数指定保存和的对象类型，在该类型上必须定义了"+"运算符 12// 错误：const...</div></div></div></a></nav><hr class="custom-hr"><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/img/butterfly-icon.png" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info-name">Liuyi Wen</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">29</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">5</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">14</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/WenLiuyi"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">The Journey Is the Reward.</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#grop%E6%96%B9%E6%B3%95"><span class="toc-number">1.</span> <span class="toc-text">GROP方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E7%82%B9"><span class="toc-number">1.1.</span> <span class="toc-text">要点</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%95%E5%85%A5critic%E4%BD%BF%E7%94%A8%E9%A2%84%E6%B5%8Bbaseline%E6%94%B9%E8%BF%9B%E5%A5%96%E5%8A%B1"><span class="toc-number">1.1.1.</span> <span class="toc-text">引入critic：使用预测baseline改进奖励</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B7%BB%E5%8A%A0%E8%A3%81%E5%89%AA%E5%92%8C%E6%9C%80%E5%B0%8F%E5%80%BC%E6%93%8D%E4%BD%9C%E9%98%B2%E6%AD%A2%E8%BF%87%E5%BA%A6%E6%9B%B4%E6%96%B0"><span class="toc-number">1.1.2.</span> <span class="toc-text">添加裁剪和最小值操作：防止过度更新</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%98%B2%E6%AD%A2%E4%BD%9C%E5%BC%8A%E5%92%8C%E6%9E%81%E7%AB%AF%E7%AD%96%E7%95%A5%E4%BD%BF%E7%94%A8kl%E6%95%A3%E5%BA%A6"><span class="toc-number">1.1.3.</span> <span class="toc-text">防止作弊和极端策略：使用KL散度</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E8%AE%AD%E7%BB%83%E5%99%A8vlmgrpotrainer"><span class="toc-number">2.</span> <span class="toc-text">自定义训练器VLMGRPOTrainer</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">2.1.</span> <span class="toc-text">初始化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="toc-number">3.</span> <span class="toc-text">训练过程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A5%96%E5%8A%B1%E5%87%BD%E6%95%B0iou_reward"><span class="toc-number">3.0.1.</span> <span class="toc-text">奖励函数：iou_reward</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#qwen2vlmodule%E7%9A%84reward"><span class="toc-number">3.0.1.1.</span> <span class="toc-text">Qwen2VLModule的reward</span></a></li></ol></li></ol></li></ol><li class="toc-item toc-level-2"><a class="toc-link" href="#grpo-config%E5%8F%82%E6%95%B0"><span class="toc-number">4.</span> <span class="toc-text">GRPO Config参数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E5%8F%82%E6%95%B0"><span class="toc-number">4.1.</span> <span class="toc-text">数据预处理参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E5%8A%A0%E9%80%9F%E7%9B%B8%E5%85%B3%E5%8F%82%E6%95%B0vllm"><span class="toc-number">4.2.</span> <span class="toc-text">生成加速相关参数（vLLM）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%8E%A7%E5%88%B6%E5%8F%82%E6%95%B0"><span class="toc-number">4.3.</span> <span class="toc-text">训练控制参数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E8%A7%84%E6%A8%A1grpo%E5%9C%A8%E5%A4%9A%E4%B8%AA%E8%8A%82%E7%82%B9%E4%B8%8A%E8%AE%AD%E7%BB%83-70b-%E6%A8%A1%E5%9E%8B"><span class="toc-number">5.</span> <span class="toc-text">大规模GRPO：在多个节点上训练 70B+ 模型</span></a></li></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/4e29148d.html" title="go-context">go-context</a><time datetime="2025-07-09T09:16:57.000Z" title="发表于 2025-07-09 17:16:57">2025-07-09</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/1e6a04d4.html" title="Transformer系列：2. Attention机制，MHA，MQA和GQA">Transformer系列：2. Attention机制，MHA，MQA和GQA</a><time datetime="2025-06-29T07:47:26.000Z" title="发表于 2025-06-29 15:47:26">2025-06-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/42e930ef.html" title="Transformer系列：1. 从RNN到Transformer">Transformer系列：1. 从RNN到Transformer</a><time datetime="2025-05-13T10:27:28.000Z" title="发表于 2025-05-13 18:27:28">2025-05-13</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/76aa9d6e.html" title="Transformer的KV Cache">Transformer的KV Cache</a><time datetime="2025-05-06T01:49:26.000Z" title="发表于 2025-05-06 09:49:26">2025-05-06</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/2ac460b1.html" title="verl框架：2. 对比OpenRLHF+colocate思路解析">verl框架：2. 对比OpenRLHF+colocate思路解析</a><time datetime="2025-05-06T01:49:06.000Z" title="发表于 2025-05-06 09:49:06">2025-05-06</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By Liuyi Wen</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(()=>{const t=()=>{if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"all"},chtml:{scale:1.1},options:{enableMenu:!0,renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const n=!!e.type.match(/; *mode=display/),a=new t.options.MathItem(e.textContent,t.inputJax[0],n),d=document.createTextNode("");e.parentNode.replaceChild(d,e),a.start={node:d,delim:"",n:0},a.end={node:d,delim:"",n:0},t.math.push(a)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}};btf.addGlobalFn("encrypt",t,"mathjax"),window.pjax?t():window.addEventListener("load",t)})()</script><script>(()=>{const n="shuoshuo"===GLOBAL_CONFIG_SITE.pageType,e=(e,o)=>{n&&(window.shuoshuoComment.destroyValine=()=>{e.children.length&&(e.innerHTML="",e.classList.add("no-comment"))});const t={el:"#vcomment",appId:"bsxtUJWr1muoPS1pmoXLOPZ2-gzGzoHsz",appKey:"wm2wUYvKLEySwyRnFn7xAbJI",avatar:"monsterid",serverURLs:"",emojiMaps:"",visitor:!1,path:n?o:window.location.pathname};new Valine(t)},o=async(n,o)=>{"function"==typeof Valine||await btf.getScript("https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"),e(n,o)};n?window.shuoshuoComment={loadComment:o}:btf.loadComment(document.getElementById("vcomment"),o)})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-info"><div class="algolia-stats"></div><div class="algolia-poweredBy"></div></div></div></div></div><div id="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/algoliasearch/dist/lite/builds/browser.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instantsearch.js/dist/instantsearch.production.min.js"></script><script src="/js/search/algolia.js"></script></div></div>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body></html>