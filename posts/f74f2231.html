<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>并行训练系列：7. Flash Attention V1/V2 | Liuyi Wen's Blog</title><meta name="author" content="Liuyi Wen"><meta name="copyright" content="Liuyi Wen"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="对于输入序列长度为\(N\)的 Transformer 类模型，其计算复杂度和存储空间为\(O(N^2)\)；Flash Attention（Fast and Memory Efficient Exact Attention with IO-Awareness） 技术旨在缓解上述计算和存储压力。 一个洞察为：计算慢的卡点在于读写速度，而非计算能力。Flash Attention 通过 tiling"><meta property="og:type" content="article"><meta property="og:title" content="并行训练系列：7. Flash Attention V1&#x2F;V2"><meta property="og:url" content="http://wenliuyi.github.io/posts/f74f2231.html"><meta property="og:site_name" content="Liuyi Wen&#39;s Blog"><meta property="og:description" content="对于输入序列长度为\(N\)的 Transformer 类模型，其计算复杂度和存储空间为\(O(N^2)\)；Flash Attention（Fast and Memory Efficient Exact Attention with IO-Awareness） 技术旨在缓解上述计算和存储压力。 一个洞察为：计算慢的卡点在于读写速度，而非计算能力。Flash Attention 通过 tiling"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://wenliuyi.github.io/img/WechatIMG105.jpg"><meta property="article:published_time" content="2025-12-16T13:23:23.000Z"><meta property="article:modified_time" content="2025-12-17T02:34:29.478Z"><meta property="article:author" content="Liuyi Wen"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="http://wenliuyi.github.io/img/WechatIMG105.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "并行训练系列：7. Flash Attention V1/V2",
  "url": "http://wenliuyi.github.io/posts/f74f2231.html",
  "image": "http://wenliuyi.github.io/img/WechatIMG105.jpg",
  "datePublished": "2025-12-16T13:23:23.000Z",
  "dateModified": "2025-12-17T02:34:29.478Z",
  "author": [
    {
      "@type": "Person",
      "name": "Liuyi Wen",
      "url": "http://wenliuyi.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://wenliuyi.github.io/posts/f74f2231.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>(()=>{const e={set:(e,t,o)=>{if(!o)return;const a=Date.now()+864e5*o;localStorage.setItem(e,JSON.stringify({value:t,expiry:a}))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const{value:o,expiry:a}=JSON.parse(t);if(!(Date.now()>a))return o;localStorage.removeItem(e)}};window.btf={saveToLocal:e,getScript:(e,t={})=>new Promise((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,Object.entries(t).forEach(([e,t])=>n.setAttribute(e,t)),n.onload=n.onreadystatechange=()=>{n.readyState&&!/loaded|complete/.test(n.readyState)||o()},n.onerror=a,document.head.appendChild(n)}),getCSS:(e,t)=>new Promise((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onload=n.onreadystatechange=()=>{n.readyState&&!/loaded|complete/.test(n.readyState)||o()},n.onerror=a,document.head.appendChild(n)}),addGlobalFn:(e,t,o=!1,a=window)=>{if(e.startsWith("pjax"))return;const n=a.globalFn||{};n[e]=n[e]||{},n[e][o||Object.keys(n[e]).length]=t,a.globalFn=n}};const t=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},o=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};btf.activateDarkMode=t,btf.activateLightMode=o;const a=e.get("theme");"dark"===a?t():"light"===a&&o();const n=e.get("aside-status");void 0!==n&&document.documentElement.classList.toggle("hide-aside","hide"===n);/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})()</script><script>const GLOBAL_CONFIG={root:"/",algolia:{appId:"VE36MEFVE6",apiKey:"f9b9ca5a3cdb9455658600dba6ae7706",indexName:"hexo-algolia indexing key",hitsPerPage:6,languages:{input_placeholder:"搜索文章",hits_empty:"未找到符合您查询的内容：${query}",hits_stats:"找到 ${hits} 条结果，耗时 ${time} 毫秒"}},localSearch:void 0,translate:void 0,highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:!1,highlightFullpage:!1,highlightMacStyle:!1},copy:{success:"复制成功",error:"复制失败",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"null",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!1,islazyloadPlugin:!1,isAnchor:!1,percent:{toc:!0,rightside:!1},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"并行训练系列：7. Flash Attention V1/V2",isHighlightShrink:!1,isToc:!0,pageType:"post"}</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Liuyi Wen's Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">并行训练系列：7. Flash Attention V1/V2</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div></div></nav><div id="post-info"><h1 class="post-title">并行训练系列：7. Flash Attention V1/V2</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-12-16T13:23:23.000Z" title="发表于 2025-12-16 21:23:23">2025-12-16</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-12-17T02:34:29.478Z" title="更新于 2025-12-17 10:34:29">2025-12-17</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Parallelism/">Parallelism</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><p>对于输入序列长度为<span class="math inline">\(N\)</span>的 Transformer 类模型，其计算复杂度和存储空间为<span class="math inline">\(O(N^2)\)</span>；Flash Attention（Fast and Memory Efficient Exact Attention with IO-Awareness） 技术旨在缓解上述计算和存储压力。</p><p>一个洞察为：计算慢的卡点在于<strong>读写速度</strong>，而非计算能力。Flash Attention 通过 tiling 和 kernel fusion 降低对显存（HBM）的访问次数以加速计算。</p><blockquote><p>计算瓶颈分析：</p><p>定义：</p><ul><li><span class="math inline">\(\pi\)</span>：<strong>硬件算力上限</strong>。每秒钟能完成的浮点运算次数，单位是 FLOPS 或 FLOP/s；</li><li><span class="math inline">\(\beta\)</span>：<strong>硬件带宽上限</strong>。每秒能完成的内存交换次数，单位是 Byte/s；</li><li><span class="math inline">\(\pi_t\)</span>：<strong>算法所需的总计算量</strong>；<span class="math inline">\(\beta_t\)</span>：<strong>算法所需的总数据读取存储量</strong>。</li></ul><p>算法的计算时间：<span class="math inline">\(T_{cal}=\frac{\pi_t}{\pi}\)</span>；算法的数据读取时间：<span class="math inline">\(T_{load}=\frac{\beta_t}{\beta}\)</span></p><p>计算限制：<span class="math inline">\(T_{cal}&gt;T_{load}\)</span>；内存限制：<span class="math inline">\(T_{cal}&lt;T_{load}\)</span></p><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/f74f2231/image.png"> 大矩阵乘法通常受计算限制；逐点运算操作（激活函数、dropout、mask、softmax、normalization 等）受内存限制。</p><p>假设矩阵<span class="math inline">\(Q, K\in\mathbb{R}^{N\times d}\)</span>，其中<span class="math inline">\(N\)</span>为序列长度，<span class="math inline">\(d\)</span>为 embedding dim；标准注意力的计算公式为：</p><p><span class="math display">\[ O=softmax(\frac{1}{\sqrt{d}}QK^T)V=softmax(S)V=PV \]</span></p><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/f74f2231/image-1.png"> 计算量：计算<span class="math inline">\(S=QK^T\in R^{N\times N}\)</span>，<span class="math inline">\(O=PV\in R^{N\times d}\)</span>.</p><p>数据读取量：</p><ol type="1"><li>第1步对<span class="math inline">\(Q, K\)</span>的读取共2次，第3步对<span class="math inline">\(S\)</span>的写入1次；</li><li>第4步对<span class="math inline">\(S\)</span>的读取1次，第7步对<span class="math inline">\(P\)</span>的写入1次；</li><li>第8步对<span class="math inline">\(P, V\)</span>的读取共2次，第10步对<span class="math inline">\(O\)</span>的写入1次。</li></ol><p><span class="math inline">\(\frac{\pi_t}{\beta_t}=\frac{4N^2d}{2Nd+2Nd+4N^2}=\frac{N^2d}{Nd+N^2}\)</span></p><p>标准注意力在 SRAM 和 HBM 之间的交互流程如下： <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/f74f2231/image-2.png"></p></blockquote><p>GPU 的计算流程是：将数据从显存（HBM）加载至 on-chip 的 SRAM 中，由 SM 读取并计算；计算结果通过 SRAM 返回给 HBM。最节省内存的做法是：<strong>以 SRAM 的存储为上限，尽可能每次加载时打满</strong>，目标是减少 HBM 和 SRAM 之间的换入换出。</p><p>标准注意力机制<span class="math inline">\(softmax(\frac{QK^T}{\sqrt{d_k}}\times V)\)</span>主要分为3个步骤，分别对应依次执行的3个 kernel：**gemm（q⁢u⁢e⁢r⁢y×k⁢e⁢y）、point-wise的softmax、gemm（a⁢t⁢t⁢n⁢_⁢s⁢c⁢o⁢r⁢e×v⁢a⁢l⁢u⁢e）**。Flash Attention 的方案是：以两个 gemm kernel 为中心进行融合，使用 SRAM 存储中间结果（不写回 HBM）；通过分块解决无法容纳整个中间矩阵的问题。</p><p>整体流程如下： <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/f74f2231/image-3.png"></p><h2 id="flash-attention-v1">Flash Attention V1</h2><h3 id="tiling">Tiling</h3><p>将<span class="math inline">\(K, V\)</span>切分成<span class="math inline">\(T_c\)</span>个小块，将<span class="math inline">\(Q, O\)</span>切分成<span class="math inline">\(T_r\)</span>个小块，执行双层循环计算：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">O_0 = 0</span><br><span class="line">for 1 &lt;= j &lt;= Tc: # K, V 外循环</span><br><span class="line">    load V_j, K_j</span><br><span class="line">    for 1 &lt;= i &lt;= Tr: # Q, O 内循环</span><br><span class="line">        load O_i^&#123;j-1&#125;, Q_i</span><br><span class="line">        S = softmax(Q_i * K_j^T) # 计算得 S</span><br><span class="line">        O_i^j = S * V_j # 得到 O_i 行的第 j 列</span><br><span class="line">        O_i = O_i^j + O_i^&#123;j-1&#125; # 完成 O_i 的一次累加更新</span><br><span class="line">        store O_i # 回写</span><br></pre></td></tr></table></figure><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/f74f2231/image-4.png"></p><p>HBM 和 SRAM 之间的数据读取/写入操作如下：</p><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/f74f2231/image-5.png"></p><h3 id="safe-softmax-动态更新">Safe Softmax 动态更新</h3><p>朴素的 softmax 计算：<span class="math inline">\(softmax(x_i)=\frac{e^{x_i}}{\sum_{j=1}^B e^{x_j}}\)</span> 容易出现上溢和下溢问题。（指数项<span class="math inline">\(e^{x_i}\)</span>超过浮点数表示范围时发生上溢；<span class="math inline">\(x_i\)</span>是较大的负值时每个<span class="math inline">\(e^{x_i}\)</span>下溢导致分母为0）</p><p>稳定的 softmax 版本：</p><p><span class="math display">\[ \begin{align*} m(x)&amp;=\max([x_1, x_2, ..., x_B]) \\ f(x)&amp;=[e^{x_1-m(x)}, ..., e^{x_B-m(x)}] \\ softmax(x)&amp;=\frac{f(x)}{\sum_i f(x)_i} \end{align*} \]</span></p><p>其中，<span class="math inline">\(\sum_i f(x)_i\)</span> 是<span class="math inline">\(f(x)\)</span>的所有元素求和。</p><p>softmax 分块计算是一大难点，因为其分母依赖于每一个元素。分块计算的关键在于更新全局最大值<span class="math inline">\(m(x)\)</span>和分母<span class="math inline">\(\sum_i f(x)_i\)</span>.</p><p>考虑向量<span class="math inline">\(x\in\mathbb{R}^{2B}\)</span>，分成两块：<span class="math inline">\(x=[x^{(1)}, x^{(2)}]\)</span>。在分块计算中先处理<span class="math inline">\(x^{(1)}\)</span>，再处理<span class="math inline">\(x^{(2)}\)</span>.有：</p><p><span class="math display">\[ \begin{align*} m(x^{(1)})&amp;=\max([x_1^{(1)}, x_2^{(1)}, ..., x_B^{(1)}]) \\ f(x^{(1)})&amp;=[e^{x_1^{(1)}-m(x^{(1)})}, ..., e^{x_B^{(1)}-m(x^{(1)})}] \\ softmax(x^{(1)})&amp;=\frac{f(x^{(1)})}{\sum_i f(x^{(1)})_i} \end{align*} \]</span></p><p>以上是<span class="math inline">\(x^{(1)}\)</span>对应的局部计算结果，<strong>不保存 <span class="math inline">\(x^{(1)}\)</span>，选择保存 <span class="math inline">\(m(x^{(1)})\)</span> 和 <span class="math inline">\(l(x^{(1)})\)</span>，用于在处理完 <span class="math inline">\(x^{(2)}\)</span> 后更新 <span class="math inline">\(x^{(1)}\)</span></strong>.</p><p><strong>保存两个全局标量：当前最大值 <span class="math inline">\(m_{max}\)</span> 和全局 exp 求和项 <span class="math inline">\(l_{all}\)</span></strong>. 当前：<span class="math inline">\(m_{max}=m(x^{(1)}), l_{all}=l(x^{(1)})\)</span>.</p><p>采取同样的流程处理<span class="math inline">\(x^{(2)}\)</span>.全局标量更新方法如下：</p><p><span class="math display">\[ \begin{align*} m_{max}^{new}&amp;=\max([m_{max}, m(x^{(2)})]) \\ l_{all}^{new}&amp;=e^{m_{max}-m_{max}^{new}}l_{all}+e^{m(x^{(2)})-m_{max}^{new}}l(x^{(2)}) \end{align*} \]</span></p><p>分块计算时，局部的<span class="math inline">\(f(x^{(2)})=[e^{x_1^{(2)}-m(x^{(2)})}, ..., e^{x_B^{(2)}-m(x^{(2)})}]\)</span>，则全局的<span class="math inline">\(f^{new}(x^{(2)})\)</span>更新为：</p><p><span class="math display">\[ \begin{align*} f^{new}(x^{(2)})&amp;=f(x^{(2)})·e^{m(x^{(2)})-m_{max}^{new}} \\ &amp;=[e^{x_1^{(2)}-m_{max}^{new}}, ..., e^{x_B^{(2)}-m_{max}^{new}}] \end{align*} \]</span></p><p>更新 <span class="math inline">\(softmax(x^{(2)})\)</span> 的分子部分：</p><p><span class="math display">\[ \begin{align*} softmax^{temp}(x^{(2)})&amp;=softmax(x^{(2)})·e^{m(x^{(2)})-m_{max}^{new}} \\ &amp;=\frac{f(x^{(2)})}{l(x^{(2)})}·e^{m(x^{(2)})-m_{max}^{new}} \\ &amp;=\frac{f^{new}(x^{(2)})}{l(x^{(2)})} \end{align*} \]</span></p><p>将分母 <span class="math inline">\(l(x^{(2)})\)</span> 替换为全局 exp 求和项 <span class="math inline">\(l_{all}^{new}\)</span>，即：</p><p><span class="math display">\[ \begin{align*} softmax^{new}(x^{(2)})&amp;=softmax^{temp}(x^{(2)})·\frac{l(x^{(2)})}{l_{all}^{new}} \\ &amp;=\frac{f^{new}(x^{(2)})}{l_{all}^{new}} \end{align*} \]</span></p><p>整合后观察 <span class="math inline">\(softmax(x^{(2)})\)</span> 到 <span class="math inline">\(softmax^{new}(x^{(2)})\)</span> 的更新方式：</p><p><span class="math display">\[ softmax^{new}(x^{(2)})=\frac{softmax(x^{(2)})·l(x^{(2)})·e^{m(x^{(2)})-m_{max}^{new}}}{l_{all}^{new}} \]</span></p><p>用到以下额外保存的量：</p><ol type="1"><li><span class="math inline">\(x^{(2)}\)</span> 的局部 softmax 值 <span class="math inline">\(softmax(x^{(2)})\)</span>；</li><li><span class="math inline">\(x^{(2)}\)</span> 的局部 exp 求和项 <span class="math inline">\(l(x^{(2)})\)</span>；</li><li><span class="math inline">\(x^{(2)}\)</span> 的局部最大值 <span class="math inline">\(m(x^{(2)})\)</span>；</li><li>全局最大值 <span class="math inline">\(m_{max}^{new}\)</span>；</li><li>全局 exp 求和项 <span class="math inline">\(l_{all}^{new}\)</span>.</li></ol><p>注意，以上从局部到全局的更新无需 <span class="math inline">\(x^{(1)}\)</span> 或 <span class="math inline">\(x^{(2)}\)</span> 的整个向量值。</p><h3 id="forward-pass">Forward Pass</h3><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/f74f2231/image-6.png"> 如上图所示：</p><ol type="1"><li><p>根据 SRAM 的大小<span class="math inline">\(M\)</span>选择合适的分块大小<span class="math inline">\(B_c, B_r\)</span>；</p></li><li><p>在 HBM 中初始化若干变量，包括：最终输出<span class="math inline">\(O\)</span>，指数求和项<span class="math inline">\(l\)</span>，每行 Attention Score 的最大值<span class="math inline">\(m\)</span>；</p></li><li><p>将 <span class="math inline">\(Q\in\mathbb{R}^{N\times d}\)</span> 沿着行切分为 <span class="math inline">\(T_r\)</span> 个大小为 <span class="math inline">\(B_r\times d\)</span> 的小块；将 <span class="math inline">\(K, V\in\mathbb{R}^{N\times d}\)</span> 沿着行切分为 <span class="math inline">\(T_c\)</span> 个大小为 <span class="math inline">\(B_c\times d\)</span> 的小块。</p></li><li><p>将 <span class="math inline">\(O\in\mathbb{R}^{N\times d}, m, l\in\mathbb{R}^{N}\)</span> 沿着行切分为 <span class="math inline">\(T_r\)</span> 个小块。</p></li><li><p><strong>外循环：遍历<span class="math inline">\(K, V\)</span>，由 <span class="math inline">\(T_c\)</span> 控制；</strong></p></li><li><p>当前外循环遍历到的<span class="math inline">\(K_j, V_j\)</span>由 HBM 读入 SRAM；</p></li><li><p><strong>内循环：遍历<span class="math inline">\(Q, O, l, m\)</span>，由 <span class="math inline">\(T_r\)</span> 控制；</strong></p></li><li><p>当前内循环遍历到的<span class="math inline">\(Q_i, O_i, l_i, m_i\)</span>由 HBM 读入 SRAM；</p></li><li><p><strong>计算当前分块的 Attention Score：<span class="math inline">\(S_{ij}=Q_iK_j^{T}\in\mathbb{R}^{B_r\times B_c}\)</span></strong>；</p></li><li><p>对于分块的 Attention Score <span class="math inline">\(S_{ij}\)</span>，计算其每一行的最大值 <span class="math inline">\(\tilde{m_{ij}}\in\mathbb{R^{B_r}}\)</span>；</p><ul><li>基于 <span class="math inline">\(\tilde{m_{ij}}\)</span>，计算指数项 <span class="math inline">\(\tilde{P_{ij}}=\exp(S_{ij}-\tilde{m_{ij}})\in\mathbb{R}^{B_r\times B_c}\)</span>；</li><li>基于 <span class="math inline">\(\tilde{P_{ij}}\)</span>，计算 exp 求和项 <span class="math inline">\(rowsum(\tilde{P_{ij}})\in\mathbb{R^{B_r}}\)</span>.</li></ul></li><li><p>计算当前的全局量 <span class="math inline">\(m_i^{new}\)</span> 和 <span class="math inline">\(l_i^{new}\)</span>（此时暂且不用于更新旧的 <span class="math inline">\(m_i\)</span> 和 <span class="math inline">\(l_i\)</span>，因为还需要参与后续计算）</p></li><li><p>softmax 计算时，行与行之间无交互，分块是列意义上的。因此暂不考虑 batch 计算，每个小块简化为 <span class="math inline">\(S_{ij}\in\mathbb{R}^{1\times B_c}\)</span>. 使用 <span class="math inline">\(S_i\)</span> 表示每一行的 Attention Score，<span class="math inline">\(SM_i\)</span> 表示每一行的 softmax.</p><ol type="1"><li><p>处理 <span class="math inline">\(S_{11}\)</span>；</p></li><li><p>处理 <span class="math inline">\(S_{12}\)</span>：计算 <span class="math inline">\(S_{12}\)</span> 的局部 softmax；<span class="math inline">\(SM_1\)</span> 更新为 <span class="math inline">\(SM_1=SM_1^{new}+SM_{12}=\frac{SM_1·l_1·e^{m_1-m_1^{new}}}{l_1^{new}}+\frac{\tilde{P_{12}}·e^{m_{12}-m_1^{new}}}{l_1^{new}}\)</span></p><blockquote><p>等式右侧第一项是一个<span class="math inline">\(N\)</span>维向量，前<span class="math inline">\(B_c\)</span>项有效，其他均为0；第二项中 <span class="math inline">\(\tilde{P_{12}}\in\mathbb{R}^{1\times B_c}\)</span>， 可以重定义为一个<span class="math inline">\(N\)</span>维向量，其他位置上均为0.</p><p>求和效果等价于：更新第<span class="math inline">\([1, B_c]\)</span>列的值；将新值写入第<span class="math inline">\([B_c+1, 2B_c]\)</span>列.</p></blockquote></li><li><p>更新完 <span class="math inline">\(SM_1\)</span> 后，输出 <span class="math inline">\(O_1\)</span> 更新为 <span class="math inline">\(O_1=O_1^{new}+O_{12}=\frac{O_1·l_1·e^{m_1-m_1^{new}}}{l_1^{new}}+\frac{\tilde{P_{12}}·e^{m_{12}-m_1^{new}}}{l_1^{new}}·V_2\)</span>.（<span class="math inline">\(V_2\)</span> 对应的是 <span class="math inline">\(S_{12}\)</span> 中的列数）</p></li></ol></li><li><p>更新 <span class="math inline">\(l_i\)</span> 和 <span class="math inline">\(m_i\)</span>.</p></li></ol><blockquote><p><span class="math inline">\(B_r, B_c\)</span> 的设置：<span class="math inline">\(B_c=\lceil\frac{M}{4d}\rceil\)</span>，<span class="math inline">\(B_r=min(\lceil\frac{M}{4d}\rceil, d)\)</span>.</p><p>假设 <span class="math inline">\(B_c=B_r=\frac{M}{4d}\)</span>，则从 HBM 加载到 SRAM 的4个矩阵 <span class="math inline">\(K_j, V_j, O_i, Q_i\)</span> 的尺寸均为 <span class="math inline">\([\frac{M}{4d}, d]\)</span>.因此数据读取的总开销等于 SRAM 的大小 <span class="math inline">\(M\)</span>（<span class="math inline">\(l_i, m_i\)</span> 可忽略不计）.</p></blockquote><h3 id="backward-pass">Backward Pass</h3><blockquote><p>Softmax 求偏导：</p><p>softmax 函数表达式为：<span class="math inline">\(y_i=\frac{e^{z_i}}{\sum_{t=1}^me^{z_t}}\)</span></p><p>求偏导有：<span class="math inline">\(\frac{\partial y_i}{\partial z_j}=\frac{\partial \frac{e^{z_i}}{\sum_{t=1}^me^{z_t}}}{\partial z_j}\)</span></p><ol type="1"><li>当 <span class="math inline">\(i=j\)</span> 时，有 <span class="math inline">\(\frac{\partial e^{z_i}}{\partial z_j}=e^{z_i}\)</span>，则：</li></ol><p><span class="math display">\[ \begin{align*} \frac{\partial y_i}{\partial z_j}&amp;=\frac{\partial \frac{e^{z_i}}{\sum_{t=1}^me^{z_t}}}{\partial z_j} \\ &amp;=\frac{e^{z_i}·\sum_{t=1}^me^{z_t}-e^{z_i}·e^{z_j}}{(\sum_{t=1}^me^{z_t})^2} \\ &amp;=\frac{e^{z_i}}{\sum_{t=1}^me^{z_t}}-\frac{e^{z_i}}{\sum_{t=1}^me^{z_t}}·\frac{e^{z_j}}{\sum_{t=1}^me^{z_t}} \\ &amp;=y_i(1-y_j) \end{align*} \]</span></p><pre><code>2. 当 $i\neq j$ 时，有 $\frac&#123;\partial e^&#123;z_i&#125;&#125;&#123;\partial z_j&#125;=0$，则：</code></pre><p><span class="math display">\[ \begin{align*} \frac{\partial y_i}{\partial z_j}&amp;=\frac{\partial \frac{e^{z_i}}{\sum_{t=1}^me^{z_t}}}{\partial z_j} \\ &amp;=\frac{0·\sum_{t=1}^me^{z_t}-e^{z_i}·e^{z_j}}{(\sum_{t=1}^me^{z_t})^2} \\ &amp;=-\frac{e^{z_i}}{\sum_{t=1}^me^{z_t}}·\frac{e^{z_j}}{\sum_{t=1}^me^{z_t}} \\ &amp;=-y_i y_j \end{align*} \]</span></p><p><span class="math inline">\(y=softmax(z)\)</span> 关于 <span class="math inline">\(z\)</span> 的求导结果是一个 Jacobian 矩阵 <span class="math inline">\(diag(y)-y^Ty\)</span>：</p><p><span class="math display">\[ \begin{align*} \frac{\partial\vec{y}}{\partial\vec{z}}&amp;=diag(\vec{y})-\vec{y}^T\vec{y} \\ &amp;=\begin{bmatrix}y_1 &amp; 0 &amp; 0 \\0 &amp; y_2 &amp; 0 \\0 &amp; 0 &amp; y_3\end{bmatrix}-\begin{bmatrix}y_1 \\y_2 \\y_3\end{bmatrix}\begin{bmatrix}y_1 &amp; y_2 &amp; y_3 \end{bmatrix} \\ &amp;=\begin{bmatrix}y_1-y_1^2 &amp; -y_1y_2 &amp; -y_1y_3 \\-y_2y_1 &amp; y_2-y_2^2 &amp; -y_2y_3 \\-y_3y_1 &amp; -y_3y_2 &amp; y_3-y_3^2\end{bmatrix} \end{align*} \]</span></p><p>记 <span class="math inline">\(\frac{\partial L}{\partial y}=[dy_1, dy_2, dy_3]\)</span>，有：</p><p><span class="math display">\[ \begin{align*} \frac{\partial L}{\partial\vec{z}}&amp;=\frac{\partial L}{\partial\vec{y}}\frac{\partial\vec{y}}{\partial\vec{z}}=d\vec{y}(diag(\vec{y})-\vec{y}^T\vec{y}) \\ &amp;=[dy_1, dy_2, dy_3]\begin{bmatrix}y_1-y_1^2 &amp; -y_1y_2 &amp; -y_1y_3 \\-y_2y_1 &amp; y_2-y_2^2 &amp; -y_2y_3 \\-y_3y_1 &amp; -y_3y_2 &amp; y_3-y_3^2\end{bmatrix} \end{align*} \]</span></p></blockquote><p><strong>forward 过程中，没有保存 <span class="math inline">\(S\)</span> 和 <span class="math inline">\(P\)</span> 这两个中间结果</strong>，而是保存 <span class="math inline">\(m\)</span> 和 <span class="math inline">\(l\)</span>，用于 backward 阶段中重新计算出 <span class="math inline">\(S\)</span> 和 <span class="math inline">\(P\)</span>.</p><p>流程如下： <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/f74f2231/image-7.png"></p><h3 id="计算量和显存需求">计算量和显存需求</h3><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/f74f2231/image-8.png"> 计算量：主要关注矩阵乘法。</p><ol type="1"><li>第9行：<span class="math inline">\(S_{ij}=Q_iK_j^T\in\mathbb{R}^{B_r\times B_c}\)</span>，其中 <span class="math inline">\(Q_i\in\mathbb{R}^{B_r\times d}, K_j\in\mathbb{R}^{d\times B_c}\)</span>，则 <span class="math inline">\(S_{ij}\)</span> 的计算量为 <span class="math inline">\(O({B_r B_c d})\)</span>；</li><li>第12行：<span class="math inline">\(\tilde{P_{ij}}V_j\)</span>，其中 <span class="math inline">\(\tilde{P_{ij}}\in\mathbb{R}^{B_r\times B_c}, V_j\in\mathbb{R}^{B_c\times d}\)</span>，计算量为 <span class="math inline">\(O({B_r B_c d})\)</span>.</li><li>执行的循环次数：<span class="math inline">\(T_cT_r=\frac{N}{B_c}\frac{N}{B_r}\)</span></li></ol><p>因此，Flash Attention V1 的 forward 总计算量为：<span class="math inline">\(O(\frac{N^2}{B_cB_r}B_rB_cd)=O(N^2d)\)</span></p><p>标准 Attention 的需要存储 <span class="math inline">\(S, P\)</span>，显存需求为 <span class="math inline">\(O(N^2)\)</span>；Flash Attention 只存储 <span class="math inline">\(m, l\)</span>，<span class="math inline">\(S, P\)</span> 在 backward 中重计算，显存需求降低至 <span class="math inline">\(O(N)\)</span>.</p><h3 id="io-复杂度">IO 复杂度</h3><p>标准 Attention 的 总开销为 <span class="math inline">\(4Nd+4N^2\)</span>，IO 复杂度为 <span class="math inline">\(O(Nd+N^2)\)</span>；</p><p>Flash Attention V1：</p><ol type="1"><li>第6行：每个外循环加载 <span class="math inline">\(K, V\)</span> 的一小块，所有外循环一共加载一次完整的 <span class="math inline">\(K, V\in\mathbb{R}^{N\times d}\)</span>，IO 复杂度为 <span class="math inline">\(O(Nd)\)</span>；</li><li>第8行：每个内循环加载 <span class="math inline">\(Q, O, m, l\)</span> 的一小块，一个外循环中的所有内循环共加载一次完整的 <span class="math inline">\(Q, O\in\mathbb{R}^{N\times d}\)</span>（<span class="math inline">\(m, l\)</span> IO 复杂度为 <span class="math inline">\(O(N)\)</span>，忽略不计），一共有 <span class="math inline">\(T_c\)</span> 次外循环。总 IO 复杂度为 <span class="math inline">\(O(T_cNd)\)</span>.</li><li>第12/13行：将<span class="math inline">\(O, m, l\)</span> 写回 HBM，IO 复杂度为 <span class="math inline">\(O(Nd)\)</span>.</li></ol><p>综上，Flash Attention V1 的 forward 总 IO 时间复杂度为：<span class="math inline">\(O(T_cNd)=O(\frac{N}{B_c}Nd)=O(\frac{4Nd}{M}Nd)=O(\frac{N^2d^2}{M})\)</span>.</p><p>论文中有述，一般 <span class="math inline">\(d\)</span> 的取值为 64～128，<span class="math inline">\(M\)</span> 的取值为 100KB 左右，因此有 <span class="math inline">\(\frac{d^2}{M}&lt;&lt;1\)</span>。</p><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/f74f2231/image-9.png"></p><p>从中间的图可知：数据块越大，读写次数越少，runtime 整体下降（IO 复杂度为 <span class="math inline">\(O(T_cNd)\)</span>，数据块越大，<span class="math inline">\(O(T_c)\)</span> 越小）；数据块大小&gt;256 后，runtime 下降不明显，因为随着矩阵扩大，计算耗时增加，抵消 IO 复杂度降低节省的时间。</p><h2 id="flash-attention-v2">Flash Attention V2</h2><p>FlashAttention V2 相对 V1 主要有3个优化点：Algorithm、Parallelism 和 Work Partitioning。</p><h3 id="algorithm">Algorithm</h3><p>观察 Flash Attention V1 的 softmax 更新公式：</p><p><span class="math display">\[ \begin{align*} softmax^{new}(x^{(2)})&amp;=softmax^{temp}(x^{(2)})·\frac{l(x^{(2)})}{l_{all}^{new}} \\ &amp;=\frac{f^{new}(x^{(2)})}{l(x^{(2)})}·\frac{l(x^{(2)})}{l_{all}^{new}} \\ &amp;=\frac{f^{new}(x^{(2)})}{l_{all}^{new}} \end{align*} \]</span></p><p>以上全局更新的关键点是：将局部的 exp 求和项 <span class="math inline">\(l(x^{(2)})\)</span> 替换为全局的 exp 求和项 <span class="math inline">\(l_{all}^{new}\)</span>.</p><p>计算 <span class="math inline">\(softmax^{new}(x^{(3)})\)</span> 时，需要乘此时的 <span class="math inline">\(l(x^{(3)})\)</span>（对应上一轮的 <span class="math inline">\(l_{all}^{new}\)</span>），再除以新的全局 exp 求和项。</p><p>每遍历到一个分块，都会做一次类似的更新操作。实际上等价于：<strong>在最后除以最新的 <span class="math inline">\(l_{all}^{new}\)</span>.</strong></p><blockquote><p>举个栗子（今天北京初雪，想吃糖炒板栗）：</p><p>对于一个等差数列：1, 2, 3, ...；Flash Attention V1 的计算方法为：</p><p><span class="math inline">\(N=2: f=\frac{1+2}{2}=1.5\)</span></p><p><span class="math inline">\(N=3: f=\frac{1.5\times 2+3}{3}=2\)</span></p><p>每一次都算出前 <span class="math inline">\(N\)</span> 个数的平均值；第 <span class="math inline">\(N+1\)</span> 步更新平均值。</p><p>Flash Attention V2 的做法优化为：</p><p><span class="math inline">\(N=2: f=1+2=3\)</span></p><p><span class="math inline">\(N=3: f=3+3=6\)</span></p><p>最后再除以 <span class="math inline">\(N\)</span>.</p></blockquote><p>从 DataFlow 的维度上看：V1 的方案是外层循环加载 <span class="math inline">\(K, V\)</span>，内层循环加载 <span class="math inline">\(Q\)</span>，那么内层循环每次计算的是 <span class="math inline">\(O_i\)</span> 的一部分，对于同一个 <span class="math inline">\(Q_i\)</span>，不同的 <span class="math inline">\(K_j, V_j\)</span> 都需要读写一次 <span class="math inline">\(O_i\)</span>，带来频繁读写 HBM 的问题。</p><p><span class="math inline">\(O_i\)</span> 的更新与 <span class="math inline">\(Q_i\)</span> 严格绑定，而不同 <span class="math inline">\(Q_i\)</span> 的 Attention Score 的计算是完全独立的。所以 <strong>V2 的做法是：以 <span class="math inline">\(Q\)</span> 作为外循环，<span class="math inline">\(K, V\)</span> 作为内循环</strong>。每一个外循环中，完成 <span class="math inline">\(O_i\)</span> 的计算，即每个 <span class="math inline">\(O_i\)</span> 只需要读写一次。</p><p>计算流程如下： <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/f74f2231/image-10.png"></p><p>HBM 和 SRAM 之间的 IO 操作如下：</p><p><a href="image-11.png"></a></p><h4 id="forward-pass-1">Forward Pass</h4><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/f74f2231/image-12.png"> 如上图所示：</p><ol type="1"><li><p>将 <span class="math inline">\(Q\in\mathbb{R}^{N\times d}\)</span> 沿着行切分为 <span class="math inline">\(T_r\)</span> 个大小为 <span class="math inline">\(B_r\times d\)</span> 的小块；将 <span class="math inline">\(K, V\in\mathbb{R}^{N\times d}\)</span> 沿着行切分为 <span class="math inline">\(T_c\)</span> 个大小为 <span class="math inline">\(B_c\times d\)</span> 的小块。</p></li><li><p>将 <span class="math inline">\(O\in\mathbb{R}^{N\times d}, m, l\in\mathbb{R}^{N}\)</span> 沿着行切分为 <span class="math inline">\(T_r\)</span> 个小块。</p></li><li><p><strong>外循环：遍历<span class="math inline">\(Q, O, l, m\)</span>，由 <span class="math inline">\(T_r\)</span> 控制；</strong></p></li><li><p>当前外循环遍历到的 <span class="math inline">\(Q_i\)</span> 由 HBM 读入 SRAM；</p></li><li><p>初始化 <span class="math inline">\(O_i\in\mathbb{R}^{B_r\times d}, l_i, m_i\in\mathbb{R}^{B_r}\)</span>；</p></li><li><p><strong>内循环：遍历<span class="math inline">\(K, V\)</span>，由 <span class="math inline">\(T_c\)</span> 控制；</strong></p></li><li><p>当前内循环遍历到的 <span class="math inline">\(K_j, V_j\)</span> 由 HBM 读入 SRAM；</p></li><li><p><strong>计算当前分块的 Attention Score：<span class="math inline">\(S_{i}^{(j)}=Q_iK_j^{T}\in\mathbb{R}^{B_r\times B_c}\)</span></strong>；</p></li><li><p>对于分块的 Attention Score <span class="math inline">\(S_{i}^{(j)}\)</span>，计算其截止到当前分块（包含当前分块）的 rowmax <span class="math inline">\(\in\mathbb{R^{B_r}}\)</span>；</p><ul><li>基于 <span class="math inline">\(m_{i}^{(j)}\)</span>，计算指数项 <span class="math inline">\(\tilde{P_{i}^{(j)}}=\exp(S_{i}^{(j)}-m_{i}^{(j)})\in\mathbb{R}^{B_r\times B_c}\)</span>；</li><li>基于 <span class="math inline">\(\tilde{P_{i}^{(j)}}\)</span>，计算截止到当前分块（包含当前分块）的 rowsum <span class="math inline">\(l_{i}^{(j)}\in\mathbb{R^{B_r}}\)</span>.</li></ul></li><li><p>遍历完所有 <span class="math inline">\(K, V\)</span> 之后，得到的 <span class="math inline">\(O_i^{(j)}\)</span> 等于最终的全局结果。</p></li></ol><p><strong>与 V1 相比，V2 不用在 FWD 中存储每一个 <span class="math inline">\(Q_i\)</span> 对应的中间结果 <span class="math inline">\(m_i, l_i\)</span></strong>；然而依然需要在 BWD 中使用 <span class="math inline">\(m_i, l_i\)</span> 完成 <span class="math inline">\(S_i^{(j)}, P_i^{(j)}\)</span> 的重计算。<strong>V2 采用一个替代方式：存储 <span class="math inline">\(L_i=m_i^{(T_c)}+\log(l_i^{(T_c)})\)</span></strong>（这里的 <span class="math inline">\(m_i, l_i\)</span> 分别对应全局的 rowmax 和 rowsum.</p><blockquote><p><span class="math inline">\(B_r, B_c\)</span> 的设置：<span class="math inline">\(B_c=\lceil\frac{M}{4d}\rceil\)</span>，<span class="math inline">\(B_r=min(\lceil\frac{M}{4d}\rceil, d)\)</span>.</p><p>假设 <span class="math inline">\(B_c=B_r=\frac{M}{4d}\)</span>，则从 HBM 加载到 SRAM 的4个矩阵 <span class="math inline">\(K_j, V_j, O_i, Q_i\)</span> 的尺寸均为 <span class="math inline">\([\frac{M}{4d}, d]\)</span>.因此数据读取的总开销等于 SRAM 的大小 <span class="math inline">\(M\)</span>（<span class="math inline">\(l_i, m_i\)</span> 可忽略不计）.</p></blockquote><h4 id="backward-pass-1">Backward Pass</h4><blockquote><p>V2 的 BWD 的内外循环依然采用：<span class="math inline">\(K, V\)</span> 外循环，<span class="math inline">\(Q\)</span> 内循环。因为在 BWD 的关键步骤是：<span class="math inline">\(dK_j, dV_j, dQ_i\)</span>（<span class="math inline">\(dK_j, dV_j\)</span> 沿着 <span class="math inline">\(i\)</span> 方向 all-reduce；<span class="math inline">\(Q_i\)</span> 沿着 <span class="math inline">\(j\)</span> 方向 all-reduce.）；简化的是外循环的计算。因此将 <span class="math inline">\(K, V\)</span> 放在外循环简化最多。</p></blockquote><p>重计算 <span class="math inline">\(P_i^{(j)}=diag(l_i^{(j)})^{-1}\exp(S_i^{(j)}-m_i^{(j)})=\exp(S_i^{(j)}-m_i^{(j)}-\log(l_i^{(j)})=\exp(S_i^{(j)}-L_i))\)</span>，其中：<span class="math inline">\(L_i=m_i^{(T_c)}+\log(l_i^{(T_c)})\)</span>.</p><p>这是在 FWD 中存储 <span class="math inline">\(L_i\)</span> 的意义。</p><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/f74f2231/image-13.png"></p><h3 id="parallelism">Parallelism</h3><p>优化 Attention 部分 thread blocks 的并行化计算，<strong>新增 seq_len 维度的并行</strong>，使得 SM 的利用率尽量打满。</p><blockquote><p>相较于 CPU，GPU 更适合并行计算。</p><p>Thread Block：在每个 thread block 中包含多个 wrap，每个 wrap 包含 32 个 threads；同一个 wrap 中的所有 threads 协作完成矩阵乘法，同一个 thread block 中的所有 wraps 共享存储空间。</p><p>Streaming multiprocessors（SM）是 GPU 中真正的物理计算单元，thread blocks 最终被调度至 SM 上计算。因此<strong>并行化的最终目标是打满 SM</strong>。</p></blockquote><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// gridDim in V1</span></span><br><span class="line"><span class="comment">// params.b = batch_size, params.h = num_heads</span></span><br><span class="line"><span class="function">dim3 <span class="title">grid</span><span class="params">(params.b, params.h)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// gridDim in V2</span></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> num_m_block = (params.seqlen_q + Kernel_traits::kBlockM - <span class="number">1</span>) / Kernel_traits::kBlockM;</span><br><span class="line"><span class="function">dim3 <span class="title">grid</span><span class="params">(num_m_block, params.b, params.h)</span></span>;</span><br></pre></td></tr></table></figure><p>V1 在 batch 内部、同一个 attention block 的 heads 之间实现并行计算（每个 attention head 对应一个 thread block）；block 将被调度至 SM 上执行。以 A100 GPU 为例（一共有 108 个 SM）：如果 block 数量较多（比如 80 个以上），说明 SM 利用率较高；然而，<strong>随着 LLM 的上下文窗口变长（即 seq_len 变长），单卡上的 batch_size 和 num_heads 将随之变小，导致较多的 SM 空转。</strong></p><p>在 V2 的 cutlass 实现中，<strong>对 <span class="math inline">\(Q\)</span> 的 seq_len 进行切分</strong>：FWD 中 <span class="math inline">\(Q\)</span> 为外循环，<span class="math inline">\(K, V\)</span> 为内循环，不同 <span class="math inline">\(Q_i\)</span> 之间的计算是独立的，因此可以并行。</p><blockquote><p>V1 后期也引入了基于 <span class="math inline">\(Q\)</span> 的 sequence parallel，grid 形式为 (batch_size, num_heads, num_m_blocks)；V2 中的 grid 形式调整为 (num_m_blocks, batch_size, num_heads).</p><p>这一改进的目的为：<strong>提升L2 cache hit rate</strong>。将 num_m_blocks 放在最外层循环，那么同一列的 block 会相邻；对于同一列的 block，读取的是相同的 <span class="math inline">\(K_j, V_j\)</span>（可以直接从 Cache 中取）</p></blockquote><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/f74f2231/image-14.png"> 上图为 V2 的 FWD/BWD 过程中的并行。</p><ul><li>FWD：每一行（一个 <span class="math inline">\(Q_i\)</span>）对应一个 thread block（<span class="math inline">\(Q\)</span> 为外循环，<span class="math inline">\(K, V\)</span> 为内循环）；</li><li>BWD：每一列（一个 <span class="math inline">\(K_j, V_j\)</span>）对应一个 thread block（<span class="math inline">\(K, V\)</span> 为外循环，<span class="math inline">\(Q\)</span> 为内循环）。</li></ul><h3 id="word-partitioning">Word Partitioning</h3><p>针对 thread block 中 wraps 的组织优化。</p><p><strong>V1 对 <span class="math inline">\(K, V\)</span> 按照 seq_len 分成 wrap</strong>（对 <span class="math inline">\(Q\)</span> 不分块）：假设将 <span class="math inline">\(K, V\)</span> 切成4块得到 <span class="math inline">\(K_1, K_2, K_3, K_4\)</span> 和 <span class="math inline">\(V_1, V_2, V_3, V_4\)</span>，大小均为 <span class="math inline">\((\frac{N}{4}, d)\)</span>. 每个 wrap 的 Self-Attention 计算流程如下：</p><p><span class="math display">\[ \begin{align*} S_i=QK_i^T\in\mathbb{R}^{N\times\frac{N}{4}} \\ P_i=softmax(S_i)\in\mathbb{R}^{N\times\frac{N}{4}} \\ O_i=P_iV_i\in\mathbb{R}^{N\times d} \\ \end{align*} \]</span></p><p>每一个 wrap 计算的输出 <span class="math inline">\(O_i\)</span> 写入共享存储区域；最终将结果叠加得到完整输出 <span class="math inline">\(O\)</span>.</p><p><strong>V2 对 <span class="math inline">\(Q\)</span> 按照 seq_len 分成 wrap</strong>（对 <span class="math inline">\(K, V\)</span> 不分块）：假设将 <span class="math inline">\(Q\)</span> 切成4块得到 <span class="math inline">\(Q_1, Q_2, Q_3, Q_4\)</span>，大小均为 <span class="math inline">\((\frac{N}{4}, d)\)</span>. 每个 wrap 的 Self-Attention 计算流程如下：</p><p><span class="math display">\[ \begin{align*} S_i=Q_iK^T\in\mathbb{R}^{\frac{N}{4}\times N} \\ P_i=softmax(S_i)\in\mathbb{R}^{\frac{N}{4}\times N} \\ O_i=P_iV_i\in\mathbb{R}^{\frac{N}{4}\times d} \\ \end{align*} \]</span></p><p>V2 只需将每个 wrap 的结果直接拼接，warps 之间无需通信；另外减少了额外的加法及它对应的读写操作，因此 V2 的 wrap 拆分策略更高效。对比图如下：</p><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/f74f2231/image-15.png"></p><h2 id="参考">参考</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2205.14135">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a></p><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2312.11918">A Case Study in CUDA Kernel Fusion: Implementing FlashAttention-2 on NVIDIA Hopper Architecture using the CUTLASS Library</a></p><p><a target="_blank" rel="noopener" href="https://tridao.me/publications/flash2/flash2.pdf">FlashAttention-2:Faster Attention with Better Parallelism and Work Partitioning</a></p><p>[图解大模型计算加速系列：FlashAttention V1，从硬件到计算逻辑</p><p>](https://zhuanlan.zhihu.com/p/669926191)</p><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/1887915323191194921">探秘Transformer系列之（18）--- FlashAttention</a></p><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/642962397">万字长文详解FlashAttention v1/v2</a></p><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/wuliytTaotao/p/10787510.html">【机器学习基础】对 softmax 和 cross-entropy 求导</a></p></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://wenliuyi.github.io">Liuyi Wen</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://wenliuyi.github.io/posts/f74f2231.html">http://wenliuyi.github.io/posts/f74f2231.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://wenliuyi.github.io" target="_blank">Liuyi Wen's Blog</a>！</span></div></div><div class="tag_share"><div class="post-share"><div class="social-share" data-image="/img/WechatIMG105.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related full-width" href="/posts/88142594.html" title="RL 系列：5. 从 TRPO 到 PPO 算法"><div class="cover" style="background:var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">RL 系列：5. 从 TRPO 到 PPO 算法</div></div><div class="info-2"><div class="info-item-1">上一篇记录了策略梯度算法的目标：通过梯度上升的方法执行策略更新，目标是最大化期望奖励，数学形式为： \[ \nabla\overline{R_\theta}=\mathbb{E}_{\tau\sim p_\theta(\tau)}[R(\tau)\nabla\log p_\theta(\tau)] \] 由于策略梯度算法是 on-policy 的，因此要求行为策略\(\mu\)和目标策略\(\pi_\theta\)相同。每次将策略\(\theta\)更新为\(\theta&#39;\)时，来源于上一轮采样数据的概率\(p_\theta(\tau)\)不能再使用，需要重新采样；这说明 on-policy 算法每次采样的数据在更新参数时只能使用一次。耗时巨大。能不能转为 off-policy 算法呢？ 一个想法是使用冻结的策略\(\pi_{\theta&#39;}\)专门与环境交互，使用\(\theta&#39;\)采样的数据训练优化目标\(\pi_\theta\)....</div></div></div></a></nav><hr class="custom-hr"><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/img/WechatIMG105.jpg" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info-name">Liuyi Wen</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">50</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">5</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/WenLiuyi"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">The Journey Is the Reward.</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#flash-attention-v1"><span class="toc-number">1.</span> <span class="toc-text">Flash Attention V1</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#tiling"><span class="toc-number">1.1.</span> <span class="toc-text">Tiling</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#safe-softmax-%E5%8A%A8%E6%80%81%E6%9B%B4%E6%96%B0"><span class="toc-number">1.2.</span> <span class="toc-text">Safe Softmax 动态更新</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#forward-pass"><span class="toc-number">1.3.</span> <span class="toc-text">Forward Pass</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#backward-pass"><span class="toc-number">1.4.</span> <span class="toc-text">Backward Pass</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E9%87%8F%E5%92%8C%E6%98%BE%E5%AD%98%E9%9C%80%E6%B1%82"><span class="toc-number">1.5.</span> <span class="toc-text">计算量和显存需求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#io-%E5%A4%8D%E6%9D%82%E5%BA%A6"><span class="toc-number">1.6.</span> <span class="toc-text">IO 复杂度</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#flash-attention-v2"><span class="toc-number">2.</span> <span class="toc-text">Flash Attention V2</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#algorithm"><span class="toc-number">2.1.</span> <span class="toc-text">Algorithm</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#forward-pass-1"><span class="toc-number">2.1.1.</span> <span class="toc-text">Forward Pass</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#backward-pass-1"><span class="toc-number">2.1.2.</span> <span class="toc-text">Backward Pass</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#parallelism"><span class="toc-number">2.2.</span> <span class="toc-text">Parallelism</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#word-partitioning"><span class="toc-number">2.3.</span> <span class="toc-text">Word Partitioning</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83"><span class="toc-number">3.</span> <span class="toc-text">参考</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/f74f2231.html" title="并行训练系列：7. Flash Attention V1/V2">并行训练系列：7. Flash Attention V1/V2</a><time datetime="2025-12-16T13:23:23.000Z" title="发表于 2025-12-16 21:23:23">2025-12-16</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/88142594.html" title="RL 系列：5. 从 TRPO 到 PPO 算法">RL 系列：5. 从 TRPO 到 PPO 算法</a><time datetime="2025-12-08T07:52:43.000Z" title="发表于 2025-12-08 15:52:43">2025-12-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/ef1b2883.html" title="并行训练系列：6. 序列并行上篇（Megatron-SP, DeepSpeed-Ulysses）">并行训练系列：6. 序列并行上篇（Megatron-SP, DeepSpeed-Ulysses）</a><time datetime="2025-12-01T07:51:23.000Z" title="发表于 2025-12-01 15:51:23">2025-12-01</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/76477a6b.html" title="verl 框架：3. 加载数据与创建 batch">verl 框架：3. 加载数据与创建 batch</a><time datetime="2025-11-24T09:29:10.000Z" title="发表于 2025-11-24 17:29:10">2025-11-24</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/49c2e3ad.html" title="RL 系列：4. 策略梯度算法">RL 系列：4. 策略梯度算法</a><time datetime="2025-11-17T14:21:39.000Z" title="发表于 2025-11-17 22:21:39">2025-11-17</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By Liuyi Wen</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(()=>{const t=()=>{if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"all"},chtml:{scale:1.1},options:{enableMenu:!0,renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const n=!!e.type.match(/; *mode=display/),a=new t.options.MathItem(e.textContent,t.inputJax[0],n),d=document.createTextNode("");e.parentNode.replaceChild(d,e),a.start={node:d,delim:"",n:0},a.end={node:d,delim:"",n:0},t.math.push(a)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}};btf.addGlobalFn("encrypt",t,"mathjax"),window.pjax?t():window.addEventListener("load",t)})()</script><script>(()=>{const n="shuoshuo"===GLOBAL_CONFIG_SITE.pageType,e=(e,o)=>{n&&(window.shuoshuoComment.destroyValine=()=>{e.children.length&&(e.innerHTML="",e.classList.add("no-comment"))});const t={el:"#vcomment",appId:"bsxtUJWr1muoPS1pmoXLOPZ2-gzGzoHsz",appKey:"wm2wUYvKLEySwyRnFn7xAbJI",avatar:"monsterid",serverURLs:"",emojiMaps:"",visitor:!1,path:n?o:window.location.pathname};new Valine(t)},o=async(n,o)=>{"function"==typeof Valine||await btf.getScript("https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"),e(n,o)};n?window.shuoshuoComment={loadComment:o}:btf.loadComment(document.getElementById("vcomment"),o)})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-info"><div class="algolia-stats"></div><div class="algolia-poweredBy"></div></div></div></div></div><div id="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/algoliasearch/dist/lite/builds/browser.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instantsearch.js/dist/instantsearch.production.min.js"></script><script src="/js/search/algolia.js"></script></div></div>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body></html>