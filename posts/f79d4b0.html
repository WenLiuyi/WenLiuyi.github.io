<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>大模型推理框架vLLM：paper + code 解析 | Liuyi Wen's Blog</title><meta name="author" content="Liuyi Wen"><meta name="copyright" content="Liuyi Wen"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="论文解读：Efficient Memory Management for Large Language Model Serving with PagedAttention 论文原文 Abstract 为了提供LLM的高吞吐量服务，每次需要批量处理足够多的请求。然而现有系统面临KV缓存内存不足的挑战：每个请求的KV缓存内存占用巨大，且动态增减。当内存管理效率低下时，碎片化和冗余复制会造成显著的内存浪"><meta property="og:type" content="article"><meta property="og:title" content="大模型推理框架vLLM：paper + code 解析"><meta property="og:url" content="http://wenliuyi.github.io/posts/f79d4b0.html"><meta property="og:site_name" content="Liuyi Wen&#39;s Blog"><meta property="og:description" content="论文解读：Efficient Memory Management for Large Language Model Serving with PagedAttention 论文原文 Abstract 为了提供LLM的高吞吐量服务，每次需要批量处理足够多的请求。然而现有系统面临KV缓存内存不足的挑战：每个请求的KV缓存内存占用巨大，且动态增减。当内存管理效率低下时，碎片化和冗余复制会造成显著的内存浪"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://wenliuyi.github.io/img/WechatIMG105.jpg"><meta property="article:published_time" content="2025-04-10T10:58:58.000Z"><meta property="article:modified_time" content="2025-09-08T09:28:54.473Z"><meta property="article:author" content="Liuyi Wen"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="http://wenliuyi.github.io/img/WechatIMG105.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "大模型推理框架vLLM：paper + code 解析",
  "url": "http://wenliuyi.github.io/posts/f79d4b0.html",
  "image": "http://wenliuyi.github.io/img/WechatIMG105.jpg",
  "datePublished": "2025-04-10T10:58:58.000Z",
  "dateModified": "2025-09-08T09:28:54.473Z",
  "author": [
    {
      "@type": "Person",
      "name": "Liuyi Wen",
      "url": "http://wenliuyi.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://wenliuyi.github.io/posts/f79d4b0.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>(()=>{const e={set:(e,t,o)=>{if(!o)return;const a=Date.now()+864e5*o;localStorage.setItem(e,JSON.stringify({value:t,expiry:a}))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const{value:o,expiry:a}=JSON.parse(t);if(!(Date.now()>a))return o;localStorage.removeItem(e)}};window.btf={saveToLocal:e,getScript:(e,t={})=>new Promise((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,Object.entries(t).forEach(([e,t])=>n.setAttribute(e,t)),n.onload=n.onreadystatechange=()=>{n.readyState&&!/loaded|complete/.test(n.readyState)||o()},n.onerror=a,document.head.appendChild(n)}),getCSS:(e,t)=>new Promise((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onload=n.onreadystatechange=()=>{n.readyState&&!/loaded|complete/.test(n.readyState)||o()},n.onerror=a,document.head.appendChild(n)}),addGlobalFn:(e,t,o=!1,a=window)=>{if(e.startsWith("pjax"))return;const n=a.globalFn||{};n[e]=n[e]||{},n[e][o||Object.keys(n[e]).length]=t,a.globalFn=n}};const t=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},o=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};btf.activateDarkMode=t,btf.activateLightMode=o;const a=e.get("theme");"dark"===a?t():"light"===a&&o();const n=e.get("aside-status");void 0!==n&&document.documentElement.classList.toggle("hide-aside","hide"===n);/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})()</script><script>const GLOBAL_CONFIG={root:"/",algolia:{appId:"VE36MEFVE6",apiKey:"f9b9ca5a3cdb9455658600dba6ae7706",indexName:"hexo-algolia indexing key",hitsPerPage:6,languages:{input_placeholder:"搜索文章",hits_empty:"未找到符合您查询的内容：${query}",hits_stats:"找到 ${hits} 条结果，耗时 ${time} 毫秒"}},localSearch:void 0,translate:void 0,highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:!1,highlightFullpage:!1,highlightMacStyle:!1},copy:{success:"复制成功",error:"复制失败",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"null",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!1,islazyloadPlugin:!1,isAnchor:!1,percent:{toc:!0,rightside:!1},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"大模型推理框架vLLM：paper + code 解析",isHighlightShrink:!1,isToc:!0,pageType:"post"}</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Liuyi Wen's Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">大模型推理框架vLLM：paper + code 解析</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div></div></nav><div id="post-info"><h1 class="post-title">大模型推理框架vLLM：paper + code 解析</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-04-10T10:58:58.000Z" title="发表于 2025-04-10 18:58:58">2025-04-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-09-08T09:28:54.473Z" title="更新于 2025-09-08 17:28:54">2025-09-08</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h2 id="论文解读efficient-memory-management-for-large-language-model-serving-with-pagedattention">论文解读：Efficient Memory Management for Large Language Model Serving with PagedAttention</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2309.06180">论文原文</a></p><h3 id="abstract">Abstract</h3><p>为了提供LLM的高吞吐量服务，每次需要批量处理足够多的请求。然而现有系统面临<strong>KV缓存内存不足</strong>的挑战：每个请求的KV缓存内存占用巨大，且动态增减。当内存管理效率低下时，碎片化和冗余复制会造成显著的内存浪费，从而限制批处理规模。为解决这一问题，我们提出PagedAttention，这是一种受经典操作系统虚拟内存与分页技术启发的注意力算法。基于此，我们构建了vLLM这一LLM服务系统，其实现了：(1) KV缓存内存接近零浪费；(2) 支持请求内及跨请求的KV缓存灵活共享，进一步降低内存占用。评估表明，在相同延迟水平下，vLLM将主流LLM的吞吐量较FasterTransformer、Orca等最先进系统提升了2-4倍。当处理更长序列、更大模型及更复杂解码算法时，性能提升尤为显著。</p><h3 id="introduction">Introduction</h3><p>当前LLM serving system主要面临：KV缓存的管理问题。通常做法是将一条request的KV cache存储在连续的内存空间内（大部分deep learning框架要求tensors在连续内存空间中存储）。KV cache和tensors的区别在于： 1. <strong>KV Cache随模型生成新token，而动态增减</strong>； 2. 其生命周期和长度无法提前预测。</p><p>因此，在两个方向上导致了内存使用的低效：</p><ol type="1"><li><p><strong>内存的内外碎片</strong>：为满足连续空间存储的要求，需要<strong>预先分配一段连续的最大内存空间</strong>（例如：2048 tokens），这会导致内部碎片（request的实际长度小于最大长度）；</p><blockquote><p>即使长度预知，预先分配也是低效的：在request的生命周期内，内存块为其保留；导致其他更短的request也无法使用当前空闲的内存块。</p></blockquote><p>另外，对于每个request，预分配不同长度的空间，会导致外部碎片。</p></li><li><p><strong>未实现内存共享的最大优化</strong>：LLM通常采用advanced decoding算法（并行采样或束搜索），这些算法为每个request生成多个输出序列，可以部分共享KV Cache，但已有系统未考虑这一点。</p></li></ol><p>PagedAttention的想法是什么呢？提出了页式虚拟内存机制。</p><p>将request的KV cache拆分为多个blocks：每个block包括固定数量tokens的attention keys和values。因此，KV Cache无需存储在连续内存空间。 &gt; 联动OS的理念：将blocks当作页面；tokens当作字节；requests当作进程。 &gt; &gt; 这个设计通过使用更小尺寸的的blocks和按需分配，消除内部碎片；通过固定大小的blocks消除外部碎片。</p><h3 id="backgrounds">BackGrounds</h3><h4 id="基于transformer的llm">基于Transformer的LLM</h4><p>LLM的任务是：对token序列<span class="math inline">\((x_1, x_2, ..., x_n)\)</span>的概率模型建模。采用<strong>自回归分解</strong>：将整个序列的联合概率，分解为条件概率的乘积： <span class="math display">\[ P(x)=P(x_1)\cdot P(x_2|x_1)\cdot\cdot\cdot P(x_n|x_1,...,x_{n-1}). \]</span></p><p>Transformer模型是大规模概率建模的事实标准架构，其核心组件是<strong>自注意力层（self-attention layer）</strong>。处理输入隐藏状态序列<span class="math inline">\((x_1, ..., x_n)\in\mathbb{R}^{n\times d}\)</span>时，首先对每个位置<span class="math inline">\(i\)</span>进行线性变换生成查询（query）、键（key）和值（value）向量： <span class="math display">\[ q_i=W_q x_i, k_i=W_k x_i, v_i=W_v x_i. \]</span> 随后，该层通过计算当前位置query向量与所有历史位置的key向量的点积，得到注意力分数<span class="math inline">\(a_{ij}\)</span>： <span class="math display">\[ a_{ij}=\frac{\exp(\frac{q_i^{T}k_j}{\sqrt{d}})}{\sum_{t=1}^{i}\exp(\frac{q_i^{T}k_t}{\sqrt{d}})}, o_i=\sum_{j=1}^{i}a_{ij}v_j. \]</span></p><h4 id="llm服务自回归生成">LLM服务&amp;自回归生成</h4><p>经过训练后，大型语言模型（LLM）通常被部署为条件生成服务（例如自动补全API或聊天机器人）。向LLM服务发出的请求会提供一组input prompt tokens<span class="math inline">\((x_1,x_2,...,x_n)\)</span>，LLM服务则根据自回归分解公式，生成output tokens<span class="math inline">\((x_{n+1},x_{n+2},...,x_{n+T})\)</span>。将input prompt tokens与output tokens的组合称为序列（sequence）。</p><p>由于自回归分解公式的分解特性，LLM只能<strong>逐个采样生成新token</strong>，且每个新token的生成过程都依赖于该序列中所有先前的tokens——特别是它们的键（key）和值（value）向量。在这一顺序生成过程中，现有token的键值向量，通常会被缓存以供后续token生成使用，即KV缓存（KV cache）。需要注意的是，<strong>某个token的KV缓存取决于其之前的所有token</strong>，这意味着同一token出现在序列不同位置时，其KV缓存也会不同。</p><p>对于给定的一个request prompt，生成过程分为两个阶段：</p><ol type="1"><li><p>prompt phase：以完整用户prompt<span class="math inline">\((x_1,x_2,...,x_n)\)</span>为输入，<strong>计算首个新token的概率<span class="math inline">\(P(x_{n+1}|x_1,...,x_n)\)</span></strong>。在此过程中，同时生成键向量<span class="math inline">\(k_1,...,k_n\)</span>和值向量<span class="math inline">\(v_1,...,v_n\)</span>。由于token<span class="math inline">\(x_1,...,x_n\)</span>均为已知，该阶段可通过矩阵-矩阵乘法实现并行计算，因此能充分利用GPU的并行计算优势。</p></li><li><p>autoregressive generation phase：按顺序生成剩余新tokens。在第<span class="math inline">\(t\)</span>次迭代时，模型接收单个token<span class="math inline">\(x_{n+t}\)</span>作为输入，基于缓存的键向量<span class="math inline">\(k_1,...,k_{n+t-1}\)</span>和值向量<span class="math inline">\(v_1,...,v_{n+t-1}\)</span>，计算概率<span class="math inline">\(P(x_{n+t+1}|x_1,...,x_{n+t})\)</span>，并生成新的键值向量<span class="math inline">\(k_{n+t}\)</span>和<span class="math inline">\(v_{n+t}\)</span>。该阶段在序列达到最大长度（用户指定或模型限制）或生成结束符<eos>时终止。<strong>由于数据依赖性，不同迭代的计算无法并行化，且多采用效率较低的矩阵-向量乘法运算</strong>，导致GPU计算资源利用率严重不足，形成内存瓶颈——这构成了单个请求延迟的主要来源。</eos></p></li></ol><h4 id="llm批处理">LLM批处理</h4><p>由于同一批次内的请求共享模型权重，权重加载的开销可被批量请求均摊——当批次规模足够大时，计算开销将完全覆盖权重传输成本。然而LLM 批处理面临两大挑战：</p><ol type="1"><li><strong>请求的异步到达</strong>特性。若采用简单批处理策略，要么让先到请求等待后续请求（导致排队延迟），要么推迟新请求直至当前批次完成（造成吞吐量下降）。</li><li><strong>请求的输入输出长度差异巨大</strong>。若强行通过填充（padding）对齐序列长度，将导致GPU计算资源和内存的严重浪费。</li></ol><blockquote><p>为解决这些问题，学界提出了<strong>细粒度批处理机制</strong>（如蜂窝批处理和迭代级调度）。与传统请求级批处理不同，这些技术基于迭代维度运作：每完成一次迭代，系统便移除已处理完成的请求，并动态加入新请求。这使得<strong>新请求仅需等待单个迭代周期即可被处理</strong>，无需阻塞至整批请求完成。此外，借助专用GPU内核，这些技术彻底消除了序列填充需求。通过降低排队延迟与填充损耗，细粒度批处理机制能显著提升LLM服务的吞吐效率。</p></blockquote><h3 id="methods">Methods</h3><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/f79d4b0/image.png"></p><h4 id="pagedattention">PagedAttention</h4><p>将每个序列的KV Cache分为若干个KV blocks，每个block包含：固定数量的键向量和值向量。将key block表示为：<span class="math inline">\(K_j=(k_{(j-1)B+1},...,v_{jB})\)</span>；value block表示为：<span class="math inline">\(V_j=(v_{(j-1)B+1},...,v_{jB})\)</span>.注意力计算公式变为： <span class="math display">\[ A_{ij}=\frac{\exp(\frac{q_{i}^{T}K_j}{\sqrt{d}})}{\sum_{t=1}^{\lceil \frac{i}{B}\rceil}\exp(\frac{q_{i}^{T}K_t}{\sqrt{d}})}, o_i=\sum_{j=1}^{\lceil \frac{i}{B}\rceil}V_jA_{ij}^T \]</span> 其中，<span class="math inline">\(A_{ij}=(a_{i,(j-1)B+1},...,a_{i,jB})\)</span>是第<span class="math inline">\(j\)</span>个KV block的注意力分数。</p><p>在注意力计算过程中，PagedAttention内核会动态识别、并分别获取不同的KV块。如图所示，键值向量分散存储在三个非连续物理内存块中（例如块0存储"Four score and seven"的键值向量）。 <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/f79d4b0/image-1.png"> 内核执行分阶段计算：</p><ol type="1"><li><strong>query-key交互</strong>：每一次计算中，内核将query token（"forth"）的query向量<span class="math inline">\(q_i\)</span>，与一个block内的key向量<span class="math inline">\(K_j\)</span>相乘，以计算注意力分数<span class="math inline">\(A_{ij}\)</span>。</li><li><strong>value聚合</strong>：将<span class="math inline">\(A_{ij}\)</span>与当前块的<span class="math inline">\(V_j\)</span>相乘，生成局部注意力输出<span class="math inline">\(o_i\)</span>。</li></ol><p>总结来说，PagedAttention算法允许KV blocks存储在非连续的物理内存空间，使得vLLM中能够采用更灵活的页内存管理。</p><h4 id="kv-cache-manager">KV Cache Manager</h4><p>vLLM内存管理器的核心设计思想源于：操作系统的<strong>虚拟内存</strong>机制。操作系统将内存划分为固定大小的页（page），并将用户程序的逻辑页映射到物理页上——<strong>连续的逻辑页可对应非连续的物理内存页，使得用户程序能以连续视角访问内存</strong>。更重要的是，物理内存空间无需预先全量分配，操作系统可<strong>按需动态分配物理页</strong>。</p><p>vLLM将虚拟内存的思想应用于LLM服务的KV缓存管理：</p><ol type="1"><li>存储结构：<ul><li>通过PagedAttention将KV缓存组织为固定大小的<strong>KV块</strong>（类比虚拟内存中的页）；</li><li>每个请求的KV缓存表示为从左到右填充的<strong>逻辑KV块序列</strong>，末块预留空位供未来生成使用。</li></ul></li><li>硬件资源管理：<ul><li>GPU工作节点：块引擎（block engine）分配连续GPU显存，并划分为物理KV块；</li><li>CPU内存：同样分块以支持交换机制</li></ul></li><li>映射系统：<ul><li>块表（block table）：维护逻辑KV块与物理KV块的映射关系</li><li>每个块表条目记录：<ul><li>逻辑块对应的物理块地址</li><li>已填充位置数量</li></ul></li></ul></li></ol><h4 id="使用pagedattention和vllm解码">使用PagedAttention和vLLM解码</h4><p>通过以下示例，说明vLLM如何在单输入序列的解码过程中执行PagedAttention并管理内存： <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/f79d4b0/image-2.png"></p><ol type="1"><li>prefill：与操作系统虚拟内存类似，vLLM无需预先为最大可能序列长度保留内存，而是仅分配prompt计算所需的KV块。<ul><li>7个prompt tokens被分配到2个逻辑KV块（块0和块1）；</li><li>逻辑块映射到物理块7和1；</li><li>使用常规自注意力算法，生成prompt的KV Cache和首个输出token；</li><li>前4个tokens存入逻辑块0，后3个tokens存入逻辑块1（末位预留空位）。</li></ul></li><li>首次自回归解码<ul><li>基于物理块7和1执行PagedAttention生成新token；</li><li>新生成的KV缓存存入逻辑块1预留槽；</li><li>块表中#filled字段更新。</li></ul></li><li>二次解码<ul><li>当逻辑块1写满时，分配新逻辑块；</li><li>从空闲池获取物理块3并建立映射；</li><li>更新块表记录新增的逻辑-物理块对应关系。</li></ul></li></ol><p>全局来看，vLLM在每次解码迭代时执行以下关键操作：</p><ol type="1"><li>动态批处理构建：选择候选序列集合进行批处理；为新需求的逻辑KV块分配物理块。</li><li>输入序列整合：将当前迭代内，所有输入tokens拼接为单一序列：提示阶段请求的所有tokens+生成阶段请求的最新token</li><li>分页注意力执行：通过PagedAttention内核：访问以逻辑KV块形式存储的历史KV缓存；将新生成的KV缓存写入分配的物理KV块。</li></ol><p>vLLM采用<strong>动态物理块分配</strong>机制：随着新token及其KV缓存的生成，系统持续为逻辑块分配新的物理块。其内存高效性体现在两个关键设计：</p><ol type="1"><li>紧凑的内存布局：<ul><li>严格遵循从左到右的填充顺序；</li><li>仅当所有现存块写满时，才分配新物理块；</li><li>将内存浪费严格限制在单个块容量内。</li></ul></li><li>弹性资源共享：<ul><li>请求完成生成后，立即释放其KV块，供其他请求复用；</li></ul><blockquote><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/f79d4b0/image-3.png"> 如图所示：两个序列的逻辑块，可映射到不同的物理块，实现GPU节点的内存共享。</p></blockquote></li></ol><h4 id="vllm在其他解码场景的应用">vLLM在其他解码场景的应用</h4><h5 id="并行采样parallel-sampling">并行采样（Parallel Sampling）</h5><p>对于一个输入prompt，LLM生成多个输出采样。用户可从多个候选者中，选出最喜欢的输出。</p><p>并行采样场景中，<strong>单个请求包含：共享相同输入prompt的多个输出样本</strong>，这使得prompt的KV缓存也可被共享。借助PagedAttention和分页内存管理机制，vLLM能够轻松实现这种内存共享优化。共享机制的实现如下图： <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/f79d4b0/image-4.png"></p><ol type="1"><li>prompt阶段：双输出样本共享相同的prompt，因此只保留一份prompt状态的拷贝；<strong>两个序列的prompts对应逻辑块，映射至相同的物理块</strong>。<ul><li>逻辑块映射：序列A1/A2的逻辑块0 → 物理块7；序列A1/A2的逻辑块1 → 物理块1</li><li>物理块引用计数：物理块7和1的引用计数均为2</li></ul></li><li>generation阶段：<strong>写时复制机制（copy-on-write）</strong><ul><li>当样本A1需修改逻辑块1时：检测物理块1引用计数&gt;1；分配新物理块3并复制原数据；物理块1引用计数降为1</li><li>样本A2写入物理块1时：引用计数已为1，直接写入</li></ul></li></ol><p>vLLM的技术优势：</p><ul><li>内存节省：多个输出共享prompt的KV缓存，显著减少长提示词场景的内存占用；</li><li>安全隔离：块级写时复制，确保多样本修改隔离性；</li><li>零冗余设计：仅末位逻辑块需写时复制，其余物理块完全共享。</li></ul><h5 id="束搜索beam-search">束搜索（Beam Search）</h5><p>在机器翻译等LLM任务中，束搜索用于获取<strong>最优k个输出</strong>。通过束宽参数<span class="math inline">\(k\)</span>，控制每一步保留的候选序列数，有效避免全量遍历样本空间的计算复杂度。其工作流程分为三步：</p><ol type="1"><li>候选扩展：对束内的每个候选序列，枚举词汇表<span class="math inline">\(V\)</span>的所有可能续接tokens；</li><li>概率评估：调用LLM计算<span class="math inline">\(k\times |V|\)</span>个候选序列各自的生成概率（<span class="math inline">\(|V|\)</span>为词汇表大小）</li><li>择优保留：筛选概率最高的<span class="math inline">\(k\)</span>个序列，进入下一轮迭代。</li></ol><p>与并行解码不同，束搜索实现了更深层次的KV块共享机制：不止共享prompt对应block，<strong>不同候选序列也共享对应blocks，共享机制随着解码过程动态迭代</strong>。</p><ol type="1"><li>动态共享拓扑：<ul><li>所有候选序列，强制共享首个block（prompt block 0）</li><li>候选序列3从第2块开始分叉；候选序列0-2共享前3块，在第四块分叉</li><li>淘汰候选序列（0和3）时自动释放其逻辑块</li></ul></li><li>智能内存管理：<ul><li>引用计数归零的物理块即时释放；</li><li>为新候选序列动态分配物理块（块9-12）</li></ul></li></ol><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/f79d4b0/image-5.png"></p><h5 id="共享前缀">共享前缀</h5><p>在LLM应用中，用户通常需要提供<strong>包含instructions和example inputs/outputs</strong>的<strong>系统提示词（system prompt）</strong>，这些内容会与实际任务input拼接，形成完整prompt。此类共享prefix可通过提示词工程进一步微调，以提升下游任务的准确率。vLLM的实现方式如下：</p><ol type="1"><li><strong>预缓存机制</strong>：预先将共享prefix的KV缓存，存入专用物理块（类比OS对共享库的内存管理）；</li><li>动态映射：含有共享prefix的用户请求，可直接将逻辑块映射到已缓存的物理块（末位块标记为copy-on-write）；</li><li>计算优化：prompt phase仅需执行用户独有输入的计算（消除对共享prefix的冗余计算）</li></ol><h4 id="调度与抢占机制">调度与抢占机制</h4><p>当请求流量超过系统容量时，vLLM优先处理部分请求。vLLM采用<strong>先来先服务（FCFS）</strong>算法，以确保公平性并避免请求饥饿。</p><p>LLM服务面临的挑战有：输入prompts的长度差异显著；输出长度无法预知（由输入和模型行为决定）。随着请求数量和输出数量增加，VLLM可能会耗尽GPU的物理块，以致无法存储新生成的KV Cache。对此有两个亟需解决的问题：</p><ol type="1"><li><p><strong>块驱逐</strong>策略：通常使用启发式算法，预测最晚访问的物理块</p><ul><li><strong>全有或全无（All-or-Nothing）原则</strong>：同一序列的所有blocks，必须同时被驱逐或保留（由于一个序列的所有blocks同时被访问）；</li><li><strong>组调度（Gang-Scheduling）</strong>：同一请求内的多序列（如束搜索中的候选序列）作为<strong>序列组</strong>统一调度（需要避免破坏序列间潜在的内存共享关系）</li></ul></li><li><p><strong>驱逐块恢复</strong>：</p><ul><li><p><strong>内存交换（Swapping）</strong>：将被驱逐的KV块，暂存至CPU RAM。工作流程如下：</p><ul><li>一旦GPU中没有空闲的物理块以分配给新token，选择待驱逐的序列组；</li><li>将该序列组的所有KV块，整体迁移至CPU RAM（在此期间，vLLM暂停接收新需求，直至所有被抢占的序列迁移完成）；</li><li>一旦请求完成，从GPU内存释放其所有blocks，被抢占的序列从CPU中迁移回GPU，继续计算。</li></ul><blockquote><p>注意：CPU RAM永不超过GPU RAM中的物理块总数，因此：CPU RAM中交换空间大小严格受限于GPU显存容量。</p></blockquote></li><li><p><strong>重计算（Recomputation）</strong>：当被抢占的序列被重新调度时，重新计算其KV Cache。</p><ul><li>加速机制：将被抢占序列的已解码tokens，与原始用户prmpt，拼接形成新prompt；通过单次prompt阶段（prefill phase）并行，重构完整KV缓存。</li></ul></li></ul></li></ol><h4 id="分布式执行">分布式执行</h4><p>许多LLMs的参数量超过了单个GPU的容量。因此，需要将参数分区并分布到多个GPU上，并采用模型并行策略处理。vLLM通过以下机制实现分布式部署：</p><ol type="1"><li><p><strong>模型并行架构</strong>：<strong>Megatron-LM</strong>风格的<strong>张量并行</strong>策略</p><p>基于<strong>SPMD</strong>的执行模式：</p><ul><li><strong>线性层</strong>：块状矩阵乘法分区计算</li><li><strong>注意力层</strong>：按注意力头维度切分（每个SPMD进程处理一部分注意力头）</li><li><strong>同步机制</strong>：通过all-reduce操作同步中间结果</li></ul></li><li><p><strong>全局KV缓存管理</strong>：（每个GPU处理相同的输入tokens）</p><ul><li>采用<strong>集中式调度器</strong>统一管理：维护逻辑块到物理块的全局映射（所有GPU共享）；为每个请求，分配物理块ID</li><li>分布式存储：相同物理块ID在不同GPU存储不同内容（对应各自分片的注意力头KV Cache）；各GPU仅保留自身注意力头对应的KV Cache分片</li></ul></li></ol><h5 id="工作流程">工作流程</h5><ol type="1"><li><strong>调度器预处理阶段</strong>：<ul><li>对于batch中的每个请求，生成<strong>包含输入tokens的ID的集合</strong>，和<strong>逻辑-物理块映射表（Block Table）</strong>；</li><li>将控制信息（token IDs+Block Table）<strong>广播至所有GPU workers</strong>；</li></ul></li><li><strong>GPU workers并行计算阶段</strong>：<ul><li>注意力层：根据控制信息中的块表，读取对应的KV Cache；各worker独立处理分配的注意力头子集；</li><li>全局同步：通过all-reduce原语自动同步中间结果（无需调度器介入）</li></ul></li><li><strong>回收迭代结果</strong>：GPU workers将采样生成的tokens回传至调度器。</li></ol><p>vLLM仅需在每个解码迭代开始时，一次性同步由调度器下发的控制信息包含的内存状态；执行期间无需额外同步内存状态。</p><h3 id="implementation">Implementation</h3><p>vLLM作为端到端的LLM服务系统，采用分层架构设计：</p><ol type="1"><li><strong>前端接口层</strong>：基于FastAPI构建RESTful服务，完整支持OpenAI API协议；其可定制的参数包括：最大序列长度，束搜索宽度<span class="math inline">\(k\)</span>，温度系数等采样参数；</li><li><strong>核心引擎层</strong>：<strong>控制平台</strong>（8.5K Python代码）包括分布式调度器和块管理器；<strong>数据平台</strong>（2K C++/CUDA代码）包括PagedAttention定制内核和高并发内存操作原语。集成PyTorch与HuggingFace Transformers等，原生适配：GPT系列，OPT和LLaMA等主流架构。</li><li><strong>分布式通信层</strong>：基于NCCL实现跨GPU张量高效同步，和全兼容Megatron-LM的并行模式。</li></ol><h4 id="内核级优化">内核级优化</h4><p>针对PagedAttention的特有内存访问模式，vLLM开发了三大定制化GPU内核：</p><ol type="1"><li><strong>融合式KV缓存写入</strong>：在每个Transformer层，KV Cache被划分为若干个blocks，重构为一个为读取blocks而优化的内存布局，再按块表写入。<ul><li>传统方案需多次内核启动完成；而当前将三级操作融合为单一内核。</li></ul></li><li><strong>块感知注意力计算</strong>：基于FasterTransformer内核改造，使得每个GPU warp专门负责读取单个KV块，支持动态批处理（变长序列混合计算）。<ul><li>该方法强制合并内存访问，实现块内计算零拷贝。</li></ul></li><li><strong>批量块拷贝</strong>：传统的<code>cudaMemcpyAsync</code>导致碎片化小拷贝；因此该方法实现非连续块拷贝操作批量提交，采用写时复制。</li></ol><h4 id="解码算法支持框架">解码算法支持框架</h4><p>vLLM通过三大原子操作实现多样化解码算法：</p><table><thead><tr><th>操作</th><th>功能描述</th><th>典型应用场景</th></tr></thead><tbody><tr><td>fork</td><td>从现有序列克隆新序列</td><td>并行采样/束搜索候选分支</td></tr><tr><td>append</td><td>追加新tokens到指定序列</td><td>自回归生成迭代step</td></tr><tr><td>free</td><td>释放序列及其KV Cache</td><td>终止条件触发/低概率路径修剪</td></tr></tbody></table><h2 id="源码">源码</h2><p>vLLM整体架构如下，支持<strong>离线批处理（同步）</strong>和<strong>在线API服务（异步）</strong>。 <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/f79d4b0/image-6.png"></p><h3 id="调用方式">调用方式</h3><h4 id="离线批推理offline-batched-inference">离线批推理（Offline Batched Inference）</h4><p>从一个最基础的离线推理脚本开始：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Sample prompts.</span></span><br><span class="line">prompts = [</span><br><span class="line">    <span class="string">&quot;Hello, my name is&quot;</span>,</span><br><span class="line">    <span class="string">&quot;The president of the United States is&quot;</span>,</span><br><span class="line">    <span class="string">&quot;The capital of France is&quot;</span>,</span><br><span class="line">    <span class="string">&quot;The future of AI is&quot;</span>,</span><br><span class="line">]</span><br><span class="line"><span class="comment"># Create a sampling params object.</span></span><br><span class="line">sampling_params = SamplingParams(temperature=<span class="number">0.8</span>, top_p=<span class="number">0.95</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create an LLM.</span></span><br><span class="line">llm = LLM(model=<span class="string">&quot;facebook/opt-125m&quot;</span>)</span><br><span class="line"><span class="comment"># Generate texts from the prompts. The output is a list of RequestOutput objects that contain the prompt, generated text, and other information.</span></span><br><span class="line">outputs = llm.generate(prompts, sampling_params)</span><br><span class="line"><span class="comment"># Print the outputs.</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nGenerated Outputs:\n&quot;</span> + <span class="string">&quot;-&quot;</span> * <span class="number">60</span>)</span><br><span class="line"><span class="keyword">for</span> output <span class="keyword">in</span> outputs:</span><br><span class="line">    prompt = output.prompt</span><br><span class="line">    generated_text = output.outputs[<span class="number">0</span>].text</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Prompt:    <span class="subst">&#123;prompt!r&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Output:    <span class="subst">&#123;generated_text!r&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;-&quot;</span> * <span class="number">60</span>)</span><br></pre></td></tr></table></figure><code>llm = LLM(model="facebook/opt-125m")</code>实例化一个<code>LLM</code>对象，其本质是实例化一个<code>LLMEngine</code>对象，通过<code>EngineArgs</code>加载配置。<p></p><p>在离线批推理中，每次给模型发送推理请求时，需要<strong>整个batch的数据一起发送、推理、返回推理结果</strong>，称为（batch内部）<strong>同步</strong>。</p><h4 id="在线api服务api-server-for-online-serving">在线API服务（API Server For Online Serving）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">curl http://localhost:<span class="number">8000</span>/v1/completions \</span><br><span class="line">    -H <span class="string">&quot;Content-Type: application/json&quot;</span> \</span><br><span class="line">    -d <span class="string">&#x27;&#123;</span></span><br><span class="line"><span class="string">        &quot;model&quot;: &quot;Qwen/Qwen2.5-1.5B-Instruct&quot;,</span></span><br><span class="line"><span class="string">        &quot;prompt&quot;: &quot;San Francisco is a&quot;,</span></span><br><span class="line"><span class="string">        &quot;max_tokens&quot;: 7,</span></span><br><span class="line"><span class="string">        &quot;temperature&quot;: 0</span></span><br><span class="line"><span class="string">    &#125;&#x27;</span></span><br></pre></td></tr></table></figure><p><strong>异步</strong>请求推理：核心处理逻辑封装在<code>AsyncLLMEngine</code>类中（继承自<code>LLMEngine</code>）。</p><h3 id="从llm开始">从<code>LLM</code>开始</h3><ol type="1"><li>通过<code>EngineArgs</code>加载配置：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">engine_args = EngineArgs(...)</span><br></pre></td></tr></table></figure></li><li>创建<code>LLMEngine</code>引擎：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 使用配置好的engine参数，初始化LLMEngine实例</span></span><br><span class="line"><span class="variable language_">self</span>.llm_engine = LLMEngine.from_engine_args(</span><br><span class="line">	engine_args=engine_args, usage_context=UsageContext.LLM_CLASS)</span><br><span class="line"><span class="variable language_">self</span>.engine_class = <span class="built_in">type</span>(<span class="variable language_">self</span>.llm_engine)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;2. 用于全局唯一的request_id：</span></span><br><span class="line"><span class="string">在vLLM中内核引擎的处理中，1个prompt视为1个request，分配全局唯一的request_id&#x27;&#x27;&#x27;</span> </span><br><span class="line"><span class="variable language_">self</span>.request_counter = Counter()</span><br><span class="line"><span class="variable language_">self</span>.default_sampling_params: <span class="type">Union</span>[<span class="built_in">dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>], <span class="literal">None</span>] = <span class="literal">None</span></span><br></pre></td></tr></table></figure></li></ol><blockquote><p>进入<code>from_engine_args</code>函数，看看引擎的创建过程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">from_engine_args</span>(<span class="params"></span></span><br><span class="line"><span class="params">       cls,</span></span><br><span class="line"><span class="params">       engine_args: EngineArgs,</span></span><br><span class="line"><span class="params">       usage_context: UsageContext = UsageContext.ENGINE_CONTEXT,</span></span><br><span class="line"><span class="params">       stat_loggers: <span class="type">Optional</span>[<span class="type">Dict</span>[<span class="built_in">str</span>, StatLoggerBase]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">   </span>) -&gt; <span class="string">&quot;LLMEngine&quot;</span>:</span><br><span class="line">       <span class="comment"># 1. 生成引擎配置对象`vllm_config`</span></span><br><span class="line">       vllm_config = engine_args.create_engine_config(usage_context)</span><br><span class="line"></span><br><span class="line">       engine_cls = cls</span><br><span class="line">       <span class="keyword">if</span> envs.VLLM_USE_V1:</span><br><span class="line">           <span class="keyword">from</span> vllm.v1.engine.llm_engine <span class="keyword">import</span> LLMEngine <span class="keyword">as</span> V1LLMEngine</span><br><span class="line">           engine_cls = V1LLMEngine</span><br><span class="line">       <span class="comment"># 2. 创建引擎实例</span></span><br><span class="line">       <span class="keyword">return</span> engine_cls.from_vllm_config(</span><br><span class="line">           vllm_config=vllm_config,</span><br><span class="line">           usage_context=usage_context,</span><br><span class="line">           stat_loggers=stat_loggers,</span><br><span class="line">           disable_log_stats=engine_args.disable_log_stats,</span><br><span class="line">       )</span><br></pre></td></tr></table></figure><p></p><ol type="1"><li>引擎配置实例的生成函数：<a target="_blank" rel="noopener" href="https://github.com/vllm-project/vllm/blob/f7030df3be651bbce42932be736129d37caa856b/vllm/engine/arg_utils.py#L1155">create_engine_config</a>：将EngineArgs分解成ModelConfig，CacheConfig， ParallelConfig和SchedulerConfig，返回一个<code>VllmConfig</code>实例；</li><li>引擎实例的创建函数：<a target="_blank" rel="noopener" href="https://github.com/vllm-project/vllm/blob/aa3b3d76e0db63a4214b45805dc9bc3e5609c30e/vllm/engine/llm_engine.py#L491">from_vllm_config</a>：（工厂方法）使用传入的 <code>VllmConfig</code> 配置对象，创建并返回一个新的 <code>LLMEngine</code> 实例。</li></ol></blockquote><p>采用推理引擎<code>LLMEngine</code>，整体架构如下： <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/f79d4b0/image-7.png"></p><p>每个推理包含两个阶段：</p><ul><li>调度预处理阶段：<code>Scheduler</code>决定可参与推理的请求，为每个请求创建：<strong>包含输入tokens的ID的集合</strong>，和<strong>逻辑-物理块映射表</strong>；</li><li><strong>Worker并行计算阶段</strong>：将请求的控制信息，分发到各个worker上推理。<code>Worker</code>中的<code>CacheEngine</code>管理KV Cache；<code>Worker</code>中的model加载模型并开展推理。</li></ul><h3 id="模型执行器executor">模型执行器<code>Executor</code></h3><ul><li><p>首先，<strong>模型执行器的类型是如何指定的呢</strong>？：<strong><code>_get_executor_cls</code>函数</strong></p><p>在创建引擎实例的函数<code>from_vllm_config</code>中，有：<code>executor_class=cls._get_executor_cls(vllm_config),</code></p><p>来到<a target="_blank" rel="noopener" href="https://github.com/vllm-project/vllm/blob/aa3b3d76e0db63a4214b45805dc9bc3e5609c30e/vllm/engine/llm_engine.py#L453"><code>_get_executor_cls</code></a>函数：根据 <code>VllmConfig</code> 配置中的 <code>distributed_executor_backend</code> 配置，动态选择并返回合适的<strong>执行器类：均为<code>ExecutorBase</code>的子类</strong>。</p></li><li><p><strong>有哪些执行器类型可供选择呢？</strong></p><p><code>ExecutorBase</code>为执行器基类；<code>DistributedExecutorBase</code>继承<code>ExecutorBase</code>，为分布式执行器的基类。</p><ol type="1"><li><p><code>UniProcExecutor</code>（继承<code>ExecutorBase</code>）：在<strong>单个节点</strong>上启动<strong>单个进程</strong>（支持单个节点上的<strong>多个GPU</strong>）；</p><ul><li><p><code>ExecutorWithExternalLauncher</code>（继承<code>UniProcExecutor</code>）：专门与 <code>torchrun-compatible</code> 的启动器配合使用：</p><p>通过<code>torchrun</code><strong>启动多个引擎，每个引擎对应一个工作进程（worker），每个进程负责一个或多个设备（GPU）</strong>；</p><p>所有进程在处理相同的输入时会生成相同的输出，无需进行状态同步；</p><p>不支持流水线并行，执行张量并行。</p></li></ul></li><li><p><code>RayDistributedExecutor</code>（继承<code>DistributedExecutorBase</code>）：使用Ray集群进行分布式训练</p><ul><li><strong>进程数：启动多个 Ray worker，每个 worker 是一个独立的进程</strong>，负责执行推理任务；（进程的数量由 <code>world_size</code> 决定）</li><li><strong>设备数：每个 worker 指定使用的 GPU 数量（通过 <code>num_gpus</code> 配置）</strong></li><li><strong>节点数：执行器支持在多个节点上运行多个 worker；节点的分配通过 Ray placement group 管理</strong></li></ul></li><li><p><code>MultiprocessingDistributedExecutor</code>（继承<code>DistributedExecutorBase</code>）：基于 Python 多进程：</p><ul><li>支持在<strong>单节点</strong>（即只有一个物理机器）上运行；通过<code>world_size</code>指定创建的工作进程数；每个进程的任务由 <code>tensor_parallel_size</code> 分配。通过回环地址进行进程间通信。</li></ul></li></ol></li><li><p><strong>执行器是何时创建的呢？</strong></p><p>由<code>LLMEngine</code>的初始化函数中以下语句创建： <code>self.model_executor = executor_class(vllm_config=vllm_config, )；</code></p></li><li><p><strong>执行器的初始化流程是怎样的呢？</strong>：<code>self._init_executor()</code></p></li></ul><h4 id="执行器初始化_init_executor函数">执行器初始化：<code>_init_executor</code>函数</h4><h5 id="uniprocexecutor的初始化"><code>UniProcExecutor</code>的初始化</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_init_executor</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">   <span class="comment"># 1. 初始化驱动进程：driver_worker</span></span><br><span class="line">   <span class="variable language_">self</span>.driver_worker = WorkerWrapperBase(vllm_config=<span class="variable language_">self</span>.vllm_config,</span><br><span class="line">                                          rpc_rank=<span class="number">0</span>) <span class="comment"># 设置了进程的 rank 为 0</span></span><br><span class="line"></span><br><span class="line">   <span class="comment"># 2. 分布式初始化方法：获取当前机器的 IP 地址（get_ip()）和一个可用的端口号（get_open_port()）</span></span><br><span class="line">   distributed_init_method = get_distributed_init_method(</span><br><span class="line">      get_ip(), get_open_port())</span><br><span class="line">   </span><br><span class="line">   <span class="comment"># 3. 设置本地设备索引（GPU编号）：local_rank</span></span><br><span class="line">   local_rank = <span class="number">0</span></span><br><span class="line">   device_info = <span class="variable language_">self</span>.vllm_config.device_config.device.__str__().split(<span class="string">&quot;:&quot;</span>)</span><br><span class="line">   <span class="keyword">if</span> <span class="built_in">len</span>(device_info) &gt; <span class="number">1</span>:</span><br><span class="line">      local_rank = <span class="built_in">int</span>(device_info[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">   <span class="comment"># 4. 设置工作进程的 rank 和 is_driver_worker</span></span><br><span class="line">   rank = <span class="number">0</span></span><br><span class="line">   kwargs = <span class="built_in">dict</span>(</span><br><span class="line">      vllm_config=<span class="variable language_">self</span>.vllm_config,</span><br><span class="line">      local_rank=local_rank,</span><br><span class="line">      rank=rank,</span><br><span class="line">      distributed_init_method=distributed_init_method,</span><br><span class="line">      <span class="comment"># 若未启用并行配置：当前进程即为驱动进程</span></span><br><span class="line">      is_driver_worker=(<span class="keyword">not</span> <span class="variable language_">self</span>.parallel_config)</span><br><span class="line">      <span class="keyword">or</span> (rank % <span class="variable language_">self</span>.parallel_config.tensor_parallel_size == <span class="number">0</span>),</span><br><span class="line">   )</span><br><span class="line">   <span class="comment"># 5. 集体 RPC 调用</span></span><br><span class="line">   <span class="variable language_">self</span>.collective_rpc(<span class="string">&quot;init_worker&quot;</span>, args=([kwargs], ))</span><br><span class="line">   <span class="variable language_">self</span>.collective_rpc(<span class="string">&quot;init_device&quot;</span>)</span><br><span class="line">   <span class="variable language_">self</span>.collective_rpc(<span class="string">&quot;load_model&quot;</span>)</span><br></pre></td></tr></table></figure><blockquote><p>工作进程包装器（<code>WorkerWrapperBase</code>）：代表一个执行器中的一个进程，延迟初始化worker实例（真正的worker实例在<code>init_worker</code>中创建），控制worker的生命周期。</p></blockquote><h5 id="executorwithexternallauncher的初始化"><code>ExecutorWithExternalLauncher</code>的初始化</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_init_executor</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">   <span class="comment"># 1. 验证配置：</span></span><br><span class="line">   <span class="comment"># 确认执行器不支持管道并行，只使用张量并行</span></span><br><span class="line">   <span class="keyword">assert</span> <span class="variable language_">self</span>.vllm_config.parallel_config.pipeline_parallel_size == <span class="number">1</span>, \</span><br><span class="line">      (<span class="string">&quot;ExecutorWithExternalLauncher does not support pipeline parallelism.&quot;</span>)</span><br><span class="line">   <span class="comment"># 确保调度器的延迟因子为 0.0，保证执行是确定性的，即每个引擎产生相同输出，无需同步状态</span></span><br><span class="line">   <span class="keyword">assert</span> <span class="variable language_">self</span>.vllm_config.scheduler_config.delay_factor == <span class="number">0.0</span>, \</span><br><span class="line">      (<span class="string">&quot;ExecutorWithExternalLauncher needs deterministic &quot;</span></span><br><span class="line">      <span class="string">&quot;execution, so it does not support delay_factor in scheduling&quot;</span>)</span><br><span class="line">   <span class="keyword">if</span> envs.VLLM_USE_V1:</span><br><span class="line">      <span class="keyword">assert</span> <span class="keyword">not</span> envs.VLLM_ENABLE_V1_MULTIPROCESSING, \</span><br><span class="line">      (<span class="string">&quot;To get deterministic execution in V1, &quot;</span></span><br><span class="line">      <span class="string">&quot;please set VLLM_ENABLE_V1_MULTIPROCESSING=0&quot;</span>)</span><br><span class="line">   </span><br><span class="line">   <span class="comment"># 2. 初始化驱动进程（rpc_rank=0）</span></span><br><span class="line">   <span class="variable language_">self</span>.driver_worker = WorkerWrapperBase(vllm_config=<span class="variable language_">self</span>.vllm_config,</span><br><span class="line">                                          rpc_rank=<span class="number">0</span>)</span><br><span class="line">   <span class="comment"># 3. 设置分布式初始化方法：&quot;env://&quot;</span></span><br><span class="line">   distributed_init_method = <span class="string">&quot;env://&quot;</span></span><br><span class="line">   rank = <span class="built_in">int</span>(os.environ[<span class="string">&quot;RANK&quot;</span>])      <span class="comment"># 当前进程的全局rank</span></span><br><span class="line">   local_rank = <span class="built_in">int</span>(os.environ[<span class="string">&quot;LOCAL_RANK&quot;</span>]) <span class="comment"># 当前进程在本地节点上的 rank，通常对应 GPU 的编号</span></span><br><span class="line">   is_driver_worker = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">   <span class="comment"># 4. 调用 collective_rpc</span></span><br><span class="line">   kwargs = <span class="built_in">dict</span>(</span><br><span class="line">      vllm_config=<span class="variable language_">self</span>.vllm_config,</span><br><span class="line">      local_rank=local_rank,</span><br><span class="line">      rank=rank,</span><br><span class="line">      distributed_init_method=distributed_init_method,</span><br><span class="line">      is_driver_worker=is_driver_worker,</span><br><span class="line">   )</span><br><span class="line">   <span class="variable language_">self</span>.collective_rpc(<span class="string">&quot;init_worker&quot;</span>, args=([kwargs], ))</span><br><span class="line">   <span class="variable language_">self</span>.collective_rpc(<span class="string">&quot;init_device&quot;</span>)</span><br><span class="line">   <span class="variable language_">self</span>.collective_rpc(<span class="string">&quot;load_model&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="raydistributedexecutor初始化"><code>RayDistributedExecutor</code>初始化</h5><ul><li>提供两种优化路径：编译DAG+SPMD / 传统RPC</li><li>硬件适配：自动检测TPU（切换通信方式：NCCL转为shm共享内存）</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_init_executor</span>(<span class="params">self</span>)-&gt;<span class="literal">None</span>:</span><br><span class="line">  <span class="string">&#x27;&#x27;&#x27;初始化和配置&#x27;&#x27;&#x27;</span></span><br><span class="line">  <span class="comment"># 1. Ray集群初始化</span></span><br><span class="line">  initialize_ray_cluster(<span class="variable language_">self</span>.parallel_config)</span><br><span class="line">  placement_group = <span class="variable language_">self</span>.parallel_config.placement_group</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 2. 创建并行的GPU Workers</span></span><br><span class="line">  <span class="variable language_">self</span>._init_workers_ray(placement_group)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 3. 消息编解码器</span></span><br><span class="line">  <span class="variable language_">self</span>.input_encoder = msgspec.msgpack.Encoder(enc_hook=encode_hook)</span><br><span class="line">  <span class="variable language_">self</span>.output_decoder = msgspec.msgpack.Decoder(</span><br><span class="line">        <span class="type">Optional</span>[<span class="type">List</span>[SamplerOutput]])</span><br></pre></td></tr></table></figure><blockquote><p>初始化Ray集群（<code>initialize_ray_cluster</code>）：负责连接或创建Ray集群，并设置资源分配策略；</p><p>初始化工作进程(workers)（<code>_init_workers_ray</code>）：创建、配置和管理分布式LLM推理所需的所有工作节点。</p><ol type="1"><li>在Ray集群中，创建并配置多个工作进程：使用Ray的Placement Group确保GPU资源正确分配；每个worker绑定到特定的资源bundle</li><li>分布式通信：单节点使用127.0.0.1优化通信；多节点使用实际IP地址</li><li>支持并行模式：流水线并行和张量并行</li></ol></blockquote><h3 id="工作进程worker">工作进程<code>Worker</code></h3><h3 id="推理引擎llmengine">推理引擎<code>LLMEngine</code></h3><p><code>LLMEngine</code>是主要的执行引擎，用于处理从客户端接收的请求，执行文本生成任务，并返回生成的结果。该类包括一个tokenizer、一个Language model（可能分布在多个 GPU 上），以及分配给中间状态（即 KV Cache）的 GPU 内存空间。</p><h4 id="llmengine初始化"><code>LLMEngine</code>初始化</h4><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/f79d4b0/image-8.png"></p><ol type="1"><li><p>初始化 <code>tokenizer</code>（可选）：根据配置中的 <code>skip_tokenizer_init</code> 参数决定是否初始化 <code>tokenizer</code>（分词器）；</p></li><li><p><strong>序列计数器</strong>：<code>self.seq_counter = Counter()</code>，追踪生成的序列数量；</p></li><li><p><strong>输入预处理器</strong>：<code>self.input_preprocessor = InputPreprocessor(self.model_config,self.tokenizer,mm_registry)</code>，将处理输入数据并将其转换为模型能够理解的格式；</p></li><li><p><strong>模型执行器</strong>：<code>self.model_executor = executor_class(vllm_config=vllm_config, )</code>；</p><p>创建继承<code>ExecutorBase</code>基类的实例：初始化函数中包括<code>self._init_executor()</code></p></li><li><p><strong>KV Cache初始化</strong>：<code>self._initialize_kv_caches();</code>（如果模型的运行类型不是 <code>pooling</code>），用于存储推理过程中间结果，减少重复计算；</p></li><li><p><strong>使用统计信息</strong></p></li><li><p><strong>创建调度器</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable language_">self</span>.scheduler = [</span><br><span class="line">   Scheduler(</span><br><span class="line">     <span class="variable language_">self</span>.scheduler_config, <span class="variable language_">self</span>.cache_config, <span class="variable language_">self</span>.lora_config,</span><br><span class="line">     <span class="variable language_">self</span>.parallel_config.pipeline_parallel_size,</span><br><span class="line">     <span class="variable language_">self</span>.async_callbacks[v_id]</span><br><span class="line">     <span class="keyword">if</span> <span class="variable language_">self</span>.model_config.use_async_output_proc <span class="keyword">else</span> <span class="literal">None</span>)</span><br><span class="line"> <span class="keyword">for</span> v_id <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.parallel_config.pipeline_parallel_size)</span><br><span class="line">]</span><br></pre></td></tr></table></figure></li><li><p><strong>统计日志记录器</strong>：支持输出到 Prometheus 或本地日志；</p></li><li><p><strong>初始化输出处理器</strong>：创建输出处理器，用于处理生成的序列，支持序列生成技术（如 Beam Search 或推测解码）；</p></li><li><p>其他初始化：初始化 <code>self.seq_id_to_seq_group: Dict[str, SequenceGroupBase] = &#123;&#125;</code> 字典，跟踪序列的组信息。</p></li></ol><h5 id="初始化kv-cache_initialize_kv_caches">初始化KV Cache：<code>_initialize_kv_caches</code></h5><ul><li>决定在GPU Cache和CPU Cache中的block数量。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_initialize_kv_caches</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        start = time.time()</span><br><span class="line">        <span class="comment"># 1. 调用模型执行器：确定可用的 GPU 和 CPU 缓存块数</span></span><br><span class="line">        num_gpu_blocks, num_cpu_blocks = (</span><br><span class="line">            <span class="variable language_">self</span>.model_executor.determine_num_available_blocks())</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2. 若存在缓存块数的覆盖配置，则使用该覆盖值</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.cache_config.num_gpu_blocks_override <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            num_gpu_blocks_override = <span class="variable language_">self</span>.cache_config.num_gpu_blocks_override</span><br><span class="line">            logger.info(</span><br><span class="line">                <span class="string">&quot;Overriding num_gpu_blocks=%d with &quot;</span></span><br><span class="line">                <span class="string">&quot;num_gpu_blocks_override=%d&quot;</span>, num_gpu_blocks,</span><br><span class="line">                num_gpu_blocks_override)</span><br><span class="line">            num_gpu_blocks = num_gpu_blocks_override</span><br><span class="line">        <span class="comment"># 3. 更新缓存配置：将GPU 和 CPU 块数，保存到cache_config配置对象中</span></span><br><span class="line">        <span class="variable language_">self</span>.cache_config.num_gpu_blocks = num_gpu_blocks</span><br><span class="line">        <span class="variable language_">self</span>.cache_config.num_cpu_blocks = num_cpu_blocks</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 4. 初始化模型的缓存</span></span><br><span class="line">        <span class="variable language_">self</span>.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)</span><br><span class="line">        elapsed = time.time() - start</span><br><span class="line">        logger.info((<span class="string">&quot;init engine (profile, create kv cache, &quot;</span></span><br><span class="line">                     <span class="string">&quot;warmup model) took %.2f seconds&quot;</span>), elapsed)</span><br></pre></td></tr></table></figure>总流程如下： <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/f79d4b0/image-9.png"></li></ul><blockquote><p>调用两个模型执行器的函数：<code>ExecutorBase</code>类的方法（所有executor的基类）</p><ul><li><p><code>determine_num_available_blocks</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 远程调用（RPC）机制：向集群中的所有 worker 节点发出请求，收集每个节点上可用的缓存块数</span></span><br><span class="line">results = <span class="variable language_">self</span>.collective_rpc(<span class="string">&quot;determine_num_available_blocks&quot;</span>)</span><br><span class="line">a = <span class="built_in">min</span>([r[<span class="number">0</span>] <span class="keyword">for</span> r <span class="keyword">in</span> results])</span><br><span class="line">b = <span class="built_in">min</span>([r[<span class="number">1</span>] <span class="keyword">for</span> r <span class="keyword">in</span> results])</span><br></pre></td></tr></table></figure><p></p></li><li><p><code>initialize_cache</code>：通过底层的 worker初始化 KV 缓存</p><ul><li>计算<strong>最大并发量</strong>：推理过程中同时处理请求的最大数量。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># block_size 是每个缓存块的大小；max_model_len 是模型处理的最大序列长度</span></span><br><span class="line">max_concurrency = (num_gpu_blocks * <span class="variable language_">self</span>.cache_config.block_size /</span><br><span class="line">                      <span class="variable language_">self</span>.model_config.max_model_len)</span><br></pre></td></tr></table></figure></li><li>调用 <code>collective_rpc("initialize_cache", args=(num_gpu_blocks, num_cpu_blocks))</code> 来通知各个 worker 初始化缓存</li></ul></li></ul></blockquote><h6 id="worker前向推理determine_num_available_blocks"><code>Worker</code>前向推理：<code>determine_num_available_blocks</code></h6><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/f79d4b0/image-10.png" alt="alt text"> <strong>在模型部署的初始化阶段（推理正式开始前），vLLM通过模拟实验的方式，来决定gpu/cpu上到底有多少个KV cache物理块可分配给后续的请求做推理</strong>。这是如何完成的呢？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.inference_mode()</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">determine_num_available_blocks</span>(<span class="params">self</span>) -&gt; <span class="type">Tuple</span>[<span class="built_in">int</span>, <span class="built_in">int</span>]:</span><br></pre></td></tr></table></figure><ol type="1"><li><p><strong>内存分析准备</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.empty_cache()               <span class="comment"># 释放当前 CUDA 上的未使用内存</span></span><br><span class="line">torch.cuda.reset_peak_memory_stats()   <span class="comment"># 重置 GPU 内存的峰值统计信息</span></span><br><span class="line"><span class="comment"># 返回：当前GPU空闲内存 和 总GPU内存</span></span><br><span class="line">free_memory_pre_profile, total_gpu_memory = torch.cuda.mem_get_info()   </span><br></pre></td></tr></table></figure><p></p></li><li><p><strong>执行内存分析</strong>：调用<code>model_runner</code>的<code>profile_run</code>方法，调用<code>_dummy_run</code>模拟一次前向推理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> memory_profiling(</span><br><span class="line">        <span class="variable language_">self</span>.baseline_snapshot,</span><br><span class="line">        weights_memory=<span class="variable language_">self</span>.model_runner.model_memory_usage) <span class="keyword">as</span> result:</span><br><span class="line">    <span class="variable language_">self</span>.model_runner.profile_run()</span><br></pre></td></tr></table></figure><p></p><ul><li><code>profile_run</code>方法：调用<code>_dummy_run</code><ul><li><code>max_num_seqs</code>为在1个推理阶段中，LLMEngine<strong>最多能处理的seq数量</strong>；</li><li><code>max_num_batched_tokens</code>为1个推理阶段中，LLMEngine<strong>最多能处理的token数量</strong>。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.inference_mode()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">profile_run</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">  max_num_batched_tokens = \</span><br><span class="line">      <span class="variable language_">self</span>.scheduler_config.max_num_batched_tokens</span><br><span class="line">  max_num_seqs = <span class="variable language_">self</span>.scheduler_config.max_num_seqs</span><br><span class="line">  <span class="variable language_">self</span>._dummy_run(max_num_batched_tokens, max_num_seqs)</span><br></pre></td></tr></table></figure></li></ul></li></ul></li><li><p><strong>模拟一次前向推理</strong>：调用<code>model_runner</code>的<code>_dummy_run</code>，通过生成虚拟数据和配置来模拟一次模型的推理过程，帮助评估内存使用情况；并不涉及实际的训练过程。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_dummy_run</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                   max_num_batched_tokens: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                   max_num_seqs: <span class="built_in">int</span> = <span class="number">1</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">with</span> <span class="variable language_">self</span>.set_in_profile_run():</span><br><span class="line">            <span class="comment"># 1. 设置配置和采样参数: top-k采样</span></span><br><span class="line">            sampling_params = \</span><br><span class="line">                SamplingParams(top_p=<span class="number">0.99</span>, top_k=<span class="variable language_">self</span>.vocab_size - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 2. 构造LoRA请求：</span></span><br><span class="line">            dummy_lora_requests: <span class="type">List</span>[LoRARequest] = []</span><br><span class="line">            dummy_lora_requests_per_seq: <span class="type">List</span>[LoRARequest] = []</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.lora_config:</span><br><span class="line">               <span class="comment"># 调用 self._add_dummy_loras() 方法生成一组虚拟的 LoRA 请求（请求数为max_loras）</span></span><br><span class="line">                dummy_lora_requests = <span class="variable language_">self</span>._add_dummy_loras(  </span><br><span class="line">                    <span class="variable language_">self</span>.lora_config.max_loras)</span><br><span class="line">                <span class="keyword">assert</span> <span class="built_in">len</span>(dummy_lora_requests) == <span class="variable language_">self</span>.lora_config.max_loras</span><br><span class="line">               <span class="comment"># 每个序列都得到一个相应的 LoRA 请求</span></span><br><span class="line">                dummy_lora_requests_per_seq = [</span><br><span class="line">                    dummy_lora_requests[idx % <span class="built_in">len</span>(dummy_lora_requests)]</span><br><span class="line">                    <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(max_num_seqs)</span><br><span class="line">                ]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 3. 处理多模态数据（可能消耗更多GPU内存）：将batch_size设置为图片的最大数量</span></span><br><span class="line">            max_mm_tokens = <span class="variable language_">self</span>.mm_registry.get_max_multimodal_tokens(</span><br><span class="line">                <span class="variable language_">self</span>.model_config)     <span class="comment"># max_mm_tokens ：多模态数据中可用的最大 token 数量</span></span><br><span class="line">            <span class="keyword">if</span> max_mm_tokens &gt; <span class="number">0</span>:      <span class="comment"># 调整最大序列数max_num_seqs</span></span><br><span class="line">                max_num_seqs_orig = max_num_seqs</span><br><span class="line">                max_num_seqs = <span class="built_in">min</span>(max_num_seqs,</span><br><span class="line">                                   max_num_batched_tokens // max_mm_tokens)</span><br><span class="line">                <span class="keyword">if</span> max_num_seqs &lt; <span class="number">1</span>:</span><br><span class="line">                    expr = (<span class="string">f&quot;min(<span class="subst">&#123;max_num_seqs_orig&#125;</span>, &quot;</span></span><br><span class="line">                            <span class="string">f&quot;<span class="subst">&#123;max_num_batched_tokens&#125;</span> // <span class="subst">&#123;max_mm_tokens&#125;</span>)&quot;</span>)</span><br><span class="line">                    logger.warning(</span><br><span class="line">                        <span class="string">&quot;Computed max_num_seqs (%s) to be less than 1. &quot;</span></span><br><span class="line">                        <span class="string">&quot;Setting it to the minimum value of 1.&quot;</span>, expr)</span><br><span class="line">                    max_num_seqs = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 4. 循环为每个序列，生成虚拟输入数据：</span></span><br><span class="line">            seqs: <span class="type">List</span>[SequenceGroupMetadata] = []</span><br><span class="line">            batch_size = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> group_id <span class="keyword">in</span> <span class="built_in">range</span>(max_num_seqs):</span><br><span class="line">               <span class="comment"># seq_len 计算当前序列的长度，确保每个序列的长度总和等于 max_num_batched_tokens</span></span><br><span class="line">                seq_len = (max_num_batched_tokens // max_num_seqs +</span><br><span class="line">                           (group_id &lt; max_num_batched_tokens % max_num_seqs))</span><br><span class="line">                batch_size += seq_len</span><br><span class="line"></span><br><span class="line">               <span class="comment"># 调用dummy_data_for_profiling：生成用于分析的虚拟数据</span></span><br><span class="line">                dummy_data = <span class="variable language_">self</span>.input_registry \</span><br><span class="line">                    .dummy_data_for_profiling(<span class="variable language_">self</span>.model_config,</span><br><span class="line">                                            seq_len,</span><br><span class="line">                                            <span class="variable language_">self</span>.mm_registry)</span><br><span class="line"></span><br><span class="line">               <span class="comment"># 为每个序列创建一个 SequenceGroupMetadata 对象</span></span><br><span class="line">                seq = SequenceGroupMetadata(</span><br><span class="line">                    request_id=<span class="built_in">str</span>(group_id),</span><br><span class="line">                    is_prompt=<span class="literal">True</span>,</span><br><span class="line">                    seq_data=&#123;group_id: dummy_data.seq_data&#125;,</span><br><span class="line">                    sampling_params=sampling_params,</span><br><span class="line">                    block_tables=<span class="literal">None</span>,</span><br><span class="line">                    lora_request=dummy_lora_requests_per_seq[group_id]</span><br><span class="line">                    <span class="keyword">if</span> dummy_lora_requests_per_seq <span class="keyword">else</span> <span class="literal">None</span>,</span><br><span class="line">                    multi_modal_data=dummy_data.multi_modal_data,</span><br><span class="line">                    multi_modal_placeholders=dummy_data.</span><br><span class="line">                    multi_modal_placeholders,</span><br><span class="line">                )</span><br><span class="line">                seqs.append(seq)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 5. 创建并执行模型推理</span></span><br><span class="line">            <span class="comment"># Run the model with the dummy inputs.</span></span><br><span class="line">            num_layers = <span class="variable language_">self</span>.model_config.get_num_layers(<span class="variable language_">self</span>.parallel_config)</span><br><span class="line">            kv_caches = [           <span class="comment"># 为每个层创建一个空的张量缓存（float32）</span></span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">            1. 使用空tensor而非None：确保框架（如 PyTorch 的 Dynamo）在处理这些参数时，将它们作为引用传递，而不是根据参数的值（如 None）进行特殊化；</span></span><br><span class="line"><span class="string">            2. 在循环中每次创建新的张量，而不是通过列表复制，避免张量别名问题。</span></span><br><span class="line"><span class="string">            &#x27;&#x27;&#x27;</span></span><br><span class="line">                torch.tensor([], dtype=torch.float32, device=<span class="variable language_">self</span>.device)</span><br><span class="line">                <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_layers)</span><br><span class="line">            ]</span><br><span class="line">            finished_requests_ids = [seq.request_id <span class="keyword">for</span> seq <span class="keyword">in</span> seqs]</span><br><span class="line">            model_input = <span class="variable language_">self</span>.prepare_model_input(         <span class="comment"># 准备模型的输入数据</span></span><br><span class="line">                seqs, finished_requests_ids=finished_requests_ids)</span><br><span class="line">            intermediate_tensors = <span class="literal">None</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> get_pp_group().is_first_rank:</span><br><span class="line">                intermediate_tensors = \</span><br><span class="line">                    <span class="variable language_">self</span>.model.make_empty_intermediate_tensors(</span><br><span class="line">                    batch_size=batch_size,</span><br><span class="line">                    dtype=<span class="variable language_">self</span>.model_config.dtype,</span><br><span class="line">                    device=<span class="variable language_">self</span>.device)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 虚拟模型推理中，禁用键值比例计算</span></span><br><span class="line">            <span class="keyword">if</span> model_input.attn_metadata <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                model_input.attn_metadata.enable_kv_scales_calculation = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 执行模型推理 </span></span><br><span class="line">            <span class="variable language_">self</span>.execute_model(model_input, kv_caches, intermediate_tensors)</span><br><span class="line">            torch.cuda.synchronize()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 6. 清理之前添加的虚拟 LoRA 请求</span></span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.lora_config:</span><br><span class="line">                <span class="variable language_">self</span>._remove_dummy_loras()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span></span><br></pre></td></tr></table></figure><p></p></li><li><p>(回到<code>Worker</code>)可分配的KV cache物理块总数：</p></li></ol><ul><li><p><strong>分配给KV cache显存 = gpu总显存 -（不使用KV cache情况下）做1次FWD时的显存占用</strong></p><blockquote><p>对于“不使用KV cache做1次FWD时的显存占用”，使用上一步中模拟的一次FWD计算得出。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">memory_for_current_instance = total_gpu_memory * <span class="variable language_">self</span>.cache_config.gpu_memory_utilization</span><br><span class="line">available_kv_cache_memory = (memory_for_current_instance - result.non_kv_cache_memory)</span><br></pre></td></tr></table></figure><p></p></blockquote></li><li><p><strong>总物理块数量 = 分配给KV Cache的显存大小/ 物理块大小，其中“大小”的单位是bytes</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cache_block_size = <span class="variable language_">self</span>.get_cache_block_size_bytes()</span><br><span class="line"><span class="keyword">if</span> cache_block_size == <span class="number">0</span>:</span><br><span class="line">    num_gpu_blocks = <span class="number">0</span></span><br><span class="line">    num_cpu_blocks = <span class="number">0</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    num_gpu_blocks = <span class="built_in">int</span>(available_kv_cache_memory // cache_block_size)</span><br><span class="line">    num_cpu_blocks = <span class="built_in">int</span>(<span class="variable language_">self</span>.cache_config.swap_space_bytes // cache_block_size)</span><br><span class="line">num_gpu_blocks = <span class="built_in">max</span>(num_gpu_blocks, <span class="number">0</span>)</span><br><span class="line">num_cpu_blocks = <span class="built_in">max</span>(num_cpu_blocks, <span class="number">0</span>)</span><br></pre></td></tr></table></figure><p></p></li></ul><blockquote><p>这里抛出一个问题：GPU上物理块大小<code>cache_block_size</code>如何计算呢？</p><p>调用<code>CacheEngine</code>的<code>get_cache_block_size_bytes</code>方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">get_cache_block_size</span>(<span class="params"></span></span><br><span class="line"><span class="params">       cache_config: CacheConfig,</span></span><br><span class="line"><span class="params">       model_config: ModelConfig,</span></span><br><span class="line"><span class="params">       parallel_config: ParallelConfig,</span></span><br><span class="line"><span class="params">   </span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">       <span class="comment"># head_size：每个 Attention 头部 的大小（即每个头部的维度）</span></span><br><span class="line">       head_size = model_config.get_head_size()     </span><br><span class="line">       <span class="comment"># num_heads：KV Cache中使用的 Attention 头的数量</span></span><br><span class="line">       num_heads = model_config.get_num_kv_heads(parallel_config)</span><br><span class="line">       <span class="comment"># num_attention_layers：Attention 层 的数量</span></span><br><span class="line">       num_attention_layers = model_config.get_num_layers_by_block_type(</span><br><span class="line">           parallel_config, LayerBlockType.attention)</span><br><span class="line">       <span class="comment"># dtype：数据类型</span></span><br><span class="line">       <span class="keyword">if</span> cache_config.cache_dtype == <span class="string">&quot;auto&quot;</span>:</span><br><span class="line">           dtype = model_config.dtype</span><br><span class="line">       <span class="keyword">else</span>:</span><br><span class="line">           dtype = STR_DTYPE_TO_TORCH_DTYPE[cache_config.cache_dtype]</span><br><span class="line">       <span class="comment"># 每个Key Cache条目的大小：num_heads（头数）* head_size（每个头的大小）</span></span><br><span class="line">       key_cache_entry = num_heads * head_size</span><br><span class="line"></span><br><span class="line">       <span class="comment"># 每个Value Cache条目的大小：如果 模型使用 MLA，则没有Value Cache；如果 模型没有使用 MLA，则 value_cache_entry 等于 key_cache_entry</span></span><br><span class="line">       value_cache_entry = key_cache_entry <span class="keyword">if</span> <span class="keyword">not</span> model_config.use_mla <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">       <span class="comment"># 每个 KV Cache所需的总内存大小：</span></span><br><span class="line">       total = num_attention_layers * cache_config.block_size * \</span><br><span class="line">           (key_cache_entry + value_cache_entry)</span><br><span class="line"></span><br><span class="line">       dtype_size = get_dtype_size(dtype)</span><br><span class="line">       <span class="comment"># 缓存块的总大小</span></span><br><span class="line">       <span class="keyword">return</span> dtype_size * total</span><br></pre></td></tr></table></figure>总结：由大模型中KV值的定义，易知：<code>K_cache_block_size = block_size * num_heads * head_size * num_layers * dtype_size</code>。其中<code>dtype_size</code>表示精度对应的大小，例如<code>fp16</code>是2，<code>fp32</code>是4；<p></p><p>同理可知：<code>V_cache_block_size = K_cache_block_size</code></p><p>最终一个物理块的大小为：<code>cache_block_size = block_size * num_heads * head_size * num_layers * dtype_size * 2</code></p><p>CPU上物理块总数也是同理，但与GPU不同的是，它无需模拟前向推理。CPU上可用的内存总数由用户通过参数传入（默认4G）。</p></blockquote><h6 id="worker初始化-kv-cacheinitialize_cache"><code>Worker</code>初始化 KV Cache：<code>initialize_cache</code></h6><p><strong>在确定KV Cache Block的大小后，创建empty tensor，将其先放置到gpu上，实现显存的预分配</strong>。这是如何完成的呢？核心函数：<strong><code>_allocate_kv_cache</code></strong></p><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/f79d4b0/image-13.png"></p><p>回到<code>LLMEngine</code>初始化函数中，调用<code>_initialize_kv_caches</code>后，进入：<code>self.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)</code>，来看看模型执行器的<code>initialize_cache</code>函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">initialize_cache</span>(<span class="params">self, num_gpu_blocks: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                         num_cpu_blocks: <span class="built_in">int</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">      <span class="comment"># 1. 验证缓存大小：检查给定的缓存大小（num_gpu_blocks 和 block_size）是否有效；</span></span><br><span class="line">        raise_if_cache_size_invalid(...)</span><br><span class="line">      <span class="comment"># 2. 更新缓存配置：</span></span><br><span class="line">        <span class="variable language_">self</span>.cache_config.num_gpu_blocks = num_gpu_blocks</span><br><span class="line">        <span class="variable language_">self</span>.cache_config.num_cpu_blocks = num_cpu_blocks</span><br><span class="line">      <span class="comment"># 3. 选择内存池和分配方式：（是否启用休眠模式）</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.vllm_config.model_config.enable_sleep_mode:</span><br><span class="line">            allocator = CuMemAllocator.get_instance()</span><br><span class="line">            context = allocator.use_memory_pool(tag=<span class="string">&quot;kv_cache&quot;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">from</span> contextlib <span class="keyword">import</span> nullcontext</span><br><span class="line">            context = nullcontext()</span><br><span class="line">      <span class="comment"># 4. 内存池上下文管理：</span></span><br><span class="line">      <span class="comment"># 如果启用了休眠模式，则在进入上下文时，调用_init_cache_engine分配内存；否则直接继续。</span></span><br><span class="line">        <span class="keyword">with</span> context:</span><br><span class="line">            <span class="variable language_">self</span>._init_cache_engine()</span><br><span class="line">        <span class="variable language_">self</span>._warm_up_model()</span><br></pre></td></tr></table></figure><p>包括两个关键步骤：</p><ol type="1"><li><p><code>_init_cache_engine()</code>：创建一个<code>CacheEngine</code>对象，并初始化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># CacheEngine的初始化函数中，包括：</span></span><br><span class="line"><span class="variable language_">self</span>.gpu_cache = <span class="variable language_">self</span>._allocate_kv_cache(</span><br><span class="line">            <span class="variable language_">self</span>.num_gpu_blocks, <span class="variable language_">self</span>.device_config.device_type)</span><br><span class="line"><span class="variable language_">self</span>.cpu_cache = <span class="variable language_">self</span>._allocate_kv_cache(<span class="variable language_">self</span>.num_cpu_blocks, <span class="string">&quot;cpu&quot;</span>)</span><br></pre></td></tr></table></figure></li></ol><p><strong><code>_allocate_kv_cache</code>预分配KV Cache内存：为每个注意力层创建全零初始化的张量</strong></p><p>​ 大小为：<code>(2, num_blocks, block_size, num_kv_heads, head_size)</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_allocate_kv_cache</span>(<span class="params"></span></span><br><span class="line"><span class="params">	self,</span></span><br><span class="line"><span class="params">	num_blocks: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">	device: <span class="built_in">str</span>,</span></span><br><span class="line"><span class="params">	</span>) -&gt; <span class="type">List</span>[torch.Tensor]:</span><br><span class="line">	<span class="comment"># 1. 从注意力后端获取合适的缓存张量形状，记为：kv_cache_shape</span></span><br><span class="line">	kv_cache_shape = <span class="variable language_">self</span>.attn_backend.get_kv_cache_shape(</span><br><span class="line">	num_blocks, <span class="variable language_">self</span>.block_size, <span class="variable language_">self</span>.num_kv_heads, <span class="variable language_">self</span>.head_size)</span><br><span class="line">  <span class="comment"># 2. 设置内存锁定选项</span></span><br><span class="line">	pin_memory = is_pin_memory_available() <span class="keyword">if</span> device == <span class="string">&quot;cpu&quot;</span> <span class="keyword">else</span> <span class="literal">False</span></span><br><span class="line">  <span class="comment"># 3. 逐层分配缓存：为每个注意力层创建全零初始化的张量</span></span><br><span class="line">	kv_cache: <span class="type">List</span>[torch.Tensor] = []</span><br><span class="line">	<span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.num_attention_layers):</span><br><span class="line">		layer_kv_cache = torch.zeros(kv_cache_shape,</span><br><span class="line">                               dtype=<span class="variable language_">self</span>.dtype,</span><br><span class="line">                               pin_memory=pin_memory,</span><br><span class="line">                               device=device)</span><br><span class="line">		kv_cache.append(layer_kv_cache)</span><br><span class="line">	<span class="keyword">return</span> kv_cache</span><br></pre></td></tr></table></figure><ol start="2" type="1"><li><p><strong>模型预热<code>_warm_up_model</code></strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_warm_up_model</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">  <span class="comment"># 1. 确定预热尺寸：在非eager模式下，过滤掉已被CUDA图捕获的尺寸，避免重复工作</span></span><br><span class="line">	warmup_sizes = <span class="variable language_">self</span>.vllm_config.compilation_config.compile_sizes.copy()</span><br><span class="line">	<span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>.model_config.enforce_eager:</span><br><span class="line">		warmup_sizes = [</span><br><span class="line">			x <span class="keyword">for</span> x <span class="keyword">in</span> warmup_sizes <span class="keyword">if</span> x <span class="keyword">not</span> <span class="keyword">in</span></span><br><span class="line">			<span class="variable language_">self</span>.vllm_config.compilation_config.cudagraph_capture_sizes</span><br><span class="line">		]</span><br><span class="line">  <span class="comment"># 2. 按尺寸降序预热：通过_dummy_run执行虚拟推理，触发内核编译和缓存预热</span></span><br><span class="line">	<span class="keyword">for</span> size <span class="keyword">in</span> <span class="built_in">sorted</span>(warmup_sizes, reverse=<span class="literal">True</span>):</span><br><span class="line">		logger.info(<span class="string">&quot;Compile and warming up model for size %d&quot;</span>, size)</span><br><span class="line">		<span class="variable language_">self</span>.model_runner._dummy_run(size)</span><br><span class="line">  <span class="comment"># 3. CUDA图捕获：capture_model</span></span><br><span class="line">	<span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>.model_config.enforce_eager:</span><br><span class="line">		<span class="variable language_">self</span>.model_runner.capture_model(<span class="variable language_">self</span>.gpu_cache)</span><br><span class="line">	set_random_seed(<span class="variable language_">self</span>.model_config.seed)</span><br></pre></td></tr></table></figure><blockquote><p>调用<code>capture_model</code>方法，通过CUDA图捕获模型的计算图：</p><ol type="1"><li>主要支持小批量decoding场景（&lt;=200 tokens)：当批处理的token数量超过200时，CUDA图带来的性能提升不明显；</li><li>需要固定大小的tensor，不支持变长批处理</li><li>使用场景：仅支持decoding request的捕获（每个序列单个token）：不支持prefill request和chunked prefill+decoding</li></ol></blockquote></li></ol><h3 id="推理">推理</h3><h4 id="序列组sequencegroup">序列组<code>SequenceGroup</code></h4><h5 id="原生输入">原生输入</h5><h5 id="sequencegroup的作用"><code>SequenceGroup</code>的作用</h5><p>1个<code>SequenceGroup</code>实例包括："<strong>1个prompt -&gt; 多个outputs</strong>"</p><p><strong>一个seq_group中的所有seq共享1个prompt</strong></p><ul><li><p><strong>其中每组"prompt -&gt; output"组成一个序列（seq，属于Sequence实例），每个seq下有若干状态(status)属性（<code>class SequenceStatus(enum.IntEnum)</code>），包括</strong>：</p><ul><li><code>WAITING</code>：正在waiting队列中（waiting队列中的序列都没有做过prefill）；</li><li><code>RUNNING</code>：正在running队列中（即已经开始做推理）；</li><li><code>SWAPPED</code>：正在swapped队列中，表示：此时gpu资源不足，相关的seq_group被抢占，导致其暂停推理，相关的KV block被置换到cpu上（swap out）；等待gpu资源充足时再置换回来重新计算（swap in）；</li><li><code>FINISHED_STOPPED</code>：正常执行完毕（例如：碰到符号，该seq的推理正常结束）；</li><li><code>FINISHED_LENGTH_CAPPED</code>：因为seq的长度达到最大长度限制，而结束推理；</li><li><code>FINISHED_ABORTED</code>：因不正常状态，而被终止的推理。例如客户端断开连接，则服务器会终止相关seq的推理；</li><li><code>FINISHED_IGNORED</code>：因prompt过长而被终止执行的推理（本质上也是受到长度限制）</li></ul><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/f79d4b0/image-11.png"></p></li></ul><p>推理过程如下：</p><ul><li><p><strong>推理开始之前</strong>：seq_group下只有1条seq，它就是prompt，状态为waiting；</p></li><li><p><strong>在第1个推理阶段</strong>：调度器选中了这个seq_group，由于它的采样参数中n = 4，所以在做完prefill之后，它会生成4个seq，它们的状态都是running；</p></li><li><p><strong>在若干个推理阶段后，gpu上的资源不够了，这个seq_group不幸被调度器抢占</strong>：它相关的KV block也被swap out到cpu上。此时所有seq的状态变为swapped。注意：当一个seq_group被抢占时，对它的处理有两种方式：</p><ul><li><p><code>Swap</code>：如果该seq_group下的seq数量 &gt; 1，此时会采取swap策略，即把<strong>seq_group下所有seq的KV block从gpu上卸载到cpu上</strong>。（seq数量比较多，直接抛弃已计算的KV block，不划算）</p></li><li><p><code>Recomputation</code>：如果该seq_group下的seq数量 = 1，此时采取recomputation策略，即<strong>释放该seq_group相关的物理块，将其重新放回waiting队列中</strong>。等下次它被选中推理时，从prefill阶段开始重新推理。（seq数量少，重新计算KV block的成本不高）</p></li></ul></li><li><p><strong>又过了若干个推理阶段，gpu上的资源又充足了，此时执行swap in操作</strong>，将卸载到cpu上的KV block重新读到gpu上，继续对该seq_group做推理，此时seq的状态又变为running；</p></li><li><p><strong>又过了若干个推理阶段，该seq_group中有1个seq已经推理完成了，其状态被标记为finish</strong>，此后这条已经完成的seq将不参与调度；</p></li><li><p><strong>又过了若干个推理阶段，这个seq_group下所有的seq都已经完成推理了</strong>，此时可作为最终output返回。</p></li></ul><h5 id="sequencegroup结构"><code>SequenceGroup</code>结构</h5><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/f79d4b0/image-12.png"></p><ul><li><p><code>self.seqs_dict = &#123;seq.seq_id: seq for seq in seqs&#125;</code>：一个seq_group下包含若干seqs，其中每个seq是一个Sequence对象；</p></li><li><p><code>self.metrics</code>：<strong>记录该seq_group相关的指标</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable language_">self</span>.metrics = RequestMetrics(arrival_time=arrival_time,</span><br><span class="line">                              last_token_time=arrival_time,</span><br><span class="line">                              first_scheduled_time=<span class="literal">None</span>,</span><br><span class="line">                              first_token_time=<span class="literal">None</span>,</span><br><span class="line">                              time_in_queue=<span class="literal">None</span>,</span><br><span class="line">                              spec_token_acceptance_counts=[<span class="number">0</span>] * draft_size)</span><br></pre></td></tr></table></figure></li><li><p><code>get_max_num_running_steps</code>：<strong>该seq_group在剩余生命周期内，并行running的最大seq数量。“剩余生命周期”指从此刻一直到seq_group中所有的seq都做完推理</strong>。</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_max_num_running_seqs</span>(<span class="params">self</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.is_single_seq:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span> <span class="keyword">if</span> <span class="variable language_">self</span>.first_seq.is_finished() <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> <span class="variable language_">self</span>.num_seqs() - <span class="variable language_">self</span>.num_finished_seqs()</span><br></pre></td></tr></table></figure>&gt; 1. <code>num_seqs()</code>函数：获取符合指定 <code>status</code> 状态的所有序列，并返回其长度； &gt; 2. <code>get_finished_seqs()</code>：返回已经完成的序列的数量（包括：<code>FINISHED_STOPPED</code>, <code>FINISHED_LENGTH_CAPPED</code>, <code>FINISHED_ABORTED</code>,<code>FINISHED_IGNORED</code>共四种状态）<p></p></li></ul><p>离线批推理中，脚本包括以下两个关键步骤：</p><ol type="1"><li><code>llm = LLM(model="facebook/opt-125m")</code>：实例化一个离线批处理的vLLM对象：LLMEngine执行一次模拟实验（profiling），来判断需要在gpu上预留多少的显存空间给KV Cache block；</li><li><code>outputs = llm.generate(prompts, sampling_params)</code>：推理入口</li></ol><h4 id="入口函数generate">入口函数：<code>generate</code></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">generate</span>(<span class="params"></span></span><br><span class="line"><span class="params">	self,</span></span><br><span class="line"><span class="params">	prompts: <span class="type">Union</span>[<span class="type">Union</span>[PromptType, <span class="type">Sequence</span>[PromptType]],</span></span><br><span class="line"><span class="params">                 <span class="type">Optional</span>[<span class="type">Union</span>[<span class="built_in">str</span>, <span class="built_in">list</span>[<span class="built_in">str</span>]]]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">  <span class="comment"># sampling_params: 采样超参，例如温度、top_k等；如果为None则使用vLLM默认的参数</span></span></span><br><span class="line"><span class="params">	sampling_params: <span class="type">Optional</span>[<span class="type">Union</span>[SamplingParams,	</span></span><br><span class="line"><span class="params">                                  <span class="type">Sequence</span>[SamplingParams]]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">  <span class="comment"># prompt_token_ids: prompt对应的token_id，如果没有提供的话，vllm会调用tokenizer进行</span></span></span><br><span class="line"><span class="params">	prompt_token_ids: <span class="type">Optional</span>[<span class="type">Union</span>[<span class="built_in">list</span>[<span class="built_in">int</span>], <span class="built_in">list</span>[<span class="built_in">list</span>[<span class="built_in">int</span>]]]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">	<span class="comment"># use_tqdm: 是否要展示process bar</span></span></span><br><span class="line"><span class="params">  use_tqdm: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span><br><span class="line"><span class="params">  <span class="comment"># lora_request：如果想请求特定的lora_adapter，可以将它的path等信息包装在该请求中</span></span></span><br><span class="line"><span class="params">	lora_request: <span class="type">Optional</span>[<span class="type">Union</span>[<span class="built_in">list</span>[LoRARequest], LoRARequest]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">	<span class="comment"># prompt_adapter_request：提示器适配请求</span></span></span><br><span class="line"><span class="params">  prompt_adapter_request: <span class="type">Optional</span>[PromptAdapterRequest] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">  <span class="comment"># guided_options_request：引导器解码选项</span></span></span><br><span class="line"><span class="params">	guided_options_request: <span class="type">Optional</span>[<span class="type">Union</span>[LLMGuidedOptions,</span></span><br><span class="line"><span class="params">                                         GuidedDecodingRequest]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">	priority: <span class="type">Optional</span>[<span class="built_in">list</span>[<span class="built_in">int</span>]] = <span class="literal">None</span>,</span>) -&gt; <span class="built_in">list</span>[RequestOutput]:</span><br><span class="line">	runner_type = <span class="variable language_">self</span>.llm_engine.model_config.runner_type</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 1. 运行器类型验证：确保模型配置支持生成任务</span></span><br><span class="line">  <span class="keyword">if</span> runner_type <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&quot;generate&quot;</span>, <span class="string">&quot;transcription&quot;</span>]:</span><br><span class="line">    messages = [<span class="string">&quot;...&quot;</span>,]</span><br><span class="line">		supported_runner_types = <span class="variable language_">self</span>.llm_engine.model_config.supported_runner_types</span><br><span class="line">	<span class="keyword">if</span> <span class="string">&quot;generate&quot;</span> <span class="keyword">in</span> supported_runner_types:</span><br><span class="line">		messages.append(<span class="string">&quot;...&quot;</span>)</span><br><span class="line">	<span class="keyword">raise</span> ValueError(<span class="string">&quot; &quot;</span>.join(messages))</span><br><span class="line">	<span class="comment"># 2. 输入处理：支持直接传入token IDs或文本提示（兼容新旧两种输入格式）</span></span><br><span class="line">	<span class="keyword">if</span> prompt_token_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">		parsed_prompts = <span class="variable language_">self</span>._convert_v1_inputs(</span><br><span class="line">			prompts=cast(<span class="type">Optional</span>[<span class="type">Union</span>[<span class="built_in">str</span>, <span class="built_in">list</span>[<span class="built_in">str</span>]]], prompts),</span><br><span class="line">			prompt_token_ids=prompt_token_ids,)</span><br><span class="line">	<span class="keyword">else</span>:</span><br><span class="line">		parsed_prompts = cast(<span class="type">Union</span>[PromptType, <span class="type">Sequence</span>[PromptType]],prompts)</span><br><span class="line">	<span class="comment"># 3. 引导解码处理</span></span><br><span class="line">	<span class="keyword">if</span> <span class="built_in">isinstance</span>(guided_options_request, <span class="built_in">dict</span>):</span><br><span class="line">		<span class="keyword">if</span> <span class="built_in">len</span>(guided_options_request) &gt; <span class="number">1</span>:</span><br><span class="line">			<span class="keyword">raise</span> ValueError(<span class="string">&quot;...&quot;</span>)</span><br><span class="line">		guided_options_request = GuidedDecodingRequest(**guided_options_request)</span><br><span class="line">	<span class="comment"># 4. 采样参数处理</span></span><br><span class="line">	<span class="keyword">if</span> sampling_params <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">		sampling_params = <span class="variable language_">self</span>.get_default_sampling_params()</span><br><span class="line">	<span class="comment"># 5. 请求验证和添加</span></span><br><span class="line">	<span class="variable language_">self</span>._validate_and_add_requests(	<span class="comment"># 验证并添加所有请求到引擎</span></span><br><span class="line">		prompts=parsed_prompts,</span><br><span class="line">		params=sampling_params,</span><br><span class="line">		lora_request=lora_request,</span><br><span class="line">		prompt_adapter_request=prompt_adapter_request,</span><br><span class="line">		guided_options=guided_options_request,</span><br><span class="line">		priority=priority)</span><br><span class="line">	<span class="comment"># 6. 执行生成</span></span><br><span class="line">	outputs = <span class="variable language_">self</span>._run_engine(use_tqdm=use_tqdm)</span><br><span class="line">	<span class="keyword">return</span> <span class="variable language_">self</span>.engine_class.validate_outputs(outputs, RequestOutput)</span><br></pre></td></tr></table></figure><blockquote><p><code>_validate_and_add_requests</code>函数内：</p><p>逐个添加请求到引擎；支持优先级调度（默认优先级为0）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i, prompt <span class="keyword">in</span> <span class="built_in">enumerate</span>(prompts):</span><br><span class="line">	<span class="variable language_">self</span>._add_request(</span><br><span class="line">	prompt,</span><br><span class="line">	params[i] <span class="keyword">if</span> <span class="built_in">isinstance</span>(params, <span class="type">Sequence</span>) <span class="keyword">else</span> params,</span><br><span class="line">	lora_request=lora_request[i] <span class="keyword">if</span> <span class="built_in">isinstance</span>(</span><br><span class="line">	lora_request, <span class="type">Sequence</span>) <span class="keyword">else</span> lora_request,</span><br><span class="line">	prompt_adapter_request=prompt_adapter_request,</span><br><span class="line">	priority=priority[i] <span class="keyword">if</span> priority <span class="keyword">else</span> <span class="number">0</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p><code>_add_request</code>函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_add_request</span>(<span class="params"></span></span><br><span class="line"><span class="params">	self,</span></span><br><span class="line"><span class="params">	prompt: PromptType,</span></span><br><span class="line"><span class="params">	params: <span class="type">Union</span>[SamplingParams, PoolingParams],</span></span><br><span class="line"><span class="params">	lora_request: <span class="type">Optional</span>[LoRARequest] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">	prompt_adapter_request: <span class="type">Optional</span>[PromptAdapterRequest] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">	priority: <span class="built_in">int</span> = <span class="number">0</span>,</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">	request_id = <span class="built_in">str</span>(<span class="built_in">next</span>(<span class="variable language_">self</span>.request_counter))	<span class="comment"># 使用计数器 request_counter 生成唯一ID</span></span><br><span class="line">	<span class="variable language_">self</span>.llm_engine.add_request(</span><br><span class="line">		request_id, prompt, params, lora_request=lora_request, prompt_adapter_request=prompt_adapter_request, priority=priority,)</span><br></pre></td></tr></table></figure><p>调用案例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 多模态请求示例</span></span><br><span class="line"><span class="variable language_">self</span>._add_request(</span><br><span class="line">    prompt=&#123;<span class="string">&quot;text&quot;</span>: <span class="string">&quot;描述这张图片&quot;</span>, <span class="string">&quot;image&quot;</span>: image_tensor&#125;,</span><br><span class="line">    params=SamplingParams(top_p=<span class="number">0.9</span>),</span><br><span class="line">    prompt_adapter_request=ClipAdapterRequest()</span><br><span class="line">)</span><br><span class="line"><span class="comment"># 高优先级实时对话</span></span><br><span class="line"><span class="variable language_">self</span>._add_request(</span><br><span class="line">    prompt=<span class="string">&quot;生成下周会议摘要&quot;</span>,params=SamplingParams(temperature=<span class="number">0.3</span>),priority=<span class="number">100</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># 低延迟场景</span></span><br><span class="line"><span class="variable language_">self</span>._add_request(</span><br><span class="line">    prompt=[token1, token2, token3],  <span class="comment"># 预分词</span></span><br><span class="line">    params=PoolingParams(stride=<span class="number">128</span>),  <span class="comment"># 池化模式</span></span><br><span class="line">    priority=<span class="number">0</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure></blockquote><p>当一条请求到来时，流程如下： <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/f79d4b0/image-14.png"></p><p><code>generate</code>函数实际上做了两件事情：</p><ol type="1"><li><code>_add_request</code>：<strong>将输入数据传给LLMEngine</strong>：<ul><li><strong>把每1个prompt包装成一个SequenceGroup对象</strong>：从客户端角度看，1个请求可能包含多个prompts，例如离线批处理场景下，可以将1个batch理解成1个请求；但是<strong>从LLMEngine的角度看，1个prompt是1个请求</strong>，所以它会对输入数据进行预处理；</li><li><strong>把包装成SequenceGroup对象的数据加入调度器（Scheduler）的waiting队列，等待处理</strong>。</li></ul></li><li><code>_run_engine</code>：<strong>执行推理</strong>。只要调度器的waiting/running/swapped队列非空，就认为此时这批batch还没有做完推理，这时会调用LLMEngine的<code>step()</code>函数，来完成1次调度以决定要送哪些数据去做推理。</li></ol><h4 id="add_request接收用户请求"><code>add_request</code>：接收用户请求</h4><ul><li>功能：将请求添加到引擎的请求池中，并在调度器的 <code>engine.step()</code> 被调用时，处理这些请求。</li></ul><p>先做输入有效性检查（<code>prompt</code>和<code>params</code>不为None；<code>lora_request</code>请求出现时，配置中是否启用LoRA；是否支持优先级调度；是否启用引导解码等）；设置请求到达时间（若无，则使用当前时间）；进行分词器验证；使用<code>input_preprocessor</code>对传入的 <code>prompt</code>、<code>lora_request</code> 和 <code>prompt_adapter_request</code> 进行预处理，转为适合模型处理的格式。</p><p>最后，将请求添加到请求池：<code>self._add_processed_request(...)</code></p><h4 id="add_processed_request请求添加至请求池"><code>_add_processed_request</code>：请求添加至请求池</h4><ul><li>功能：处理请求，生成相应的序列；根据当前调度器的负载情况（未完成的序列数量），选择最适合的调度器，<strong>将序列组添加到调度队列</strong>中。</li></ul><ol type="1"><li><p>处理多采样请求：如果采样请求需要多个序列（即 <code>params.n &gt; 1</code>），将请求添加到 <code>ParallelSampleSequenceGroup</code> 中进行并行处理，方法直接返回 <code>None</code>；</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="built_in">isinstance</span>(params, SamplingParams) <span class="keyword">and</span> params.n &gt; <span class="number">1</span>:</span><br><span class="line">    ParallelSampleSequenceGroup.add_request(</span><br><span class="line">        ......</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure><p></p></li><li><p><strong>创建序列</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 加载每个KV cache block的大小（默认为16）；</span></span><br><span class="line">block_size = <span class="variable language_">self</span>.cache_config.block_size</span><br><span class="line">seq_id = <span class="built_in">next</span>(<span class="variable language_">self</span>.seq_counter)     <span class="comment"># 当前seq的id</span></span><br><span class="line">eos_token_id = <span class="variable language_">self</span>.input_preprocessor.get_eos_token_id(lora_request) <span class="comment"># 结束符token ID</span></span><br><span class="line"></span><br><span class="line">// <span class="number">2.</span> <span class="built_in">input</span>拆分为：编码器、解码器输入</span><br><span class="line">encoder_inputs, decoder_inputs = split_enc_dec_inputs(processed_inputs)</span><br><span class="line"></span><br><span class="line">// <span class="number">3.</span> 创建序列：</span><br><span class="line">seq = <span class="type">Sequence</span>(seq_id, decoder_inputs, block_size, eos_token_id,</span><br><span class="line">                       lora_request, prompt_adapter_request)</span><br><span class="line"></span><br><span class="line">encoder_seq = (<span class="literal">None</span> <span class="keyword">if</span> encoder_inputs <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="type">Sequence</span>(</span><br><span class="line">            seq_id, encoder_inputs, block_size, eos_token_id, lora_request,</span><br><span class="line">            prompt_adapter_request))</span><br></pre></td></tr></table></figure><p></p></li><li><p><strong>每个prompt被包装成一个<code>SequenceGroup</code>实例</strong>：</p><p>根据<code>params</code>创建<code>SequenceGroup</code>：是<code>SamplingParams</code>，创建采样序列组；是<code>PoolingParams</code>，创建池化序列组。</p><blockquote><ol type="1"><li><code>SamplingParams</code>：调用<code>_create_sequence_group_with_sampling</code>函数</li></ol><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_create_sequence_group_with_sampling</span>(<span class="params"></span></span><br><span class="line"><span class="params">	self, request_id: <span class="built_in">str</span>, seq: <span class="type">Sequence</span>, sampling_params: SamplingParams, arrival_time: <span class="built_in">float</span>, lora_request: <span class="type">Optional</span>[LoRARequest], trace_headers: <span class="type">Optional</span>[Mapping[<span class="built_in">str</span>, <span class="built_in">str</span>]] = <span class="literal">None</span>, prompt_adapter_request: <span class="type">Optional</span>[PromptAdapterRequest] = <span class="literal">None</span>, encoder_seq: <span class="type">Optional</span>[<span class="type">Sequence</span>] = <span class="literal">None</span>, priority: <span class="built_in">int</span> = <span class="number">0</span>,</span>) -&gt; SequenceGroup:</span><br><span class="line">	<span class="comment"># 1. 验证Logprobs参数</span></span><br><span class="line">	max_logprobs = <span class="variable language_">self</span>.get_model_config().max_logprobs</span><br><span class="line">	<span class="keyword">if</span> (sampling_params.logprobs</span><br><span class="line">		<span class="keyword">and</span> sampling_params.logprobs &gt; max_logprobs) <span class="keyword">or</span> (</span><br><span class="line">			sampling_params.prompt_logprobs</span><br><span class="line">			<span class="keyword">and</span> sampling_params.prompt_logprobs &gt; max_logprobs):</span><br><span class="line">		<span class="keyword">raise</span> ValueError(<span class="string">f&quot;Cannot request more than <span class="subst">&#123;max_logprobs&#125;</span> logprobs.&quot;</span>)</span><br><span class="line">	<span class="comment"># 2. 构建Logits处理器：用于调整生成过程中的 logits 值</span></span><br><span class="line">	sampling_params = <span class="variable language_">self</span>._build_logits_processors(sampling_params, lora_request)</span><br><span class="line">	<span class="comment"># 3. 复制采样参数：对 sampling_params 进行防御性复制（clone），确保在后续操作中不会修改原始的采样参数</span></span><br><span class="line">	sampling_params = sampling_params.clone()</span><br><span class="line">  <span class="comment"># 4. 更新生成配置</span></span><br><span class="line">	sampling_params.update_from_generation_config(</span><br><span class="line">		<span class="variable language_">self</span>.generation_config_fields, seq.eos_token_id)</span><br><span class="line">	<span class="comment"># 5. 创建序列组：</span></span><br><span class="line">  <span class="comment"># 5.1 确定draft_size：如果配置中启用了推测性解码（speculative_config），则根据推测性解码的配置调整 draft_size</span></span><br><span class="line">	draft_size = <span class="number">1</span></span><br><span class="line">	<span class="keyword">if</span> <span class="variable language_">self</span>.vllm_config.speculative_config <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">		draft_size = \</span><br><span class="line">			<span class="variable language_">self</span>.vllm_config.speculative_config.num_speculative_tokens + <span class="number">1</span></span><br><span class="line">  <span class="comment"># 5.2 创建 SequenceGroup 对象</span></span><br><span class="line">	seq_group = SequenceGroup(</span><br><span class="line">	    request_id=request_id, </span><br><span class="line">     seqs=[seq], </span><br><span class="line">     arrival_time=arrival_time, </span><br><span class="line">     sampling_params=sampling_params, </span><br><span class="line">     lora_request=lora_request, </span><br><span class="line">     trace_headers=trace_headers, </span><br><span class="line">     prompt_adapter_request=prompt_adapter_request,</span><br><span class="line"> 		encoder_seq=encoder_seq, </span><br><span class="line">     priority=priority, </span><br><span class="line">     draft_size=draft_size)</span><br><span class="line">	<span class="keyword">return</span> seq_group</span><br></pre></td></tr></table></figure><p></p></blockquote></li><li><p><strong>选择最空闲的调度器</strong>，添加序列组：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">costs = [</span><br><span class="line">    scheduler.get_num_unfinished_seq_groups()</span><br><span class="line">    <span class="keyword">for</span> scheduler <span class="keyword">in</span> <span class="variable language_">self</span>.scheduler</span><br><span class="line">]</span><br><span class="line">min_cost_scheduler = <span class="variable language_">self</span>.scheduler[costs.index(<span class="built_in">min</span>(costs))]</span><br><span class="line">min_cost_scheduler.add_seq_group(seq_group)</span><br></pre></td></tr></table></figure><p></p></li></ol><blockquote><ol type="1"><li>如何定义最空闲的调度器？</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_num_unfinished_seq_groups</span>(<span class="params">self</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">       <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.waiting) + <span class="built_in">len</span>(<span class="variable language_">self</span>.running) + <span class="built_in">len</span>(<span class="variable language_">self</span>.swapped);</span><br></pre></td></tr></table></figure><ol start="2" type="1"><li><code>add_seq_group</code>：将<code>seq_group</code>中所有序列，添加进scheduler的<code>self.waiting</code>队列中<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">add_seq_group</span>(<span class="params">self, seq_group: SequenceGroup</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">  <span class="variable language_">self</span>.swapped.append(seq_group)</span><br></pre></td></tr></table></figure></li></ol></blockquote><p>回到入口函数<code>generate</code>，在<code>_validate_and_add_requests</code>函数之后，所有的<code>seq_group</code>都已经被送入调度器（Scheduler）的<code>waiting</code>队列中。</p><p>接下来通过<code>_run_engine</code>执行推理：在1个推理阶段中，调用一次<code>step</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_run_engine</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self, *, use_tqdm: <span class="built_in">bool</span></span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="built_in">list</span>[<span class="type">Union</span>[RequestOutput, PoolingRequestOutput]]:</span><br><span class="line">    <span class="comment"># 1. 初始化进度条</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;...&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. 初始化输出列表和统计变量：</span></span><br><span class="line">    <span class="comment"># outputs 存储引擎产生的输出；total_in_toks 和 total_out_toks 分别跟踪输入和输出的总 token 数</span></span><br><span class="line">    outputs: <span class="built_in">list</span>[<span class="type">Union</span>[RequestOutput, PoolingRequestOutput]] = []</span><br><span class="line">    total_in_toks = <span class="number">0</span></span><br><span class="line">    total_out_toks = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. 处理未完成请求： step()完成1次调度</span></span><br><span class="line">    <span class="keyword">while</span> <span class="variable language_">self</span>.llm_engine.has_unfinished_requests():</span><br><span class="line">        step_outputs = <span class="variable language_">self</span>.llm_engine.step()</span><br><span class="line">        <span class="comment"># 4. 遍历 step_outputs 中的每个output：如果输出已完成，则将其加入到 outputs 列表中</span></span><br><span class="line">        <span class="keyword">for</span> output <span class="keyword">in</span> step_outputs:</span><br><span class="line">            <span class="keyword">if</span> output.finished:</span><br><span class="line">                outputs.append(output)</span><br><span class="line">                <span class="keyword">if</span> use_tqdm:</span><br><span class="line">                    <span class="keyword">if</span> <span class="built_in">isinstance</span>(output, RequestOutput):</span><br><span class="line">                        <span class="string">&#x27;&#x27;&#x27;...&#x27;&#x27;&#x27;</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        pbar.update(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> use_tqdm:</span><br><span class="line">        pbar.close()</span><br><span class="line">    <span class="comment"># 5. 按照请求 ID 对输出进行排序</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sorted</span>(outputs, key=<span class="keyword">lambda</span> x: <span class="built_in">int</span>(x.request_id))</span><br></pre></td></tr></table></figure><p></p><p>接下来的问题是：<code>step()</code>中如何决定送哪些<code>seq_group</code>去做推理呢？先来看看调度器的结构。</p><h3 id="调度器scheduler">调度器<code>Scheduler</code></h3><h4 id="调度器结构">调度器结构</h4><p>调度器重要属性如下： <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/f79d4b0/image-16.png"></p><ul><li><p><code>self.waiting, self.running, self.swapped</code>（双端队列：均通过<code>Deque[SequenceGroup] = deque()</code>初始化）：</p><ul><li><p><strong>waiting队列</strong>：存放所有<strong>尚未开始推理（未经历prefill阶段）或被抢占的seq_group</strong>；初始化时，waiting队列中的seq_group只有一个seq，即原始的prompt；</p></li><li><p><strong>running队列</strong>：存放<strong>当前正在做推理的seq_group</strong>。更准确地说，它存放的是：<strong>上1个推理阶段被送去推理的所有seq_group</strong>；在开始新一轮推理阶段时，调度器会根据本轮的筛选结果，更新running队列，即决定本轮要送哪些seq_group去做推理；</p></li><li><p><strong>swapped队列</strong>：存放<strong>被抢占的seq_group</strong>。若一个seq_group被抢占，调度器会对它执行swap或recomputation操作，分别对应着将它送去swapped队列或waiting队列。</p></li></ul></li></ul><h4 id="整体调度流程">整体调度流程</h4><p><code>_schedule_default</code>： 调度待执行的SequenceGroup，在调度过程中根据当前的资源状况（例如 GPU 内存），优先处理prefill请求并按需调度decode请求。最终返回一个包含调度结果的 SchedulerOutputs 对象。</p><p>预算由<code>SchedulingBudget</code>定义：<code>max_num_seqs</code>，<code>max_num_batched_tokens</code>分别为1个推理阶段中，LLMEngine<strong>最多能处理的seq数量</strong>和<strong>最多能处理的token数量</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">budget = SchedulingBudget(</span><br><span class="line">        token_budget=<span class="variable language_">self</span>.scheduler_config.max_num_batched_tokens,</span><br><span class="line">        max_num_seqs=<span class="variable language_">self</span>.scheduler_config.max_num_seqs,</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p></p><ul><li><p><strong>如果当前swapped队列为空</strong>：<strong>检查是否能从waiting队列中调度seq_group</strong>（调用<code>_schedule_prefills</code>），直到不满足调度条件为止（gpu空间不足，或waiting队列已为空等）。<strong>此时，1个推理阶段中，所有的seq_group都处在prefill阶段。</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>.swapped:</span><br><span class="line">    prefills = <span class="variable language_">self</span>._schedule_prefills(budget,</span><br><span class="line">                                        curr_loras,</span><br><span class="line">                                        enable_chunking=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>#### <code>_schedule_prefills</code>：从waiting队列中调度seq_group（调度的prefill） 初始化：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ignored_seq_groups: <span class="type">List</span>[SequenceGroup] = []</span><br><span class="line">seq_groups: <span class="type">List</span>[ScheduledSequenceGroup] = []</span><br><span class="line">waiting_queue = <span class="variable language_">self</span>.waiting</span><br><span class="line">leftover_waiting_sequences: Deque[SequenceGroup] = deque()</span><br></pre></td></tr></table></figure><p></p></li><li><p><code>ignored_seq_groups</code>：存储被忽略的<code>seq_group</code>，即某个<code>seq_group</code>无法在当前调度中被处理（例如，因为资源不足或超出了容量限制）,包括以下两种情况：</p><ul><li><code>num_new_tokens &gt; prompt_limit</code></li><li><code>can_allocate == AllocStatus.NEVER</code>：block_manager无法分配物理块（容量不够）</li></ul></li><li><p><code>seq_groups</code>：存储已成功调度并开始执行的<code>seq_group</code>，每个<code>seq_group</code>在成功调度后，都会被包装为一个<code>ScheduledSequenceGroup</code>对象，并添加到这个列表中。同时，调度信息（例如新分配的 token 数量）也会被更新到<code>budget</code>中；</p></li><li><p><code>leftover_waiting_sequences</code>：存储因某些原因（<code>partial_prefill_metadata</code>非空且不支持调度；没有额外空间分配给新的LoRA请求）暂时无法调度的<code>seq_group</code>。最后，未能调度的<code>seq_group</code>被重新加入<code>waiting_queue</code>中，等待下次调度。</p></li><li><p><code>waiting_queue</code>：当前处于等待状态的<code>seq_group</code>队列。即已经进入调度系统，但还没有被分配资源来执行。在调度过程中，代码会逐个检查<code>waiting_queue</code>中的<code>seq_group</code>（以下while循环）：</p><ul><li>成功调度：从队列中移除，从状态从<code>WAITING</code>转为<code>RUNNING</code>；</li><li>不能调度：留在队列中，直到符合调度条件为止。</li></ul></li></ul><ol type="1"><li><p>waiting队列循环：</p><ul><li>当前时间到达waiting队列的调度间隔阈值，且waiting队列非空：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="variable language_">self</span>._passed_delay(time.time()) <span class="keyword">and</span> waiting_queue:</span><br><span class="line">    <span class="comment"># 1. 取出最早到达的seq_group</span></span><br><span class="line">    seq_group = waiting_queue[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    waiting_seqs = seq_group.get_seqs(status=SequenceStatus.WAITING)</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;......&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;计算给定seq_group中，缓存/未缓存的tokens数：</span></span><br><span class="line"><span class="string">    遍历seq_group中的每个seq:</span></span><br><span class="line"><span class="string">        1. 解码序列：当前序列每次生成一个新的未缓存的token；</span></span><br><span class="line"><span class="string">        2. 预填充序列：all_num_new_tokens_seq=seq总长度-该seq已计算的tokens数量</span></span><br><span class="line"><span class="string">            2.1 未启用前缀缓存：所有的新token都视为未缓存的token，直接计入；</span></span><br><span class="line"><span class="string">                即：num_uncached_new_tokens += all_num_new_tokens_seq</span></span><br><span class="line"><span class="string">            2.2 启用前缀缓存：获取当前seq缓存的tokens数量，即：</span></span><br><span class="line"><span class="string">                num_cached_tokens_seq = self.block_manager.get_num_cached_tokens(</span></span><br><span class="line"><span class="string">                seq)</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    num_new_tokens_uncached, num_new_tokens_cached = (</span><br><span class="line">        <span class="variable language_">self</span>._get_num_new_uncached_and_cached_tokens(</span><br><span class="line">            seq_group,</span><br><span class="line">            SequenceStatus.WAITING,</span><br><span class="line">            enable_chunking,</span><br><span class="line">            budget,</span><br><span class="line">            partial_prefill_metadata=partial_prefill_metadata,</span><br><span class="line">        ))</span><br><span class="line">    num_new_tokens = num_new_tokens_uncached + num_new_tokens_cached</span><br><span class="line"></span><br><span class="line">   <span class="string">&#x27;&#x27;&#x27;...&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># If the sequence group cannot be allocated, stop.</span></span><br><span class="line">    <span class="comment"># 2. block_manager判断：是否有充足gpu空间，为该seq_group分配物理块，用于prefill</span></span><br><span class="line">    can_allocate = <span class="variable language_">self</span>.block_manager.can_allocate(</span><br><span class="line">        seq_group, num_lookahead_slots=num_lookahead_slots)</span><br><span class="line">    <span class="keyword">if</span> can_allocate == AllocStatus.LATER:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">elif</span> can_allocate == AllocStatus.NEVER:</span><br><span class="line">        logger.warning(</span><br><span class="line">            <span class="string">&quot;Input prompt (%d tokens) + lookahead slots (%d) is &quot;</span></span><br><span class="line">            <span class="string">&quot;too long and exceeds the capacity of block_manager&quot;</span>,</span><br><span class="line">            num_new_tokens,</span><br><span class="line">            num_lookahead_slots,</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">for</span> seq <span class="keyword">in</span> waiting_seqs:</span><br><span class="line">            seq.status = SequenceStatus.FINISHED_IGNORED</span><br><span class="line">        ignored_seq_groups.append(seq_group)</span><br><span class="line">        waiting_queue.popleft()</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">    lora_int_id = <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.lora_enabled:</span><br><span class="line">        lora_int_id = seq_group.lora_int_id</span><br><span class="line">        <span class="keyword">assert</span> curr_loras <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="variable language_">self</span>.lora_config <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="variable language_">self</span>.lora_enabled <span class="keyword">and</span> lora_int_id &gt; <span class="number">0</span></span><br><span class="line">                <span class="keyword">and</span> lora_int_id <span class="keyword">not</span> <span class="keyword">in</span> curr_loras</span><br><span class="line">                <span class="keyword">and</span> <span class="built_in">len</span>(curr_loras) &gt;= <span class="variable language_">self</span>.lora_config.max_loras):</span><br><span class="line">            <span class="comment"># We don&#x27;t have a space for another LoRA, so</span></span><br><span class="line">            <span class="comment"># we ignore this request for now.</span></span><br><span class="line">            leftover_waiting_sequences.appendleft(seq_group)</span><br><span class="line">            waiting_queue.popleft()</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. 本次调度的tokens和seq数是否满足：num_new_tokens_uncached, num_new_seqs最大数量的限制</span></span><br><span class="line">    <span class="keyword">if</span> (budget.num_batched_tokens</span><br><span class="line">            &gt;= <span class="variable language_">self</span>.scheduler_config.max_num_batched_tokens):</span><br><span class="line">        <span class="comment"># We&#x27;ve reached the budget limit - since there might be</span></span><br><span class="line">        <span class="comment"># continuous prefills in the running queue, we should break</span></span><br><span class="line">        <span class="comment"># to avoid scheduling any new prefills.</span></span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    num_new_seqs = seq_group.get_max_num_running_seqs()</span><br><span class="line">    <span class="keyword">if</span> num_new_tokens_uncached == <span class="number">0</span> <span class="keyword">or</span> <span class="keyword">not</span> budget.can_schedule(</span><br><span class="line">            num_new_tokens=num_new_tokens_uncached,</span><br><span class="line">            num_new_seqs=num_new_seqs,</span><br><span class="line">    ):</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4. 满足3中条件：开始调度</span></span><br><span class="line">    <span class="keyword">if</span> curr_loras <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> lora_int_id &gt; <span class="number">0</span>:</span><br><span class="line">        curr_loras.add(lora_int_id)</span><br><span class="line">    <span class="comment"># 4.1 从waiting_queue中移除队首元素</span></span><br><span class="line">    waiting_queue.popleft()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4.2 block_manager为该seq_group分配物理块，将每个seq的状态标为RUNNING</span></span><br><span class="line">    <span class="variable language_">self</span>._allocate_and_set_running(seq_group)</span><br><span class="line"></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;省略：</span></span><br><span class="line"><span class="string">    1. 若enable_chunking和调度器配置的is_multi_step为True，执行以下操作：</span></span><br><span class="line"><span class="string">        初始化一个空的 blocks_to_copy 列表。</span></span><br><span class="line"><span class="string">        调用 self._append_slots(seq_group, blocks_to_copy, enable_chunking) 来处理多步骤分配；</span></span><br><span class="line"><span class="string">        assert not blocks_to_copy 断言检查，确保 blocks_to_copy 在执行完后为空（避免副本写操作）。如果发生副本写操作，可能会引发此断言。</span></span><br><span class="line"><span class="string">    2. </span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 4.3 将seq_group包装为ScheduledSequenceGroup，添加到调度的序列组中</span></span><br><span class="line">    seq_groups.append(</span><br><span class="line">        ScheduledSequenceGroup(seq_group=seq_group,</span><br><span class="line">                                token_chunk_size=num_new_tokens))</span><br><span class="line">    <span class="comment"># 4.4 更新budget中的token使用情况和序列数</span></span><br><span class="line">    budget.add_num_batched_tokens(</span><br><span class="line">        seq_group.request_id,</span><br><span class="line">        num_batched_tokens=num_new_tokens_uncached,</span><br><span class="line">        num_cached_tokens=num_new_tokens_cached,</span><br><span class="line">    )</span><br><span class="line">    budget.add_num_seqs(seq_group.request_id, num_new_seqs)</span><br></pre></td></tr></table></figure></li></ul></li><li><p>将<code>leftover_waiting_sequences</code>重新加入<code>waiting_queue</code>队首，等待下一次调度：</p></li><li><p>返回<code>SchedulerPrefillOutputs</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> SchedulerPrefillOutputs(</span><br><span class="line">    seq_groups=seq_groups,</span><br><span class="line">    ignored_seq_groups=ignored_seq_groups,</span><br><span class="line">    num_lookahead_slots=<span class="variable language_">self</span>._get_num_lookahead_slots(</span><br><span class="line">        is_prefill=<span class="literal">True</span>, enable_chunking=enable_chunking),</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p></p></li></ol><h5 id="passed_delay判断调度waiting队列的时间点"><code>_passed_delay</code>：判断调度waiting队列的时间点</h5><p>模型在推理时，waiting队列中源源不断地有新的seq_group加入。<strong>一旦选择调度waiting队列，就会停下对running/swapped中seq_group的decode处理，转而去做waiting中seq_group的prefill</strong>（prefill和decode同一时期只有一个在处理中）；也即vLLM必须在新来的seq_group和已经在做推理的seq_group之前达成平衡。“waiting队列调度间隔阈值”就是来控制这种均衡的：</p><ul><li><p><strong>调度间隔设置得太小</strong>：每次调度都只关心waiting中的新请求，这样发送旧请求的用户迟迟得不到反馈结果；此时waiting队列中积累的新请求数量可能比较少，不利于batching，浪费了并发处理的能力。</p></li><li><p><strong>调度间隔设置得太大</strong>：waiting中的请求持续挤压，对vLLM推理的整体吞吐有影响。</p></li></ul><blockquote><p><code>self.prev_prompt</code>：记录上一次调度中，是否从选择了waiting队列中调度seq； * <code>Scheduler</code>初始化时设置为<code>False</code>；若wating队列中有可调度的<code>seq_group</code>（<code>_schedule_prefills</code>中<code>len(seq_groups) &gt; 0</code>），设置为<code>True</code>。 <code>self.prev_time</code>：上一次调度的时间点</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_passed_delay</span>(<span class="params">self, now: <span class="built_in">float</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">    <span class="comment"># 1. 若上一次从waiting队列中调度：计算两次调度的时间间隔</span></span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.prev_prompt:</span><br><span class="line">        <span class="variable language_">self</span>.last_prompt_latency = now - <span class="variable language_">self</span>.prev_time</span><br><span class="line">    <span class="variable language_">self</span>.prev_time, <span class="variable language_">self</span>.prev_prompt = now, <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. 延迟调度，使得waiting队列尽量填满（delay_factor自定义）</span></span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.scheduler_config.delay_factor &gt; <span class="number">0</span> <span class="keyword">and</span> <span class="variable language_">self</span>.waiting:</span><br><span class="line">        <span class="comment"># 2.1 计算waiting队列中，seq_group的最早到达时间</span></span><br><span class="line">        <span class="comment"># now - earliest_arrival_time: seq_group实际等待的时间</span></span><br><span class="line">        <span class="comment"># self.scheduler_config.delay_factor * self.last_prompt_latency：seq_group应该等待的时间</span></span><br><span class="line">        earliest_arrival_time = <span class="built_in">min</span>(</span><br><span class="line">            [e.metrics.arrival_time <span class="keyword">for</span> e <span class="keyword">in</span> <span class="variable language_">self</span>.waiting])</span><br><span class="line">        passed_delay = ((now - earliest_arrival_time)</span><br><span class="line">                        &gt; (<span class="variable language_">self</span>.scheduler_config.delay_factor *</span><br><span class="line">                            <span class="variable language_">self</span>.last_prompt_latency) <span class="keyword">or</span> <span class="keyword">not</span> <span class="variable language_">self</span>.running)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        passed_delay = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">return</span> passed_delay</span><br></pre></td></tr></table></figure><p></p></blockquote><h5 id="can_allocateblock_manager判断能否为seq_group分配物理块做prefill"><code>can_allocate</code>：block_manager判断能否为<code>seq_group</code>分配物理块做prefill</h5><p>当前我们已从waiting队列中取出了一个<code>seq_group</code>，将对它进行prefill操作。因此需要判断：gpu上是否有充足的物理块分配给该seq_group做prefill呢？</p><ul><li>这里假设：seq_group中的所有sequences共用同一个prompt（在preempted sequences中不一定成立）<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">can_allocate</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                    seq_group: SequenceGroup,</span></span><br><span class="line"><span class="params">                    num_lookahead_slots: <span class="built_in">int</span> = <span class="number">0</span></span>) -&gt; AllocStatus:</span><br><span class="line"></span><br><span class="line">    check_no_caching_or_swa_for_blockmgr_encdec(<span class="variable language_">self</span>, seq_group)</span><br><span class="line"></span><br><span class="line">     <span class="comment"># 1. 取出这个seq_group下处于waiting状态的序列</span></span><br><span class="line">    seq = seq_group.get_seqs(status=SequenceStatus.WAITING)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 2. 计算seq所需的逻辑块数量</span></span><br><span class="line">    num_required_blocks = BlockTable.get_num_required_blocks(</span><br><span class="line">        seq.get_token_ids(),</span><br><span class="line">        block_size=<span class="variable language_">self</span>.block_size,</span><br><span class="line">        num_lookahead_slots=num_lookahead_slots,</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># 3. 如果是encoder-decoder模型（通常是Transformer架构），计算编码器的逻辑块数量：</span></span><br><span class="line">    <span class="keyword">if</span> seq_group.is_encoder_decoder():</span><br><span class="line">        encoder_seq = seq_group.get_encoder_seq()</span><br><span class="line">        <span class="keyword">assert</span> encoder_seq <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">        num_required_blocks += BlockTable.get_num_required_blocks(</span><br><span class="line">            encoder_seq.get_token_ids(),</span><br><span class="line">            block_size=<span class="variable language_">self</span>.block_size,</span><br><span class="line">        )</span><br><span class="line">    <span class="comment"># 4. 考虑最大块滑动窗口：确保逻辑块数量不会请求超过最大窗口的块数</span></span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.max_block_sliding_window <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        num_required_blocks = <span class="built_in">min</span>(num_required_blocks,</span><br><span class="line">                                    <span class="variable language_">self</span>.max_block_sliding_window)</span><br><span class="line">    <span class="comment"># 5. 获取当前GPU上可用的空闲块数量</span></span><br><span class="line">    num_free_gpu_blocks = <span class="variable language_">self</span>.block_allocator.get_num_free_blocks(</span><br><span class="line">        device=Device.GPU)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 6. 检查是否有足够的块可以分配：</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="variable language_">self</span>.num_total_gpu_blocks - num_required_blocks</span><br><span class="line">            &lt; <span class="variable language_">self</span>.watermark_blocks):</span><br><span class="line">        <span class="keyword">return</span> AllocStatus.NEVER    <span class="comment"># 不分配</span></span><br><span class="line">    <span class="keyword">if</span> num_free_gpu_blocks - num_required_blocks &gt;= <span class="variable language_">self</span>.watermark_blocks:</span><br><span class="line">        <span class="keyword">return</span> AllocStatus.OK       <span class="comment"># 立即分配</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> AllocStatus.LATER    <span class="comment"># 稍后分配</span></span><br></pre></td></tr></table></figure></li></ul><blockquote><ul><li><code>watermark_blocks</code>：水位线block数量，起到缓冲作用，防止在1次调度中把gpu上预留给KV Cache的显存空间基本占满，出现一些意外风险（因为预留的显存空间也是估计值）。</li><li><code>NEVER</code>和<code>LATER</code><ul><li>相同点：都是因为当前显存空间不够，而无法继续调度seq_group；</li><li>不同点：<code>NEVER</code>是因为<strong>seq过长（即prompt太长）</strong>，以至于gpu上所有的block（num_total_gpu_blocks）都无法完成处理，因此后续步骤中直接将该seq标记为完成，不再处理；<code>LATER</code>是因为<strong>之前调度的seq_group占据相当一部分显存空间</strong>，导致gpu上剩余的可用block（num_free_gpu_blocks）不够，因此延迟处理。</li></ul></li></ul></blockquote><h4 id="schedule_running"><code>_schedule_running</code>：</h4><p>running队列包含：decode和chunked prefill请求</p><p>初始化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">decode_seq_groups: <span class="type">List</span>[ScheduledSequenceGroup] = ret.decode_seq_groups</span><br><span class="line">prefill_seq_groups: <span class="type">List</span>[</span><br><span class="line">    ScheduledSequenceGroup] = ret.prefill_seq_groups</span><br><span class="line">preempted: <span class="type">List</span>[SequenceGroup] = ret.preempted <span class="comment"># 存放被抢占的seq_group（recomputation模式）</span></span><br><span class="line">swapped_out: <span class="type">List</span>[SequenceGroup] = ret.swapped_out <span class="comment"># 存放被抢占的seq_group（swap模式）</span></span><br><span class="line"></span><br><span class="line">running_queue = <span class="variable language_">self</span>.running</span><br></pre></td></tr></table></figure><p></p><ol type="1"><li>running队列循环：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> running_queue:</span><br><span class="line">    <span class="comment"># 1. 取出当前running队列中，最早到达的seq_group</span></span><br><span class="line">    seq_group = running_queue[<span class="number">0</span>]</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; 可以丢弃缓存tokens的信息：</span></span><br><span class="line"><span class="string">        1. 如果seq采用chunked prefill，cached tokens info在第一次prefill已使用；</span></span><br><span class="line"><span class="string">        2. 如果seq采用non-chunked prefill，有解码序列，与cached tokens info无关。</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    num_uncached_new_tokens, _ = \</span><br><span class="line">        <span class="variable language_">self</span>._get_num_new_uncached_and_cached_tokens(</span><br><span class="line">        seq_group,</span><br><span class="line">        SequenceStatus.RUNNING,</span><br><span class="line">        enable_chunking,</span><br><span class="line">        budget,</span><br><span class="line">        partial_prefill_metadata,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    num_running_tokens = num_uncached_new_tokens</span><br><span class="line">    <span class="keyword">if</span> num_running_tokens == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># No budget =&gt; Stop</span></span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    running_queue.popleft()</span><br><span class="line"></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;如果启用异步输出处理 (use_async_output_proc)，在序列长度超过最大模型长度时：暂停当前序列并加入 _async_stopped 列表，以避免内存溢出。&#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="variable language_">self</span>.use_async_output_proc <span class="keyword">and</span> seq_group.seqs[<span class="number">0</span>].get_len()</span><br><span class="line">            &gt; <span class="variable language_">self</span>.scheduler_config.max_model_len):</span><br><span class="line">        <span class="variable language_">self</span>._async_stopped.append(seq_group)</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. block_manager循环判断：是否有足够的KV cache空间分配给该seq_group做decode</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> <span class="variable language_">self</span>._can_append_slots(seq_group, enable_chunking):</span><br><span class="line">        <span class="comment"># 没有充足空闲物理块：执行抢占</span></span><br><span class="line">        budget.subtract_num_batched_tokens(seq_group.request_id,</span><br><span class="line">                                            num_running_tokens)</span><br><span class="line">        num_running_seqs = seq_group.get_max_num_running_seqs()</span><br><span class="line">        budget.subtract_num_seqs(seq_group.request_id,</span><br><span class="line">                                    num_running_seqs)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (curr_loras <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> seq_group.lora_int_id &gt; <span class="number">0</span></span><br><span class="line">                <span class="keyword">and</span> seq_group.lora_int_id <span class="keyword">in</span> curr_loras):</span><br><span class="line">            curr_loras.remove(seq_group.lora_int_id)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2.1 决定被抢占的seq_group：抢占running队列最低优先级的seq_group（队首，FCFS）；若running队列为空，抢占当前seq_group（此时跳出循环，因为没有seq_group可供抢占）</span></span><br><span class="line">        cont_loop = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">if</span> running_queue:</span><br><span class="line">            victim_seq_group = running_queue.pop()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            victim_seq_group = seq_group</span><br><span class="line">            cont_loop = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;3. 省略：抢占前确定没有正在进行的异步后处理任务&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2.2 执行抢占：两种模式</span></span><br><span class="line">        <span class="comment"># swap模式：被抢占的seq_group进入swap队列</span></span><br><span class="line">        <span class="comment"># recomputation模式：被抢占的seq_group进入waiting队列</span></span><br><span class="line">        <span class="keyword">if</span> do_preempt:</span><br><span class="line">            preempted_mode = <span class="variable language_">self</span>._preempt(victim_seq_group,</span><br><span class="line">                                            blocks_to_swap_out)</span><br><span class="line">            <span class="keyword">if</span> preempted_mode == PreemptionMode.RECOMPUTE:</span><br><span class="line">                preempted.append(victim_seq_group)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                swapped_out.append(victim_seq_group)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> cont_loop:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 有充足空闲物理块：进行分配</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="variable language_">self</span>._append_slots(seq_group, blocks_to_copy, enable_chunking)</span><br><span class="line">        is_prefill = seq_group.is_prefill()</span><br><span class="line"></span><br><span class="line">        scheduled_seq_group: ScheduledSequenceGroup = (</span><br><span class="line">            <span class="variable language_">self</span>._scheduled_seq_group_cache[</span><br><span class="line">                <span class="variable language_">self</span>.cache_id].get_object())</span><br><span class="line">        scheduled_seq_group.seq_group = seq_group</span><br><span class="line">        <span class="keyword">if</span> is_prefill:</span><br><span class="line">            scheduled_seq_group.token_chunk_size = num_running_tokens</span><br><span class="line">            prefill_seq_groups.append(scheduled_seq_group)</span><br><span class="line">            ret.prefill_seq_groups_list.append(seq_group)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            scheduled_seq_group.token_chunk_size = <span class="number">1</span></span><br><span class="line">            decode_seq_groups.append(scheduled_seq_group)</span><br><span class="line">            ret.decode_seq_groups_list.append(seq_group)</span><br><span class="line"></span><br><span class="line">        budget.add_num_batched_tokens(seq_group.request_id,</span><br><span class="line">                                        num_running_tokens)</span><br><span class="line"></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;优化：get_max_num_running_seqs()是计算昂贵的，对于默认调度阶段，如果enable_chunking==num_seqs在调用该方法前已更新，因此这里不再更新&#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">if</span> enable_chunking:</span><br><span class="line">            num_running_seqs = seq_group.get_max_num_running_seqs()</span><br><span class="line">            budget.add_num_seqs(seq_group.request_id, num_running_seqs)</span><br><span class="line">        <span class="keyword">if</span> curr_loras <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> seq_group.lora_int_id &gt; <span class="number">0</span>:</span><br><span class="line">            curr_loras.add(seq_group.lora_int_id)</span><br></pre></td></tr></table></figure></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_schedule_running</span>(<span class="params"></span></span><br><span class="line"><span class="params">    self,</span></span><br><span class="line"><span class="params">    budget: SchedulingBudget,</span></span><br><span class="line"><span class="params">    curr_loras: <span class="type">Optional</span>[<span class="type">Set</span>[<span class="built_in">int</span>]],</span></span><br><span class="line"><span class="params">    enable_chunking: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    partial_prefill_metadata: <span class="type">Optional</span>[PartialPrefillMetadata] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params"></span>) -&gt; SchedulerRunningOutputs:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Schedule sequence groups that are running.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Running queue should include decode and chunked prefill requests.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        budget: The scheduling budget. The argument is in-place updated</span></span><br><span class="line"><span class="string">            when any decodes are preempted.</span></span><br><span class="line"><span class="string">        curr_loras: Currently batched lora request ids. The argument is</span></span><br><span class="line"><span class="string">            in-place updated when any decodes are preempted.</span></span><br><span class="line"><span class="string">        enable_chunking: If True, seq group can be chunked and only a</span></span><br><span class="line"><span class="string">            chunked number of tokens are scheduled  if</span></span><br><span class="line"><span class="string">            `budget.num_batched_tokens` has not enough capacity to schedule</span></span><br><span class="line"><span class="string">            all tokens.</span></span><br><span class="line"><span class="string">        partial_prefill_metadata: information about the partial prefills</span></span><br><span class="line"><span class="string">        that are currently running</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        SchedulerRunningOutputs.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    ret: SchedulerRunningOutputs = <span class="variable language_">self</span>._scheduler_running_outputs_cache[</span><br><span class="line">        <span class="variable language_">self</span>.cache_id].get_object()</span><br><span class="line">    ret.blocks_to_swap_out.clear()</span><br><span class="line">    ret.blocks_to_copy.clear()</span><br><span class="line">    ret.decode_seq_groups.clear()</span><br><span class="line">    ret.prefill_seq_groups.clear()</span><br><span class="line">    ret.preempted.clear()</span><br><span class="line">    ret.swapped_out.clear()</span><br><span class="line"></span><br><span class="line">    ret.num_lookahead_slots = <span class="variable language_">self</span>._get_num_lookahead_slots(</span><br><span class="line">        is_prefill=<span class="literal">False</span>, enable_chunking=enable_chunking)</span><br><span class="line"></span><br><span class="line">    ret.decode_seq_groups_list.clear()</span><br><span class="line">    ret.prefill_seq_groups_list.clear()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Blocks that need to be swapped or copied before model execution.</span></span><br><span class="line">    blocks_to_swap_out: <span class="type">List</span>[<span class="type">Tuple</span>[<span class="built_in">int</span>, <span class="built_in">int</span>]] = ret.blocks_to_swap_out</span><br><span class="line">    blocks_to_copy: <span class="type">List</span>[<span class="type">Tuple</span>[<span class="built_in">int</span>, <span class="built_in">int</span>]] = ret.blocks_to_copy</span><br><span class="line"></span><br><span class="line">    decode_seq_groups: <span class="type">List</span>[ScheduledSequenceGroup] = ret.decode_seq_groups</span><br><span class="line">    prefill_seq_groups: <span class="type">List</span>[</span><br><span class="line">        ScheduledSequenceGroup] = ret.prefill_seq_groups</span><br><span class="line">    preempted: <span class="type">List</span>[SequenceGroup] = ret.preempted</span><br><span class="line">    swapped_out: <span class="type">List</span>[SequenceGroup] = ret.swapped_out</span><br><span class="line"></span><br><span class="line">    running_queue = <span class="variable language_">self</span>.running</span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(<span class="variable language_">self</span>._async_stopped) == <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="variable language_">self</span>._scheduler_running_outputs_cache[<span class="variable language_">self</span>.next_cache_id].reset()</span><br><span class="line">    <span class="variable language_">self</span>._scheduled_seq_group_cache[<span class="variable language_">self</span>.next_cache_id].reset()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ret</span><br></pre></td></tr></table></figure><h5 id="can_append_slotsblock_manager判断是否能为seq_group分配充足物理块做decode"><code>_can_append_slots</code>：block_+manager判断是否能为seq_group分配充足物理块做decode</h5><p>做decode时，给每个seq分配1个token的位置；那么running队列中，seq_group下的n个seqs在上1个推理阶段共生成了n个token。本次调度中，先为这n个token分配物理空间，存放其在本次调度中即将产生的KV值。</p><p>当往1个seq的物理块上添加1个token时，可能有两种情况： * 之前的物理块已满，新分配一个物理块； * 之前的物理块没满，直接添加在最后一个物理块的空槽位上；</p><p><strong>因此对于n个seqs来说，最坏的情况就是添加n个物理块</strong>。</p><p><strong>考虑最坏情况：判断当前可用的物理块数量，是否至少为n</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">can_append_slots</span>(<span class="params">self, seq_group: SequenceGroup,</span></span><br><span class="line"><span class="params">                        num_lookahead_slots: <span class="built_in">int</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">    num_touched_blocks = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> seq <span class="keyword">in</span> seq_group.get_seqs(status=SequenceStatus.RUNNING):</span><br><span class="line">        block_table = <span class="variable language_">self</span>.block_tables[seq.seq_id]</span><br><span class="line"></span><br><span class="line">        num_touched_blocks += (</span><br><span class="line">            block_table.get_num_blocks_touched_by_append_slots(</span><br><span class="line">                token_ids=block_table.get_unseen_token_ids(</span><br><span class="line">                    seq.get_token_ids()),</span><br><span class="line">                num_lookahead_slots=num_lookahead_slots,</span><br><span class="line">            ))</span><br><span class="line"></span><br><span class="line">    num_free_gpu_blocks = <span class="variable language_">self</span>.block_allocator.get_num_free_blocks(</span><br><span class="line">        Device.GPU)</span><br><span class="line">    <span class="keyword">return</span> num_touched_blocks &lt;= num_free_gpu_blocks    <span class="comment"># 判断：空闲物理块是否至少为n</span></span><br></pre></td></tr></table></figure><p></p><h3 id="块管理器blockmanager">块管理器<code>BlockManager</code></h3><h3 id="step完成一次调度"><code>step()</code>：完成一次调度</h3><p><code>step()</code>方法：<strong>执行一次解码迭代</strong>，并返回新生成的结果。</p><ol type="1"><li>调度在下一次迭代中执行的序列，以及需要交换、复制或移入/移出的 token 块：</li></ol><ul><li>如果seq group中还有剩余的步骤，则不调用<code>Scheduler</code>，保证<code>Scheduler</code>只在当前batch完成后调用；</li><li>如果单个请求导致上一步引擎执行失败，那么<code>Scheduler</code>也会被跳过，之前的调度需要重新执行。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>._has_remaining_steps(</span><br><span class="line">        seq_group_metadata_list</span><br><span class="line">) <span class="keyword">and</span> <span class="keyword">not</span> <span class="variable language_">self</span>._skip_scheduling_next_step:</span><br><span class="line">    <span class="comment"># Schedule iteration</span></span><br><span class="line">    (seq_group_metadata_list, scheduler_outputs,</span><br><span class="line">        allow_async_output_proc</span><br><span class="line">        ) = <span class="variable language_">self</span>.scheduler[virtual_engine].schedule()</span><br><span class="line"></span><br><span class="line">    ctx.seq_group_metadata_list = seq_group_metadata_list</span><br><span class="line">    ctx.scheduler_outputs = scheduler_outputs</span><br><span class="line"></span><br><span class="line">    finished_requests_ids = <span class="variable language_">self</span>.scheduler[</span><br><span class="line">        virtual_engine].get_and_reset_finished_requests_ids()</span><br><span class="line">    <span class="comment"># When n&gt;1, elements in self.seq_id_to_seq_group should be deleted</span></span><br><span class="line">    <span class="comment"># here, otherwise memory leaks.</span></span><br><span class="line">    <span class="keyword">for</span> finished_request_id <span class="keyword">in</span> finished_requests_ids:</span><br><span class="line">        <span class="keyword">if</span> finished_request_id <span class="keyword">in</span> <span class="variable language_">self</span>.seq_id_to_seq_group:</span><br><span class="line">            <span class="keyword">del</span> <span class="variable language_">self</span>.seq_id_to_seq_group[finished_request_id]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Maybe switch from async mode to sync mode</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> allow_async_output_proc <span class="keyword">and</span> <span class="built_in">len</span>(ctx.output_queue) &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="variable language_">self</span>._process_model_outputs(ctx=ctx)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (<span class="variable language_">self</span>.scheduler_config.is_multi_step</span><br><span class="line">            <span class="keyword">and</span> scheduler_outputs.num_lookahead_slots &gt; <span class="number">0</span>):</span><br><span class="line">        <span class="comment"># cache the scheduler outputs for the next iteration if we have</span></span><br><span class="line">        <span class="comment"># lookahead slots</span></span><br><span class="line">        <span class="variable language_">self</span>._cache_scheduler_outputs_for_multi_step(</span><br><span class="line">            virtual_engine, seq_group_metadata_list, scheduler_outputs,</span><br><span class="line">            allow_async_output_proc)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    finished_requests_ids = <span class="built_in">list</span>()</span><br></pre></td></tr></table></figure><ol start="2" type="1"><li><p>调用分布式执行器，<code>execute_model</code>执行模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> scheduler_outputs.is_empty():</span><br><span class="line">    <span class="comment"># Check if we have a cached last_output from the previous iteration.</span></span><br><span class="line">    <span class="comment"># For supporting PP this is probably the best way to pass the</span></span><br><span class="line">    <span class="comment"># sampled_token_ids, as a separate broadcast over all the PP stages</span></span><br><span class="line">    <span class="comment"># will cause one virtual engine&#x27;s microbatch to block the pipeline.</span></span><br><span class="line">    last_sampled_token_ids = \</span><br><span class="line">        <span class="variable language_">self</span>._get_last_sampled_token_ids(virtual_engine)</span><br><span class="line"></span><br><span class="line">    execute_model_req = ExecuteModelRequest(</span><br><span class="line">        seq_group_metadata_list=seq_group_metadata_list,</span><br><span class="line">        blocks_to_swap_in=scheduler_outputs.blocks_to_swap_in,</span><br><span class="line">        blocks_to_swap_out=scheduler_outputs.blocks_to_swap_out,</span><br><span class="line">        blocks_to_copy=scheduler_outputs.blocks_to_copy,</span><br><span class="line">        num_lookahead_slots=scheduler_outputs.num_lookahead_slots,</span><br><span class="line">        running_queue_size=scheduler_outputs.running_queue_size,</span><br><span class="line">        finished_requests_ids=finished_requests_ids,</span><br><span class="line">        <span class="comment"># We use ExecuteModelRequest to pass the last sampled_token_ids</span></span><br><span class="line">        <span class="comment"># to each of the non-last PP stages for in-place prepare_input.</span></span><br><span class="line">        last_sampled_token_ids=last_sampled_token_ids)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> allow_async_output_proc:</span><br><span class="line">        execute_model_req.async_callback = <span class="variable language_">self</span>.async_callbacks[</span><br><span class="line">            virtual_engine]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        outputs = <span class="variable language_">self</span>.model_executor.execute_model(</span><br><span class="line">            execute_model_req=execute_model_req)</span><br><span class="line">        <span class="variable language_">self</span>._skip_scheduling_next_step = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">except</span> InputProcessingError <span class="keyword">as</span> e:</span><br><span class="line">        <span class="comment"># The input for this request cannot be processed, so we must</span></span><br><span class="line">        <span class="comment"># abort it. If there are remaining requests in the batch that</span></span><br><span class="line">        <span class="comment"># have been scheduled, they will be retried on the next step.</span></span><br><span class="line">        invalid_request_id = e.request_id</span><br><span class="line">        <span class="variable language_">self</span>._abort_and_cache_schedule(</span><br><span class="line">            request_id=invalid_request_id,</span><br><span class="line">            virtual_engine=virtual_engine,</span><br><span class="line">            seq_group_metadata_list=seq_group_metadata_list,</span><br><span class="line">            scheduler_outputs=scheduler_outputs,</span><br><span class="line">            allow_async_output_proc=allow_async_output_proc)</span><br><span class="line">        <span class="comment"># Raise so the caller is notified that this request failed</span></span><br><span class="line">        <span class="keyword">raise</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># We need to do this here so that last step&#x27;s sampled_token_ids can</span></span><br><span class="line">    <span class="comment"># be passed to the next iteration for PP.</span></span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.scheduler_config.is_multi_step:</span><br><span class="line">        <span class="variable language_">self</span>._update_cached_scheduler_output(virtual_engine, outputs)</span><br></pre></td></tr></table></figure><p></p></li><li><p>处理输出</p></li></ol><h2 id="致谢">致谢</h2><p>部分图转自：</p><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/706685260">vllm模型执行笔记: LLMEngine, Executor, Worker, ModelRunner</a></p><p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/UCdqQUM_9a36uXkO36wpSg">图解大模型计算加速系列：vLLM源码解析2，调度器策略(Scheduler)</a></p></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://wenliuyi.github.io">Liuyi Wen</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://wenliuyi.github.io/posts/f79d4b0.html">http://wenliuyi.github.io/posts/f79d4b0.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://wenliuyi.github.io" target="_blank">Liuyi Wen's Blog</a>！</span></div></div><div class="tag_share"><div class="post-share"><div class="social-share" data-image="/img/WechatIMG105.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/posts/abf55f5c.html" title="《算法导论》笔记"><div class="cover" style="background:var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">《算法导论》笔记</div></div><div class="info-2"><div class="info-item-1">该篇为《算法导论》学习笔记，包含部分章节的理论阐释、典型问题、和代码实现。 算法 时间复杂度 1.渐近符号： θ--渐近紧确界； f(n)=θ(g(n)):g(n)是f(n)的渐进紧确界. 定义:存在c1,c2,n0,对任意n&gt;=n0,有：0&lt;=c1g(n)&lt;=f(n)&lt;=c2g(n). f(n)=θ(g(n)),当且仅当:f(n)=O(g(n))且f(n)=欧姆. O--渐近上界；[欧姆]--渐近下界 o--非紧确渐近上界；ω--非紧确渐近下界 f(n)=O(g(n))中,0&lt;=f(n)&lt;cg(n)对某个常量c&gt;0成立； f(n)=o(g(n))中,0&lt;=f(n)&lt;cg(n)对所有常量c&gt;0成立. 2.主定理求时间复杂度: T(n)=aT(n/b)+f(n). 比较n1和f(n):选择多项式意义上更大的。 1.f(n)=O(n(logb(a)-ε)),则:T(n)=θ(n(logb(a)). 2.f(n)=0(n(logb(a))),则:T(n)=θ(n(logb(a)lgn)....</div></div></div></a><a class="pagination-related" href="/posts/27b7154a.html" title="基于图像处理的智能纤维截面分析系统：系统展示"><div class="cover" style="background:var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">基于图像处理的智能纤维截面分析系统：系统展示</div></div><div class="info-2"><div class="info-item-1">系统演示 (function(){var player = new DPlayer({"container":document.getElementById("dplayer4"),"theme":"#FADFA3","loop":true,"video":{"url":"/posts/27b7154a/1.mp4","pic":"1.png"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})()</div></div></div></a></nav><hr class="custom-hr"><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/img/WechatIMG105.jpg" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info-name">Liuyi Wen</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">41</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">5</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">8</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/WenLiuyi"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">The Journey Is the Reward.</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BBefficient-memory-management-for-large-language-model-serving-with-pagedattention"><span class="toc-number">1.</span> <span class="toc-text">论文解读：Efficient Memory Management for Large Language Model Serving with PagedAttention</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#abstract"><span class="toc-number">1.1.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#introduction"><span class="toc-number">1.2.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#backgrounds"><span class="toc-number">1.3.</span> <span class="toc-text">BackGrounds</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8Etransformer%E7%9A%84llm"><span class="toc-number">1.3.1.</span> <span class="toc-text">基于Transformer的LLM</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#llm%E6%9C%8D%E5%8A%A1%E8%87%AA%E5%9B%9E%E5%BD%92%E7%94%9F%E6%88%90"><span class="toc-number">1.3.2.</span> <span class="toc-text">LLM服务&amp;自回归生成</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#llm%E6%89%B9%E5%A4%84%E7%90%86"><span class="toc-number">1.3.3.</span> <span class="toc-text">LLM批处理</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#methods"><span class="toc-number">1.4.</span> <span class="toc-text">Methods</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#pagedattention"><span class="toc-number">1.4.1.</span> <span class="toc-text">PagedAttention</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#kv-cache-manager"><span class="toc-number">1.4.2.</span> <span class="toc-text">KV Cache Manager</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8pagedattention%E5%92%8Cvllm%E8%A7%A3%E7%A0%81"><span class="toc-number">1.4.3.</span> <span class="toc-text">使用PagedAttention和vLLM解码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#vllm%E5%9C%A8%E5%85%B6%E4%BB%96%E8%A7%A3%E7%A0%81%E5%9C%BA%E6%99%AF%E7%9A%84%E5%BA%94%E7%94%A8"><span class="toc-number">1.4.4.</span> <span class="toc-text">vLLM在其他解码场景的应用</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%B9%B6%E8%A1%8C%E9%87%87%E6%A0%B7parallel-sampling"><span class="toc-number">1.4.4.1.</span> <span class="toc-text">并行采样（Parallel Sampling）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%9D%9F%E6%90%9C%E7%B4%A2beam-search"><span class="toc-number">1.4.4.2.</span> <span class="toc-text">束搜索（Beam Search）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%85%B1%E4%BA%AB%E5%89%8D%E7%BC%80"><span class="toc-number">1.4.4.3.</span> <span class="toc-text">共享前缀</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B0%83%E5%BA%A6%E4%B8%8E%E6%8A%A2%E5%8D%A0%E6%9C%BA%E5%88%B6"><span class="toc-number">1.4.5.</span> <span class="toc-text">调度与抢占机制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E6%89%A7%E8%A1%8C"><span class="toc-number">1.4.6.</span> <span class="toc-text">分布式执行</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B"><span class="toc-number">1.4.6.1.</span> <span class="toc-text">工作流程</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#implementation"><span class="toc-number">1.5.</span> <span class="toc-text">Implementation</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%86%85%E6%A0%B8%E7%BA%A7%E4%BC%98%E5%8C%96"><span class="toc-number">1.5.1.</span> <span class="toc-text">内核级优化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%A3%E7%A0%81%E7%AE%97%E6%B3%95%E6%94%AF%E6%8C%81%E6%A1%86%E6%9E%B6"><span class="toc-number">1.5.2.</span> <span class="toc-text">解码算法支持框架</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%BA%90%E7%A0%81"><span class="toc-number">2.</span> <span class="toc-text">源码</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B0%83%E7%94%A8%E6%96%B9%E5%BC%8F"><span class="toc-number">2.1.</span> <span class="toc-text">调用方式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A6%BB%E7%BA%BF%E6%89%B9%E6%8E%A8%E7%90%86offline-batched-inference"><span class="toc-number">2.1.1.</span> <span class="toc-text">离线批推理（Offline Batched Inference）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9C%A8%E7%BA%BFapi%E6%9C%8D%E5%8A%A1api-server-for-online-serving"><span class="toc-number">2.1.2.</span> <span class="toc-text">在线API服务（API Server For Online Serving）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%8Ellm%E5%BC%80%E5%A7%8B"><span class="toc-number">2.2.</span> <span class="toc-text">从LLM开始</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%89%A7%E8%A1%8C%E5%99%A8executor"><span class="toc-number">2.3.</span> <span class="toc-text">模型执行器Executor</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%89%A7%E8%A1%8C%E5%99%A8%E5%88%9D%E5%A7%8B%E5%8C%96_init_executor%E5%87%BD%E6%95%B0"><span class="toc-number">2.3.1.</span> <span class="toc-text">执行器初始化：_init_executor函数</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#uniprocexecutor%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">2.3.1.1.</span> <span class="toc-text">UniProcExecutor的初始化</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#executorwithexternallauncher%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">2.3.1.2.</span> <span class="toc-text">ExecutorWithExternalLauncher的初始化</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#raydistributedexecutor%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">2.3.1.3.</span> <span class="toc-text">RayDistributedExecutor初始化</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B7%A5%E4%BD%9C%E8%BF%9B%E7%A8%8Bworker"><span class="toc-number">2.4.</span> <span class="toc-text">工作进程Worker</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8Ellmengine"><span class="toc-number">2.5.</span> <span class="toc-text">推理引擎LLMEngine</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#llmengine%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">2.5.1.</span> <span class="toc-text">LLMEngine初始化</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96kv-cache_initialize_kv_caches"><span class="toc-number">2.5.1.1.</span> <span class="toc-text">初始化KV Cache：_initialize_kv_caches</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#worker%E5%89%8D%E5%90%91%E6%8E%A8%E7%90%86determine_num_available_blocks"><span class="toc-number">2.5.1.1.1.</span> <span class="toc-text">Worker前向推理：determine_num_available_blocks</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#worker%E5%88%9D%E5%A7%8B%E5%8C%96-kv-cacheinitialize_cache"><span class="toc-number">2.5.1.1.2.</span> <span class="toc-text">Worker初始化 KV Cache：initialize_cache</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%A8%E7%90%86"><span class="toc-number">2.6.</span> <span class="toc-text">推理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BA%8F%E5%88%97%E7%BB%84sequencegroup"><span class="toc-number">2.6.1.</span> <span class="toc-text">序列组SequenceGroup</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%8E%9F%E7%94%9F%E8%BE%93%E5%85%A5"><span class="toc-number">2.6.1.1.</span> <span class="toc-text">原生输入</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#sequencegroup%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="toc-number">2.6.1.2.</span> <span class="toc-text">SequenceGroup的作用</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#sequencegroup%E7%BB%93%E6%9E%84"><span class="toc-number">2.6.1.3.</span> <span class="toc-text">SequenceGroup结构</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%A5%E5%8F%A3%E5%87%BD%E6%95%B0generate"><span class="toc-number">2.6.2.</span> <span class="toc-text">入口函数：generate</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#add_request%E6%8E%A5%E6%94%B6%E7%94%A8%E6%88%B7%E8%AF%B7%E6%B1%82"><span class="toc-number">2.6.3.</span> <span class="toc-text">add_request：接收用户请求</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#add_processed_request%E8%AF%B7%E6%B1%82%E6%B7%BB%E5%8A%A0%E8%87%B3%E8%AF%B7%E6%B1%82%E6%B1%A0"><span class="toc-number">2.6.4.</span> <span class="toc-text">_add_processed_request：请求添加至请求池</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B0%83%E5%BA%A6%E5%99%A8scheduler"><span class="toc-number">2.7.</span> <span class="toc-text">调度器Scheduler</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B0%83%E5%BA%A6%E5%99%A8%E7%BB%93%E6%9E%84"><span class="toc-number">2.7.1.</span> <span class="toc-text">调度器结构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B4%E4%BD%93%E8%B0%83%E5%BA%A6%E6%B5%81%E7%A8%8B"><span class="toc-number">2.7.2.</span> <span class="toc-text">整体调度流程</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#passed_delay%E5%88%A4%E6%96%AD%E8%B0%83%E5%BA%A6waiting%E9%98%9F%E5%88%97%E7%9A%84%E6%97%B6%E9%97%B4%E7%82%B9"><span class="toc-number">2.7.2.1.</span> <span class="toc-text">_passed_delay：判断调度waiting队列的时间点</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#can_allocateblock_manager%E5%88%A4%E6%96%AD%E8%83%BD%E5%90%A6%E4%B8%BAseq_group%E5%88%86%E9%85%8D%E7%89%A9%E7%90%86%E5%9D%97%E5%81%9Aprefill"><span class="toc-number">2.7.2.2.</span> <span class="toc-text">can_allocate：block_manager判断能否为seq_group分配物理块做prefill</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#schedule_running"><span class="toc-number">2.7.3.</span> <span class="toc-text">_schedule_running：</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#can_append_slotsblock_manager%E5%88%A4%E6%96%AD%E6%98%AF%E5%90%A6%E8%83%BD%E4%B8%BAseq_group%E5%88%86%E9%85%8D%E5%85%85%E8%B6%B3%E7%89%A9%E7%90%86%E5%9D%97%E5%81%9Adecode"><span class="toc-number">2.7.3.1.</span> <span class="toc-text">_can_append_slots：block_+manager判断是否能为seq_group分配充足物理块做decode</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9D%97%E7%AE%A1%E7%90%86%E5%99%A8blockmanager"><span class="toc-number">2.8.</span> <span class="toc-text">块管理器BlockManager</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#step%E5%AE%8C%E6%88%90%E4%B8%80%E6%AC%A1%E8%B0%83%E5%BA%A6"><span class="toc-number">2.9.</span> <span class="toc-text">step()：完成一次调度</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%B4%E8%B0%A2"><span class="toc-number">3.</span> <span class="toc-text">致谢</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/7824989e.html" title="从 TCP 粘包到分帧">从 TCP 粘包到分帧</a><time datetime="2025-09-26T10:04:55.000Z" title="发表于 2025-09-26 18:04:55">2025-09-26</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/eecc19a2.html" title="并行训练系列：5. Megatron 之分布式环境初始化">并行训练系列：5. Megatron 之分布式环境初始化</a><time datetime="2025-09-25T09:41:56.000Z" title="发表于 2025-09-25 17:41:56">2025-09-25</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/d7906c2a.html" title="并行训练系列：4. 张量并行（TP）">并行训练系列：4. 张量并行（TP）</a><time datetime="2025-09-18T15:53:50.000Z" title="发表于 2025-09-18 23:53:50">2025-09-18</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/7529a079.html" title="并行训练系列：3. 数据并行下篇（DeepSeed-ZeRO）">并行训练系列：3. 数据并行下篇（DeepSeed-ZeRO）</a><time datetime="2025-09-17T10:00:42.000Z" title="发表于 2025-09-17 18:00:42">2025-09-17</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/3858f068.html" title="并行训练系列：2. 数据并行上篇（DP，DDP）">并行训练系列：2. 数据并行上篇（DP，DDP）</a><time datetime="2025-09-16T07:43:28.000Z" title="发表于 2025-09-16 15:43:28">2025-09-16</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By Liuyi Wen</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(()=>{const t=()=>{if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"all"},chtml:{scale:1.1},options:{enableMenu:!0,renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const n=!!e.type.match(/; *mode=display/),a=new t.options.MathItem(e.textContent,t.inputJax[0],n),d=document.createTextNode("");e.parentNode.replaceChild(d,e),a.start={node:d,delim:"",n:0},a.end={node:d,delim:"",n:0},t.math.push(a)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}};btf.addGlobalFn("encrypt",t,"mathjax"),window.pjax?t():window.addEventListener("load",t)})()</script><script>(()=>{const n="shuoshuo"===GLOBAL_CONFIG_SITE.pageType,e=(e,o)=>{n&&(window.shuoshuoComment.destroyValine=()=>{e.children.length&&(e.innerHTML="",e.classList.add("no-comment"))});const t={el:"#vcomment",appId:"bsxtUJWr1muoPS1pmoXLOPZ2-gzGzoHsz",appKey:"wm2wUYvKLEySwyRnFn7xAbJI",avatar:"monsterid",serverURLs:"",emojiMaps:"",visitor:!1,path:n?o:window.location.pathname};new Valine(t)},o=async(n,o)=>{"function"==typeof Valine||await btf.getScript("https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"),e(n,o)};n?window.shuoshuoComment={loadComment:o}:btf.loadComment(document.getElementById("vcomment"),o)})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-info"><div class="algolia-stats"></div><div class="algolia-poweredBy"></div></div></div></div></div><div id="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/algoliasearch/dist/lite/builds/browser.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instantsearch.js/dist/instantsearch.production.min.js"></script><script src="/js/search/algolia.js"></script></div></div>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body></html>