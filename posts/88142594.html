<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>RL 系列：5. 从 TRPO 到 PPO 算法 | Liuyi Wen's Blog</title><meta name="author" content="Liuyi Wen"><meta name="copyright" content="Liuyi Wen"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="上一篇记录了策略梯度算法的目标：通过梯度上升的方法执行策略更新，目标是最大化期望奖励，数学形式为： \[ \nabla\overline{R_\theta}&#x3D;\mathbb{E}_{\tau\sim p_\theta(\tau)}[R(\tau)\nabla\log p_\theta(\tau)] \] 由于策略梯度算法是 on-policy 的，因此要求行为策略\(\mu\)和目标策略\(\pi"><meta property="og:type" content="article"><meta property="og:title" content="RL 系列：5. 从 TRPO 到 PPO 算法"><meta property="og:url" content="http://wenliuyi.github.io/posts/88142594.html"><meta property="og:site_name" content="Liuyi Wen&#39;s Blog"><meta property="og:description" content="上一篇记录了策略梯度算法的目标：通过梯度上升的方法执行策略更新，目标是最大化期望奖励，数学形式为： \[ \nabla\overline{R_\theta}&#x3D;\mathbb{E}_{\tau\sim p_\theta(\tau)}[R(\tau)\nabla\log p_\theta(\tau)] \] 由于策略梯度算法是 on-policy 的，因此要求行为策略\(\mu\)和目标策略\(\pi"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://wenliuyi.github.io/img/WechatIMG105.jpg"><meta property="article:published_time" content="2025-12-08T07:52:43.000Z"><meta property="article:modified_time" content="2025-12-08T08:10:45.028Z"><meta property="article:author" content="Liuyi Wen"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="http://wenliuyi.github.io/img/WechatIMG105.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "RL 系列：5. 从 TRPO 到 PPO 算法",
  "url": "http://wenliuyi.github.io/posts/88142594.html",
  "image": "http://wenliuyi.github.io/img/WechatIMG105.jpg",
  "datePublished": "2025-12-08T07:52:43.000Z",
  "dateModified": "2025-12-08T08:10:45.028Z",
  "author": [
    {
      "@type": "Person",
      "name": "Liuyi Wen",
      "url": "http://wenliuyi.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://wenliuyi.github.io/posts/88142594.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>(()=>{const e={set:(e,t,o)=>{if(!o)return;const a=Date.now()+864e5*o;localStorage.setItem(e,JSON.stringify({value:t,expiry:a}))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const{value:o,expiry:a}=JSON.parse(t);if(!(Date.now()>a))return o;localStorage.removeItem(e)}};window.btf={saveToLocal:e,getScript:(e,t={})=>new Promise((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,Object.entries(t).forEach(([e,t])=>n.setAttribute(e,t)),n.onload=n.onreadystatechange=()=>{n.readyState&&!/loaded|complete/.test(n.readyState)||o()},n.onerror=a,document.head.appendChild(n)}),getCSS:(e,t)=>new Promise((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onload=n.onreadystatechange=()=>{n.readyState&&!/loaded|complete/.test(n.readyState)||o()},n.onerror=a,document.head.appendChild(n)}),addGlobalFn:(e,t,o=!1,a=window)=>{if(e.startsWith("pjax"))return;const n=a.globalFn||{};n[e]=n[e]||{},n[e][o||Object.keys(n[e]).length]=t,a.globalFn=n}};const t=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},o=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};btf.activateDarkMode=t,btf.activateLightMode=o;const a=e.get("theme");"dark"===a?t():"light"===a&&o();const n=e.get("aside-status");void 0!==n&&document.documentElement.classList.toggle("hide-aside","hide"===n);/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})()</script><script>const GLOBAL_CONFIG={root:"/",algolia:{appId:"VE36MEFVE6",apiKey:"f9b9ca5a3cdb9455658600dba6ae7706",indexName:"hexo-algolia indexing key",hitsPerPage:6,languages:{input_placeholder:"搜索文章",hits_empty:"未找到符合您查询的内容：${query}",hits_stats:"找到 ${hits} 条结果，耗时 ${time} 毫秒"}},localSearch:void 0,translate:void 0,highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:!1,highlightFullpage:!1,highlightMacStyle:!1},copy:{success:"复制成功",error:"复制失败",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"null",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!1,islazyloadPlugin:!1,isAnchor:!1,percent:{toc:!0,rightside:!1},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"RL 系列：5. 从 TRPO 到 PPO 算法",isHighlightShrink:!1,isToc:!0,pageType:"post"}</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Liuyi Wen's Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">RL 系列：5. 从 TRPO 到 PPO 算法</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div></div></nav><div id="post-info"><h1 class="post-title">RL 系列：5. 从 TRPO 到 PPO 算法</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-12-08T07:52:43.000Z" title="发表于 2025-12-08 15:52:43">2025-12-08</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-12-08T08:10:45.028Z" title="更新于 2025-12-08 16:10:45">2025-12-08</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/RL/">RL</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><p>上一篇记录了策略梯度算法的目标：通过<strong>梯度上升</strong>的方法执行策略更新，目标是最大化期望奖励，数学形式为：</p><p><span class="math display">\[ \nabla\overline{R_\theta}=\mathbb{E}_{\tau\sim p_\theta(\tau)}[R(\tau)\nabla\log p_\theta(\tau)] \]</span></p><p>由于策略梯度算法是 on-policy 的，因此要求行为策略<span class="math inline">\(\mu\)</span>和目标策略<span class="math inline">\(\pi_\theta\)</span>相同。每次将策略<span class="math inline">\(\theta\)</span>更新为<span class="math inline">\(\theta&#39;\)</span>时，来源于上一轮采样数据的概率<span class="math inline">\(p_\theta(\tau)\)</span>不能再使用，需要重新采样；这说明 <strong>on-policy 算法每次采样的数据在更新参数时只能使用一次</strong>。耗时巨大。能不能转为 off-policy 算法呢？</p><p>一个想法是使用冻结的策略<span class="math inline">\(\pi_{\theta&#39;}\)</span>专门与环境交互，使用<span class="math inline">\(\theta&#39;\)</span>采样的数据训练优化目标<span class="math inline">\(\pi_\theta\)</span>. 引入重要性采样（importance sampling）的概念。</p><h2 id="重要性采样">重要性采样</h2><p>“采样是概率密度函数的逆向应用。”对于具备分布<span class="math inline">\(p\)</span> 的随机变量<span class="math inline">\(x\)</span>，可以通过从<span class="math inline">\(p\)</span>中采样若干数据<span class="math inline">\(x^i\)</span>后代入<span class="math inline">\(f(x)\)</span>，用于近似<span class="math inline">\(\mathbb{E}[f(x)]\)</span>.</p><p>可不可以通过从分布<span class="math inline">\(q\)</span>中采样的数据<span class="math inline">\(x^i\)</span>，计算符合分布<span class="math inline">\(p\)</span>的<span class="math inline">\(\mathbb{E}[f(x)]\)</span>呢？（on-policy 转向 off-policy）有：</p><p><span class="math display">\[ \mathbb{E}_{x\sim p}[f(x)]=\int f(x)p(x)dx=\int f(x)\frac{p(x)}{q(x)}q(x)dx=\mathbb{E}_{x\sim q}[f(x)\frac{p(x)}{q(x)}] \]</span></p><p>上式通过 <strong>重要性权重<span class="math inline">\(\frac{p(x)}{q(x)}\)</span></strong> 修正两个分布的差异。</p><blockquote><p>虽然上式左右两端的期望值在数学上等价，但是方差随着<span class="math inline">\(p\)</span>和<span class="math inline">\(q\)</span>的差异而变化。有：</p><p><span class="math display">\[ Var_{x\sim p}[f(x)]=\mathbb{E}_{x\sim p}[f(x)^2]-(\mathbb{E}_{x\sim p}[f(x)])^2 \]</span></p><p><span class="math display">\[ \begin{align*} Var_{x\sim q}[f(x)\frac{p(x)}{q(x)}]&amp;=\mathbb{E}_{x\sim q}[(f(x)\frac{p(x)}{q(x)})^2]-(\mathbb{E}_{x\sim q}[f(x)\frac{p(x)}{q(x)}])^2 \\ &amp;=\mathbb{E}_{x\sim p}[f(x)^2\frac{p(x)}{q(x)}]-(\mathbb{E}_{x\sim p}[f(x)])^2 \end{align*} \]</span></p><p><span class="math inline">\(Var_{x\sim p}[f(x)]\)</span> 和 <span class="math inline">\(Var_{x\sim q}[f(x)\frac{p(x)}{q(x)}]\)</span>的差异在于第一项：如果<span class="math inline">\(\frac{p(x)}{q(x)}\)</span>的差距较大，则方差较大；当采样次数不足够多时，可能得到差距非常大的结果，此时该近似方法不准确。</p></blockquote><p>使用重要性采样将 on-policy 算法改为 off-policy.</p><p>假设有另外一个 Actor 采用策略<span class="math inline">\(\pi_{\theta&#39;}\)</span>做采样，有：</p><p><span class="math display">\[ \nabla\overline{R_\theta}=\mathbb{E}_{\tau\sim p_\theta&#39;(\tau)}[\frac{p_\theta(\tau)}{p_\theta&#39;(\tau)}R(\tau)\nabla\log p_\theta(\tau)] \]</span></p><p>重要性权重为<span class="math inline">\(\frac{p_\theta(\tau)}{p_\theta&#39;(\tau)}\)</span>. 采用<span class="math inline">\(\theta&#39;\)</span>与环境交互采样大量的数据，用于更新<span class="math inline">\(\theta\)</span>.</p><h2 id="优势函数">优势函数</h2><p><strong>价值函数<span class="math inline">\(V_{\pi_\theta}(s)\)</span></strong> 的含义为：在某个状态<span class="math inline">\(s\)</span>一直与环境交互直至游戏结束的期望奖励。有：</p><p><span class="math display">\[ R(\theta)=\mathbb{E}[V_{\pi_\theta}(s_0)]=\mathbb{E}_{\pi_\theta}[\sum_{t=0}^\infty\gamma^t r(s_t, a_t)] \]</span></p><p>在策略<span class="math inline">\(\pi_\theta\)</span>下的优化目标写成在<span class="math inline">\(\pi_{\theta&#39;}\)</span>下的形式（初始状态<span class="math inline">\(s_0\)</span>的分布与策略无关）：</p><p><span class="math display">\[ \begin{align*} R(\theta)&amp;=\mathbb{E}[V_{\pi_\theta}(s_0)] \\ &amp;=\mathbb{E}_{\pi_{\theta&#39;}}[\sum_{t=0}^\infty\gamma^t V_{\pi_\theta}(s_t)-\sum_{t=1}^\infty\gamma^t V_{\pi_\theta}(s_t)] \\ &amp;=-\mathbb{E}_{\pi_{\theta&#39;}}[\sum_{t=0}^\infty\gamma^t(\gamma V_{\pi_\theta}(s_{t+1})-V_{\pi_\theta}(s_t))] \end{align*} \]</span></p><p>策略<span class="math inline">\(\theta\)</span>和<span class="math inline">\(\theta&#39;\)</span>的目标函数差距为：</p><p><span class="math display">\[ \begin{align*} R(\theta&#39;)-R(\theta)&amp;=\mathbb{E}[V_{\pi_\theta&#39;}(s_0)]-\mathbb{E}[V_{\pi_\theta}(s_0)] \\ &amp;=\mathbb{E}_{\pi_\theta&#39;}[\sum_{t=0}^\infty\gamma^t r(s_t, a_t)]+\mathbb{E}_{\pi_{\theta&#39;}}[\sum_{t=0}^\infty\gamma^t(\gamma V_{\pi_\theta}(s_{t+1})-V_{\pi_\theta}(s_t))] \\ &amp;=\mathbb{E}_{\pi_\theta&#39;}[\sum_{t=0}^\infty\gamma^t[r(s_t, a_t)+\gamma V_{\pi_\theta}(s_{t+1})-V_{\pi_\theta}(s_t)]] \end{align*} \]</span></p><p><strong>当前状态-动作对的优势函数（Advantage）<span class="math inline">\(A^\theta(s_t, a_t)\)</span></strong> 定义为：</p><p><span class="math display">\[ A=r(s_t, a_t)+\gamma V_{\pi_\theta}(s_{t+1})-V_{\pi_\theta}(s_t) \]</span></p><p>这一项为时序差分残差，使用累积奖励减去 baseline，用于估测：在状态<span class="math inline">\(s_t\)</span>采取动作<span class="math inline">\(a_t\)</span>的优劣。</p><p><strong>梯度更新</strong>过程为：</p><p><span class="math display">\[ \mathbb{E}_{(s_t, a_t)\sim\pi_\theta}[A^\theta(s_t, a_t)\nabla\log p_\theta(a_t^n|s_t^n)] \]</span></p><p>如果<span class="math inline">\(A^\theta(s_t, a_t)\)</span>为正，增加概率；反之则降低概率。</p><p>使用重要性采样技术将 on-policy 转为 off-policy 时：</p><p><span class="math display">\[ \mathbb{E}_{(s_t, a_t)\sim\pi_\theta&#39;}[\frac{p_\theta(s_t, a_t)}{p_{\theta&#39;}(s_t, a_t)}A^\theta(s_t, a_t)\nabla\log p_\theta(a_t^n|s_t^n)] \]</span></p><p>这里<strong>采用<span class="math inline">\(A^{\theta&#39;}(s_t, a_t)\)</span>近似<span class="math inline">\(A^{\theta}(s_t, a_t)\)</span></strong>.</p><p>继续转化为：</p><p><span class="math display">\[ \mathbb{E}_{(s_t, a_t)\sim\pi_\theta&#39;}[\frac{p_\theta(a_t|s_t)}{p_{\theta&#39;}(a_t|s_t)}\frac{p_\theta(s_t)}{p_{\theta&#39;}(s_t)}A^{\theta&#39;}(s_t, a_t)\nabla\log p_\theta(a_t^n|s_t^n)] \]</span></p><p>这里采用<span class="math inline">\(p_{\theta&#39;}(s_t)\)</span>近似<span class="math inline">\(p_\theta(s_t)\)</span>.</p><p>从以上梯度更新的公式反推待优化的目标函数：</p><p><span class="math display">\[ R^{\theta&#39;}(\theta)=\mathbb{E}_{(s_t, a_t)\sim\pi_{\theta&#39;}}[\frac{p_\theta(a_t|s_t)}{p_{\theta&#39;}(a_t|s_t)}A^{\theta&#39;}(s_t, a_t)] \]</span></p><p>其含义为：使用<span class="math inline">\(\theta&#39;\)</span>与环境交互，采样得到<span class="math inline">\(s_t\)</span>, <span class="math inline">\(a_t\)</span>，计算优势<span class="math inline">\(A^{\theta}(s_t, a_t)\)</span>；最后用于更新<span class="math inline">\(\theta\)</span>.</p><p>也即：</p><p><span class="math display">\[ R^{\theta&#39;}(\theta)=\mathbb{E}_{s\sim\nu^{\pi_{\theta&#39;}}}\mathbb{E}_{a\sim\pi_{\theta&#39;}(·|s)}[\frac{\pi_{\theta}(a|s)}{\pi_{\theta&#39;}(a|s)}A^{\pi_{\theta&#39;}}(s, a)] \]</span></p><h3 id="广义优势估计gae">广义优势估计（GAE）</h3><p>优势函数衡量某个动作相对于其他动作的好坏程度，即：在某个状态下采取某个动作，是否比策略中其他动作表现地更好。通常定义为：</p><p><span class="math display">\[ A(s, a)=Q(s, a)-V(s) \]</span></p><ol type="1"><li><p>采用蒙特卡洛方法估计优势：采样完整轨迹，利用从状态<span class="math inline">\(s\)</span> 到结束的总回报<span class="math inline">\(R(s,a)\)</span>进行<strong>无偏估计</strong>，即：</p><p><span class="math display">\[ A(s, a)=R(s, a)-V(s) \]</span></p><p>方差大，单次采样容易受到偶然因素的影响。</p></li><li><p>采用时序差分方法估计优势：采用一步/多步预测估计，即：</p><p><span class="math display">\[ A(s, a)=r+\gamma V(s&#39;)-V(s) \]</span></p><p>其中<span class="math inline">\(V(s&#39;)\)</span>是下一个状态<span class="math inline">\(s&#39;\)</span>的价值。方差小（依赖多个小步骤的估计），偏差大（使用估计的价值函数更新）</p></li></ol><p>广义优势估计（GAE）同时结合蒙特卡洛和时序差分，引入一个混合系数<span class="math inline">\(\lambda\)</span>在方差和偏差之间灵活调节。GAE 的核心思想是：利用多个 TD 步骤的加权和估计优势。定义为：</p><p><span class="math display">\[ A_t^{GAE}=\sum_{l=0}^\infty(\gamma\lambda)^l\delta_{t+l} \]</span></p><p>其中，<span class="math inline">\(\delta_t=r_t+\gamma V(s_{t+1})-V(s_t)\)</span>是时序差分误差。当<span class="math inline">\(\lambda=1\)</span>时，退化为蒙特卡洛，方差较大但无偏；当<span class="math inline">\(\lambda=0\)</span>时，退化为时序差分，偏差大但方差小。</p><p>根据多步时序差分思想，有：</p><p><span class="math display">\[ \begin{align*} A_t^{(1)}=\delta_t &amp;=-V(s_t)+r_t+\gamma V(s_{t+1}) \\ A_t^{(2)}=\delta_t+\gamma\delta_{t+1} &amp;=-V(s_t)+r_t+\gamma r_{t+1}+\gamma^2 V(s_{t+2}) \\ A_t^{(2)}=\delta_t+\gamma\delta_{t+1}+\gamma^2\delta_{t+2} &amp;=-V(s_t)+r_t+\gamma r_{t+1}+\gamma^2r_{t+2}+ \gamma^3V(s_{t+3}) \\ A_t^{(k)}=\sum_{l=0}^{k-1}\gamma^l\delta_{t+l} &amp;=-V(s_t)+r_t+\gamma r_{t+1}+...+\gamma^{k-1}r_{t+k-1}+\gamma^k V(s_{t+k}) \end{align*} \]</span></p><p>将不同步数的优势估计进行指数加权平均：</p><p><span class="math display">\[ \begin{align*} A_t^{GAE}&amp;=(1-\lambda)(A_t^{(1)}+\lambda A_t^{(2)}+\lambda^2 A_t^{(3)}+...) \\ &amp;=(1-\lambda)(\delta_t+\lambda(\delta_t+\gamma\delta_{t+1})+\lambda^2(\delta_t+\gamma\delta_{t+1}+\gamma^2\delta_{t+2})...) \\ &amp;=(1-\lambda)(\delta_t(1+\lambda+\lambda^2+...)+\gamma\delta_{t+1}(\lambda+\lambda^2+\lambda^3+...)+\gamma^2\delta_{t+2}(\lambda^2+\lambda^3+\lambda^4+...)) \\ &amp;=(1-\lambda)(\delta_t\frac{1}{1-\lambda}+\gamma\delta_{t+1}\frac{\lambda}{1-\lambda}+\gamma^2\delta_{t+2}\frac{\lambda^2}{1-\lambda}+...) \\ &amp;=\sum_{l=0}^\infty(\gamma\lambda)^l\delta_{t+l} \end{align*} \]</span></p><p>当<span class="math inline">\(\lambda=0\)</span>时，<span class="math inline">\(A_t^{GAE}=\delta_t=r_t+\gamma V(s_{t+1})-V(s_t)\)</span>，只看一步差分；当<span class="math inline">\(\lambda=1\)</span>时，<span class="math inline">\(A_t^{GAE}=\sum_{l=0}^\infty\gamma^l\delta_{t+l}=\sum_{l=0}^\infty\gamma^lr_{t+l}-V(s_t)\)</span>，看每一步差分的完全平均值。</p><h2 id="kl-散度">KL 散度</h2><p>回到重要性采样谈到的一个观点：使用<span class="math inline">\(\theta&#39;\)</span>近似<span class="math inline">\(\theta\)</span>时，其分布不宜相差太多；这里意为：<strong>最新策略<span class="math inline">\(\pi_\theta\)</span> 不宜离采样策略<span class="math inline">\(\pi_{\theta&#39;}\)</span>太远</strong>，否则影响重要性采样的拟合效果。因此需要在训练时加一个约束项。</p><p>TRPO 采用 KL 散度衡量策略之间的距离，整体优化公式为：</p><p><span class="math display">\[ \max_{\theta}R^{\theta&#39;}(\theta) \\ s.t. \\ \mathbb{E}_{s\sim\nu^{\pi_{\theta_k}}}[D_{KL}(\pi_{\theta_k}(·|s), \pi_{\theta}(·|s))]\leq \delta \]</span></p><p>其中，<span class="math inline">\(\nu^{\pi}(s)=(1-\gamma)\sum_{t=0}^\infty\gamma^t P_t^\pi(s)\)</span>是状态分布的定义。</p><p>上述优化目标中的 KL 约束定义了策略空间中的一个 KL 球，称为<strong>信任区域</strong>。</p><blockquote><p>近似求解：</p><p>假设<span class="math inline">\(\theta_k\)</span>为第<span class="math inline">\(k\)</span>次迭代后的目标策略。对目标函数和 KL 约束分别在<span class="math inline">\(\theta_k\)</span>处执行泰勒展开，分别使用一阶、二阶近似：</p><p><span class="math display">\[ R^{\theta&#39;}(\theta)\approx g^T(\theta-\theta_k) \\ \mathbb{E}_{s\sim\nu^{\pi_{\theta_k}}}[D_{KL}(\pi_{\theta_k}(·|s), \pi_{\theta}(·|s))]\approx \frac{1}{2}(\theta-\theta_k)^T H(\theta-\theta_k) \]</span></p><p>其中：<span class="math inline">\(g=\nabla_{\theta}\mathbb{E}_{s\sim\nu^{\pi_{\theta_k}}}\mathbb{E}_{a\sim\pi_{\theta_k}(·|s)}[\frac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)}A^{\pi_{\theta_k}}(s, a)]\)</span>，即目标函数的梯度；<span class="math inline">\(H=\bold{H}[\mathbb{E}_{s\sim\nu^{\pi_{\theta_k}}}[D_{KL}(\pi_{\theta_k}(·|s), \pi_{\theta}(·|s))]]\)</span>表示策略之间平均 KL 距离的黑塞矩阵。</p><p>优化目标转为： <span class="math display">\[ \theta_{k+1}=\argmax_{\theta}g^T(\theta-\theta_k) \\ s.t. \\ \frac{1}{2}(\theta-\theta_k)^T H(\theta-\theta_k)\leq\delta \]</span></p><p>可以用 KKT 条件直接导出上述问题的解。</p></blockquote><p>TRPO 的约束并不好处理。如果放在目标函数中呢？这就是 PPO.</p><p><span class="math display">\[ R_{PPO}^{\theta&#39;}(\theta)=R^{\theta&#39;}(\theta)-\beta KL(\theta, \theta&#39;) \]</span></p><p>注意：这里的<span class="math inline">\(KL(\theta, \theta&#39;)\)</span>并非参数上的距离，而是行为上的距离（给定相同状态，输出动作的差距）</p><h2 id="ppo">PPO</h2><h3 id="ppo-penalty">PPO-penalty</h3><p>对于 off-policy 策略，使用<span class="math inline">\(\theta_k\)</span>采样一次得到的数据，多次用于目标策略<span class="math inline">\(\theta\)</span>的更新。即：</p><p><span class="math display">\[ R_{PPO}^{\theta_k}(\theta)=R^{\theta_k}(\theta)-\beta KL(\theta, \theta_k) \]</span></p><p>采用<strong>自适应 KL 散度</strong>（动态调整<span class="math inline">\(\beta\)</span>）。假设优化完成后， KL 散度值太大，说明惩罚项<span class="math inline">\(\beta KL(\theta, \theta_k)\)</span>没有发挥作用，应该增大<span class="math inline">\(\beta\)</span>；如果优化后 KL 散度值太小，说明惩罚项太强，应该减小<span class="math inline">\(\beta\)</span>.即：</p><ul><li>如果 <span class="math inline">\(KL(\theta, \theta_k)&gt;KL_{max}\)</span>，增大<span class="math inline">\(\beta\)</span>；</li><li>如果 <span class="math inline">\(KL(\theta, \theta_k)&lt;KL_{min}\)</span>，减小<span class="math inline">\(\beta\)</span>.</li></ul><p>优化目标为：</p><p><span class="math display">\[ R_{PPO}^{\theta_k}=R_{PPO}^{\theta}-\beta KL(\theta, \theta_k) \\ R^{\theta_k}(\theta)\approx\sum_{(s_t, a_t)}\frac{p_\theta(a_t|s_t)}{p_{\theta_k}(a_t|s_t)}A^{\theta_k}(s_t, a_t) \]</span></p><h3 id="ppo-clip">PPO-clip</h3><p>计算 KL 散度太复杂，目标函数改写为：</p><p><span class="math display">\[ R_{PPO}^{\theta_k}(\theta)\approx\sum_{(s_t, a_t)}\min(\frac{p_\theta(a_t|s_t)}{p_{\theta_k}(a_t|s_t)}A^{\theta_k}(s_t, a_t), clip(\frac{p_\theta(a_t|s_t)}{p_{\theta_k}(a_t|s_t)}, 1-\epsilon, 1+\epsilon)A^{\theta_k}(s_t, a_t)) \]</span></p><p>clip 的意义是使得<span class="math inline">\(p_\theta(a_t|s_t)\)</span>与<span class="math inline">\(p_{\theta_k}(a_t|s_t)\)</span>尽可能接近，优化后差距不太大。</p><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/88142594/image.png"> 如上图，在大多数 RL 任务中，PPO 算法效果较好。</p><h2 id="参考">参考</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/1973206684907365344">【前沿追踪】从两策略到三策略：行为策略和参考策略不一致下的 TRPO 扩展</a></p></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://wenliuyi.github.io">Liuyi Wen</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://wenliuyi.github.io/posts/88142594.html">http://wenliuyi.github.io/posts/88142594.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://wenliuyi.github.io" target="_blank">Liuyi Wen's Blog</a>！</span></div></div><div class="tag_share"><div class="post-share"><div class="social-share" data-image="/img/WechatIMG105.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/posts/ef1b2883.html" title="并行训练系列：6. 序列并行上篇（Megatron-SP, DeepSpeed-Ulysses）"><div class="cover" style="background:var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">并行训练系列：6. 序列并行上篇（Megatron-SP, DeepSpeed-Ulysses）</div></div><div class="info-2"><div class="info-item-1">中间激活值的显存占用量 参考：https://lywencoding.com/posts/76aa9d6e.html 对 Transformer 在训练过程中的模型参数、中间激活值的显存占用量进行了分析。直接给出结论： 对于\(l\)层的 Transformer 模型：模型参数量为 \((12h^2+13h)*l\)；中间激活值为\((34bsh+5bs^2a)*l\)。 可以发现：模型参数的显存占用量与输入数据大小无关；中间激活值的显存占用量与输入数据大小（批次大小\(b\)和序列长度\(s\)）成正相关。 显存占用量随着批量大小线性增长，并随着序列长度的平方增长，那么：激活内存是最容易“膨胀”的部分；对于短序列（或者小批量大小），激活几乎可以忽略不计；但从大约 2-4k 个 token 开始，它们开始占用大量显存。 如何优化激活值的占用呢？有以下几种策略： 激活值重计算：抛弃 FWD 的部分激活值；在 BWD 时实时重新计算。（当前大多数框架使用 Flash Attention） 梯度累积：将 batch 拆分为若干个小的 micro-batch；依次在每个...</div></div></div></a><a class="pagination-related" href="/posts/f74f2231.html" title="并行训练系列：7. Flash Attention V1/V2"><div class="cover" style="background:var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">并行训练系列：7. Flash Attention V1/V2</div></div><div class="info-2"><div class="info-item-1">对于输入序列长度为\(N\)的 Transformer 类模型，其计算复杂度和存储空间为\(O(N^2)\)；Flash Attention（Fast and Memory Efficient Exact Attention with IO-Awareness） 技术旨在缓解上述计算和存储压力。 一个洞察为：计算慢的卡点在于读写速度，而非计算能力。Flash Attention 通过 tiling 和 kernel fusion 降低对显存（HBM）的访问次数以加速计算。 计算瓶颈分析： 定义： \(\pi\)：硬件算力上限。每秒钟能完成的浮点运算次数，单位是 FLOPS 或...</div></div></div></a></nav><hr class="custom-hr"><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/img/WechatIMG105.jpg" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info-name">Liuyi Wen</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">50</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">5</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/WenLiuyi"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">The Journey Is the Reward.</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%87%8D%E8%A6%81%E6%80%A7%E9%87%87%E6%A0%B7"><span class="toc-number">1.</span> <span class="toc-text">重要性采样</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%98%E5%8A%BF%E5%87%BD%E6%95%B0"><span class="toc-number">2.</span> <span class="toc-text">优势函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B9%BF%E4%B9%89%E4%BC%98%E5%8A%BF%E4%BC%B0%E8%AE%A1gae"><span class="toc-number">2.1.</span> <span class="toc-text">广义优势估计（GAE）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#kl-%E6%95%A3%E5%BA%A6"><span class="toc-number">3.</span> <span class="toc-text">KL 散度</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ppo"><span class="toc-number">4.</span> <span class="toc-text">PPO</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ppo-penalty"><span class="toc-number">4.1.</span> <span class="toc-text">PPO-penalty</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ppo-clip"><span class="toc-number">4.2.</span> <span class="toc-text">PPO-clip</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83"><span class="toc-number">5.</span> <span class="toc-text">参考</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/f74f2231.html" title="并行训练系列：7. Flash Attention V1/V2">并行训练系列：7. Flash Attention V1/V2</a><time datetime="2025-12-16T13:23:23.000Z" title="发表于 2025-12-16 21:23:23">2025-12-16</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/88142594.html" title="RL 系列：5. 从 TRPO 到 PPO 算法">RL 系列：5. 从 TRPO 到 PPO 算法</a><time datetime="2025-12-08T07:52:43.000Z" title="发表于 2025-12-08 15:52:43">2025-12-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/ef1b2883.html" title="并行训练系列：6. 序列并行上篇（Megatron-SP, DeepSpeed-Ulysses）">并行训练系列：6. 序列并行上篇（Megatron-SP, DeepSpeed-Ulysses）</a><time datetime="2025-12-01T07:51:23.000Z" title="发表于 2025-12-01 15:51:23">2025-12-01</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/76477a6b.html" title="verl 框架：3. 加载数据与创建 batch">verl 框架：3. 加载数据与创建 batch</a><time datetime="2025-11-24T09:29:10.000Z" title="发表于 2025-11-24 17:29:10">2025-11-24</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/49c2e3ad.html" title="RL 系列：4. 策略梯度算法">RL 系列：4. 策略梯度算法</a><time datetime="2025-11-17T14:21:39.000Z" title="发表于 2025-11-17 22:21:39">2025-11-17</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By Liuyi Wen</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(()=>{const t=()=>{if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"all"},chtml:{scale:1.1},options:{enableMenu:!0,renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const n=!!e.type.match(/; *mode=display/),a=new t.options.MathItem(e.textContent,t.inputJax[0],n),d=document.createTextNode("");e.parentNode.replaceChild(d,e),a.start={node:d,delim:"",n:0},a.end={node:d,delim:"",n:0},t.math.push(a)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}};btf.addGlobalFn("encrypt",t,"mathjax"),window.pjax?t():window.addEventListener("load",t)})()</script><script>(()=>{const n="shuoshuo"===GLOBAL_CONFIG_SITE.pageType,e=(e,o)=>{n&&(window.shuoshuoComment.destroyValine=()=>{e.children.length&&(e.innerHTML="",e.classList.add("no-comment"))});const t={el:"#vcomment",appId:"bsxtUJWr1muoPS1pmoXLOPZ2-gzGzoHsz",appKey:"wm2wUYvKLEySwyRnFn7xAbJI",avatar:"monsterid",serverURLs:"",emojiMaps:"",visitor:!1,path:n?o:window.location.pathname};new Valine(t)},o=async(n,o)=>{"function"==typeof Valine||await btf.getScript("https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"),e(n,o)};n?window.shuoshuoComment={loadComment:o}:btf.loadComment(document.getElementById("vcomment"),o)})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-info"><div class="algolia-stats"></div><div class="algolia-poweredBy"></div></div></div></div></div><div id="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/algoliasearch/dist/lite/builds/browser.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instantsearch.js/dist/instantsearch.production.min.js"></script><script src="/js/search/algolia.js"></script></div></div>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body></html>