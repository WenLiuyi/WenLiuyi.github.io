<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>Transformer系列：2. Attention机制，MHA，MQA和GQA | Liuyi Wen's Blog</title><meta name="author" content="Liuyi Wen"><meta name="copyright" content="Liuyi Wen"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Scaled Dot-Product Attention 只使用一个注意力头计算权重。 假设有输入序列\(X&#x3D;(x_1, x_2,..., x_n)\)，对于每个词\(x_i\)，计算其与所有其他词的相关性，并赋予不同的权重，最后对这些信息加权求和，得到新的表示。 \[ Attention(Q, K, V)&#x3D;softmax(\frac{QK^{T}}{\sqrt{d_k}})V \]  分为以下几"><meta property="og:type" content="article"><meta property="og:title" content="Transformer系列：2. Attention机制，MHA，MQA和GQA"><meta property="og:url" content="http://wenliuyi.github.io/posts/1e6a04d4.html"><meta property="og:site_name" content="Liuyi Wen&#39;s Blog"><meta property="og:description" content="Scaled Dot-Product Attention 只使用一个注意力头计算权重。 假设有输入序列\(X&#x3D;(x_1, x_2,..., x_n)\)，对于每个词\(x_i\)，计算其与所有其他词的相关性，并赋予不同的权重，最后对这些信息加权求和，得到新的表示。 \[ Attention(Q, K, V)&#x3D;softmax(\frac{QK^{T}}{\sqrt{d_k}})V \]  分为以下几"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://wenliuyi.github.io/img/butterfly-icon.png"><meta property="article:published_time" content="2025-06-29T07:47:26.000Z"><meta property="article:modified_time" content="2025-06-29T14:17:52.126Z"><meta property="article:author" content="Liuyi Wen"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="http://wenliuyi.github.io/img/butterfly-icon.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Transformer系列：2. Attention机制，MHA，MQA和GQA",
  "url": "http://wenliuyi.github.io/posts/1e6a04d4.html",
  "image": "http://wenliuyi.github.io/img/butterfly-icon.png",
  "datePublished": "2025-06-29T07:47:26.000Z",
  "dateModified": "2025-06-29T14:17:52.126Z",
  "author": [
    {
      "@type": "Person",
      "name": "Liuyi Wen",
      "url": "http://wenliuyi.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://wenliuyi.github.io/posts/1e6a04d4.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>(()=>{const e={set:(e,t,o)=>{if(!o)return;const a=Date.now()+864e5*o;localStorage.setItem(e,JSON.stringify({value:t,expiry:a}))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const{value:o,expiry:a}=JSON.parse(t);if(!(Date.now()>a))return o;localStorage.removeItem(e)}};window.btf={saveToLocal:e,getScript:(e,t={})=>new Promise((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,Object.entries(t).forEach(([e,t])=>n.setAttribute(e,t)),n.onload=n.onreadystatechange=()=>{n.readyState&&!/loaded|complete/.test(n.readyState)||o()},n.onerror=a,document.head.appendChild(n)}),getCSS:(e,t)=>new Promise((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onload=n.onreadystatechange=()=>{n.readyState&&!/loaded|complete/.test(n.readyState)||o()},n.onerror=a,document.head.appendChild(n)}),addGlobalFn:(e,t,o=!1,a=window)=>{if(e.startsWith("pjax"))return;const n=a.globalFn||{};n[e]=n[e]||{},n[e][o||Object.keys(n[e]).length]=t,a.globalFn=n}};const t=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},o=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};btf.activateDarkMode=t,btf.activateLightMode=o;const a=e.get("theme");"dark"===a?t():"light"===a&&o();const n=e.get("aside-status");void 0!==n&&document.documentElement.classList.toggle("hide-aside","hide"===n);/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})()</script><script>const GLOBAL_CONFIG={root:"/",algolia:{appId:"VE36MEFVE6",apiKey:"f9b9ca5a3cdb9455658600dba6ae7706",indexName:"hexo-algolia indexing key",hitsPerPage:6,languages:{input_placeholder:"搜索文章",hits_empty:"未找到符合您查询的内容：${query}",hits_stats:"找到 ${hits} 条结果，耗时 ${time} 毫秒"}},localSearch:void 0,translate:void 0,highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:!1,highlightFullpage:!1,highlightMacStyle:!1},copy:{success:"复制成功",error:"复制失败",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"null",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!1,islazyloadPlugin:!1,isAnchor:!1,percent:{toc:!0,rightside:!1},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"Transformer系列：2. Attention机制，MHA，MQA和GQA",isHighlightShrink:!1,isToc:!0,pageType:"post"}</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Liuyi Wen's Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">Transformer系列：2. Attention机制，MHA，MQA和GQA</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div></div></nav><div id="post-info"><h1 class="post-title">Transformer系列：2. Attention机制，MHA，MQA和GQA</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-06-29T07:47:26.000Z" title="发表于 2025-06-29 15:47:26">2025-06-29</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-06-29T14:17:52.126Z" title="更新于 2025-06-29 22:17:52">2025-06-29</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Transformer/">Transformer</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h2 id="scaled-dot-product-attention">Scaled Dot-Product Attention</h2><p>只使用一个注意力头计算权重。</p><p>假设有输入序列<span class="math inline">\(X=(x_1, x_2,..., x_n)\)</span>，<strong>对于每个词<span class="math inline">\(x_i\)</span>，计算其与所有其他词的相关性，并赋予不同的权重</strong>，最后对这些信息加权求和，得到新的表示。 <span class="math display">\[ Attention(Q, K, V)=softmax(\frac{QK^{T}}{\sqrt{d_k}})V \]</span></p><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/1e6a04d4/image-18.png"></p><p>分为以下几个步骤：</p><ol type="1"><li><p><strong>计算Query, Key, Value矩阵</strong>：每个输入token被映射为三个不同的向量：</p><ul><li>Q：当前需要关注的内容，例如在机器翻译中，查询可能是目标语言句子中的一个token；</li><li>K：与查询进行匹配的内容，例如源语言句子中的token；</li><li>V：最终要提取的信息，通常与键对应。</li></ul><p>转换矩阵： <span class="math display">\[ Q=XW_Q, K=XW_K, V=XW_V \]</span> 其中，<span class="math inline">\(W_Q, W_K, W_V\)</span>是可学习的参数矩阵。</p><p><strong>输入：维度<span class="math inline">\(d_k\)</span>的queries和keys</strong>；<strong>输出：维度为<span class="math inline">\(d_v\)</span>的values</strong></p><blockquote><p>查询矩阵Q的维度：[<span class="math inline">\(n_q, d_k\)</span>]，<span class="math inline">\(n_q\)</span>为queries的数量；<span class="math inline">\(d_k\)</span>是每个query的维度</p><p>键矩阵K的维度：[<span class="math inline">\(n_k, d_k\)</span>]，<span class="math inline">\(n_q\)</span>为keys的数量；<span class="math inline">\(d_k\)</span>是每个key的维度</p><p>值矩阵V的维度：[<span class="math inline">\(n_k, d_v\)</span>]，<span class="math inline">\(n_k\)</span>为queries的数量；<span class="math inline">\(d_k\)</span>是每个query的维度</p><ol type="1"><li><strong>Q和K的维度必须一致</strong>：V和Q/K的维度可以不一致；</li><li><strong>K和V的长度必须一致</strong>：K和V本质上对应同一个sequence在不同空间的表达。</li></ol><p>Attention得到的output：[<span class="math inline">\(n_q, d_v\)</span>]，维度与V一致，长度与K一致。</p></blockquote></li><li><p><strong>计算点积</strong>：得到注意力分数矩阵 <span class="math display">\[ scores=QK^{T} \]</span></p></li><li><p><strong>缩放</strong>：将点积除以<span class="math inline">\(\sqrt{d_k}\)</span>，其中：<span class="math inline">\(\sqrt{d_k}\)</span>是Key向量的维度，<span class="math inline">\(\sqrt{d_k}\)</span>是缩放因子，避免数值过大导致梯度消失。</p><blockquote><p><strong>为什么要使用缩放因子<span class="math inline">\(\sqrt{d_k}\)</span>？</strong> <strong>归一化</strong></p><p>假设<span class="math inline">\(Q, K\)</span>里的元素均值为0，方差为1，那么：<span class="math inline">\(A=QK^{T}\)</span>中元素均值为0，方差为<span class="math inline">\(d\)</span>。当d变得很大时，<span class="math inline">\(A\)</span>中的元素方差也变得很大，导致<span class="math inline">\(softmax(A)\)</span>的分布也趋于陡峭（分布的方差大，分布集中在绝对值大的区域）。</p><p><span class="math inline">\(A\)</span>中每一个元素乘上<span class="math inline">\(\frac{1}{\sqrt{d_k}}\)</span>后，方差又回到1，使得：<span class="math inline">\(softmax(A)\)</span>的分布陡峭程度与<span class="math inline">\(d\)</span>解耦，从而使得训练过程中，梯度值保持稳定。</p></blockquote></li><li><p><strong>softmax归一化</strong>：对缩放后的点积结果，应用softmax函数，得到注意力权重矩阵A： <span class="math display">\[ A=softmax(\frac{QK^{T}}{\sqrt{d_k}}) \]</span></p></li><li><p><strong>加权求和</strong>：将注意力权重矩阵<span class="math inline">\(A\)</span>与值矩阵<span class="math inline">\(V\)</span>相乘，得到加权求和的结果。</p></li></ol><p>单头注意力机制代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SingleHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embed_dim</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        单头注意力机制的初始化。</span></span><br><span class="line"><span class="string">        :param embed_dim: 嵌入维度，Query、Key 和 Value 的维度</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(SingleHeadAttention, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.embed_dim = embed_dim</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义线性层，将输入映射到 Query、Key 和 Value</span></span><br><span class="line">        <span class="variable language_">self</span>.query_linear = nn.Linear(embed_dim, embed_dim)</span><br><span class="line">        <span class="variable language_">self</span>.key_linear = nn.Linear(embed_dim, embed_dim)</span><br><span class="line">        <span class="variable language_">self</span>.value_linear = nn.Linear(embed_dim, embed_dim)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 缩放因子，用于防止点积结果过大</span></span><br><span class="line">        <span class="variable language_">self</span>.scale = torch.sqrt(torch.FloatTensor($embed_dim]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, query, key, value</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        单头注意力的前向传播。</span></span><br><span class="line"><span class="string">        :param query: 查询张量，形状为 $batch_size, seq_len_q, embed_dim]</span></span><br><span class="line"><span class="string">        :param key: 键张量，形状为 $batch_size, seq_len_k, embed_dim]</span></span><br><span class="line"><span class="string">        :param value: 值张量，形状为 $batch_size, seq_len_k, embed_dim]</span></span><br><span class="line"><span class="string">        :return: 输出张量，形状为 $batch_size, seq_len_q, embed_dim]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 将输入映射到 Query、Key 和 Value</span></span><br><span class="line">        Q = <span class="variable language_">self</span>.query_linear(query)</span><br><span class="line">        K = <span class="variable language_">self</span>.key_linear(key)</span><br><span class="line">        V = <span class="variable language_">self</span>.value_linear(value)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算点积注意力分数</span></span><br><span class="line">        attention_scores = torch.matmul(Q, K.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / <span class="variable language_">self</span>.scale</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 应用 Softmax 函数，得到注意力权重</span></span><br><span class="line">        attention_weights = F.softmax(attention_scores, dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 加权求和，得到最终输出</span></span><br><span class="line">        output = torch.matmul(attention_weights, V)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output, attention_weights</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例输入</span></span><br><span class="line"><span class="comment"># 假设我们有以下输入张量：</span></span><br><span class="line"><span class="comment"># - query: $batch_size, seq_len_q, embed_dim]</span></span><br><span class="line"><span class="comment"># - key: $batch_size, seq_len_k, embed_dim]</span></span><br><span class="line"><span class="comment"># - value: $batch_size, seq_len_k, embed_dim]</span></span><br><span class="line">batch_size = <span class="number">2</span></span><br><span class="line">seq_len_q = <span class="number">3</span><span class="comment"># query的序列长度</span></span><br><span class="line">seq_len_k = <span class="number">4</span><span class="comment">#k,v的序列长度，注意这里K、V是成对存在的</span></span><br><span class="line">embed_dim = <span class="number">6</span><span class="comment"># 假设embedding的维度为6</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机生成输入数据</span></span><br><span class="line">query = torch.randn(batch_size, seq_len_q, embed_dim)</span><br><span class="line">key = torch.randn(batch_size, seq_len_k, embed_dim)</span><br><span class="line">value = torch.randn(batch_size, seq_len_k, embed_dim)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化单头注意力模块</span></span><br><span class="line">attention = SingleHeadAttention(embed_dim)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line">output, attention_weights = attention(query, key, value)</span><br></pre></td></tr></table></figure><h2 id="mha">MHA</h2><p>单头注意力中，模型只能通过一个注意力头来捕捉输入数据中的特征，这限制了模型对复杂关系的建模能力。而多头注意力（Multi-Head Attention）是Transformer架构的核心组件，它的核心思想是：<strong>将输入数据分解为多个子空间，每个子空间通过一个独立的注意力“头”（heads）进行处理，最后将所有heads的输出合并</strong>，从而能够捕捉到输入数据中不同子空间的特征；同时其复杂度并无增加。</p><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/1e6a04d4/image-19.png"></p><p>步骤如下：</p><ol type="1"><li><p><strong>计算Query，Key，Value矩阵</strong>： <span class="math display">\[ Q=XW_Q, K=XW_K, V=XW_V \]</span></p></li><li><p><strong>分割多个heads</strong>：假设有<span class="math inline">\(h\)</span>个heads，每个head的维度为<span class="math inline">\(d_k\)</span>，则有： <span class="math display">\[ d_k=\frac{d_{dim}}{h} \]</span> 其中，<span class="math inline">\(d_{dim}\)</span>是模型的嵌入维度。</p><p>分割后的Q，K，V如下： <span class="math display">\[ Q_i=split(Q, i) \\ K_i=split(K, i) \\ V_i=split(V, i) \]</span> 其中，<span class="math inline">\(i\)</span>表示第<span class="math inline">\(i\)</span>个头。</p></li><li><p><strong>计算每个head的注意力</strong>：</p><ol type="1"><li><p><strong>计算点积注意力分数</strong>： <span class="math display">\[ A_i=Q_i\times K_i^{T} \]</span></p></li><li><p><strong>缩放</strong>： <span class="math display">\[ S_i=\frac{A_i}{\sqrt{d_k}} \]</span></p></li><li><p><strong>SoftMax</strong>： <span class="math display">\[ W_i=softmax(S_i) \]</span></p></li><li><p><strong>加权求和</strong>： <span class="math display">\[ O_i=W_i\times V_i \]</span></p></li></ol></li><li><p><strong>合并所有head的输出</strong>： <span class="math display">\[ O=concat(O_1,O_2,...,O_h)W^{O} \]</span></p></li></ol><p>用一些示意图辅助理解：</p><ol type="1"><li><p>假设输入序列的seq_len=4，hidden_size=8，使用2头注意力。弱化batch_size（假设为1）.</p><blockquote><p><span class="math inline">\(Q=XW_Q\)</span>：<span class="math inline">\([s, h]\times [h, h]]\rightarrow [s, h]\)</span></p></blockquote><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/1e6a04d4/image-20.png"></p></li><li><p><strong>每个head</strong>：对于每个<span class="math inline">\(Q_i, K_i, V_i\)</span>，分别计算attention，最后得到一个[2, 4, 4]的矩阵，即<strong><span class="math inline">\([h, s, d_i]\)</span></strong>. （<strong>引入head，切分hidden_size</strong>，设每个head的hidden_size为<span class="math inline">\(d_i\)</span>）</p><blockquote><p><span class="math inline">\(QK^T=[h, s, d_i]\times [h, d_i, s]\rightarrow [h, s, s]\)</span></p></blockquote><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/1e6a04d4/image-21.png"></p></li><li><p>重新拼接为[8,4]的矩阵，即<span class="math inline">\([s, d]\)</span>；再经过<span class="math inline">\(W_O\)</span>，得到<span class="math inline">\(O\)</span>矩阵，即输出。</p><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/1e6a04d4/image-22.png"></p></li></ol><p>MHA代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embed_dim, num_heads</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        多头注意力机制的初始化。</span></span><br><span class="line"><span class="string">        :param embed_dim: 嵌入维度</span></span><br><span class="line"><span class="string">        :param num_heads: 头的数量</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttention, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.embed_dim = embed_dim</span><br><span class="line">        <span class="variable language_">self</span>.num_heads = num_heads</span><br><span class="line">        <span class="variable language_">self</span>.head_dim = embed_dim // num_heads</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> <span class="variable language_">self</span>.head_dim * num_heads == embed_dim, <span class="string">&quot;Embed size needs to be divisible by heads&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义线性层，将输入映射到 Query、Key 和 Value</span></span><br><span class="line">        <span class="variable language_">self</span>.query_linear = nn.Linear(embed_dim, embed_dim)</span><br><span class="line">        <span class="variable language_">self</span>.key_linear = nn.Linear(embed_dim, embed_dim)</span><br><span class="line">        <span class="variable language_">self</span>.value_linear = nn.Linear(embed_dim, embed_dim)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义输出的线性层</span></span><br><span class="line">        <span class="variable language_">self</span>.out = nn.Linear(embed_dim, embed_dim)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 缩放因子</span></span><br><span class="line">        <span class="variable language_">self</span>.scale = torch.sqrt(torch.FloatTensor([<span class="variable language_">self</span>.head_dim]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, query, key, value</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        多头注意力的前向传播。</span></span><br><span class="line"><span class="string">        :param query: 查询张量，形状为 [batch_size, seq_len_q, embed_dim]</span></span><br><span class="line"><span class="string">        :param key: 键张量，形状为 [batch_size, seq_len_k, embed_dim]</span></span><br><span class="line"><span class="string">        :param value: 值张量，形状为 [batch_size, seq_len_k, embed_dim]</span></span><br><span class="line"><span class="string">        :return: 输出张量，形状为 [batch_size, seq_len_q, embed_dim]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        batch_size = query.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将输入映射到 Query、Key 和 Value</span></span><br><span class="line">        Q = <span class="variable language_">self</span>.query_linear(query)  <span class="comment"># [batch_size, seq_len_q, embed_dim]</span></span><br><span class="line">        K = <span class="variable language_">self</span>.key_linear(key)      <span class="comment"># [batch_size, seq_len_k, embed_dim]</span></span><br><span class="line">        V = <span class="variable language_">self</span>.value_linear(value)  <span class="comment"># [batch_size, seq_len_k, embed_dim]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 分割成多个头</span></span><br><span class="line">        Q = Q.view(batch_size, -<span class="number">1</span>, <span class="variable language_">self</span>.num_heads, <span class="variable language_">self</span>.head_dim).transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># [batch_size, num_heads, seq_len_q, head_dim]</span></span><br><span class="line">        K = K.view(batch_size, -<span class="number">1</span>, <span class="variable language_">self</span>.num_heads, <span class="variable language_">self</span>.head_dim).transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># [batch_size, num_heads, seq_len_k, head_dim]</span></span><br><span class="line">        V = V.view(batch_size, -<span class="number">1</span>, <span class="variable language_">self</span>.num_heads, <span class="variable language_">self</span>.head_dim).transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># [batch_size, num_heads, seq_len_k, head_dim]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算点积注意力分数</span></span><br><span class="line">        attention_scores = torch.matmul(Q, K.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / <span class="variable language_">self</span>.scale  <span class="comment"># [batch_size, num_heads, seq_len_q, seq_len_k]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 应用 Softmax 函数，得到注意力权重</span></span><br><span class="line">        attention_weights = F.softmax(attention_scores, dim=-<span class="number">1</span>)  <span class="comment"># [batch_size, num_heads, seq_len_q, seq_len_k]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 加权求和，得到每个头的输出</span></span><br><span class="line">        output = torch.matmul(attention_weights, V)  <span class="comment"># [batch_size, num_heads, seq_len_q, head_dim]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 合并所有头的输出</span></span><br><span class="line">        output = output.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(batch_size, -<span class="number">1</span>, <span class="variable language_">self</span>.embed_dim)  <span class="comment"># [batch_size, seq_len_q, embed_dim]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通过输出的线性层</span></span><br><span class="line">        output = <span class="variable language_">self</span>.out(output)  <span class="comment"># [batch_size, seq_len_q, embed_dim]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output, attention_weights</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例输入</span></span><br><span class="line">batch_size = <span class="number">2</span></span><br><span class="line">seq_len_q = <span class="number">3</span></span><br><span class="line">seq_len_k = <span class="number">4</span></span><br><span class="line">embed_dim = <span class="number">16</span></span><br><span class="line">num_heads = <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机生成输入数据</span></span><br><span class="line">query = torch.randn(batch_size, seq_len_q, embed_dim)</span><br><span class="line">key = torch.randn(batch_size, seq_len_k, embed_dim)</span><br><span class="line">value = torch.randn(batch_size, seq_len_k, embed_dim)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化多头注意力模块</span></span><br><span class="line">attention = MultiHeadAttention(embed_dim, num_heads)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line">output, attention_weights = attention(query, key, value)</span><br></pre></td></tr></table></figure><h2 id="kv-cache">KV Cache</h2><p>大模型在<strong>decode阶段采用自回归的方式</strong>。即：<strong>最新的token输出依赖于先前生成或者预先填入的Token</strong>。</p><p>假如我们输入“窗前明月光下一句是”：decode过程如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">step0: 输入=[BOS]窗前明月光下一句是；输出=疑</span><br><span class="line">step1: 输入=[BOS]窗前明月光下一句是疑；输出=是</span><br><span class="line">step2: 输入=[BOS]窗前明月光下一句是疑是；输出=地</span><br><span class="line">step3: 输入=[BOS]窗前明月光下一句是疑是地；输出=上</span><br><span class="line">step4: 输入=[BOS]窗前明月光下一句是疑是地上；输出=霜</span><br><span class="line">step5: 输入=[BOS]窗前明月光下一句是疑是地上霜；输出=[EOS]</span><br></pre></td></tr></table></figure><p>在生成“疑”字时，用的是<strong>输入序列中“是”字的最后一层hidden state</strong>，再通过最后的分类头预测。可以注意到：下一个step的输入包含了上一个step的内容，而且只在最后面多一个token；因此下一个step的计算也包含了上一个step的计算。</p><p>由于<strong>decoder是casual的（一个token的attention只依赖于之前的token，得益于mask attention）</strong>。因此在自回归生成的过程中，每一步会重复计算之前所有tokens的attention，可简化为：只计算新token的attention。</p><p>如下图：空的方块代表可以以前的steps中重用的计算部分：</p><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/1e6a04d4/image-23.png"></p><h3 id="key-cache">Key Cache</h3><p>维护一个密钥缓存，存储：在每次迭代中计算的键向量。当前step的流程如下：</p><ol type="1"><li><p>只计算一个Query向量和一个Key向量：</p><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/1e6a04d4/image-24.png"></p></li><li><p>从Key Cache中提取先前steps计算的Key Vectors，计算Attention Score的最后一行，即新的Query Vector与所有Key Vectors的点积：</p><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/1e6a04d4/image-25.png"></p></li></ol><h3 id="value-cache">Value Cache</h3><p>与Key Vector类似，每个step只需要计算最新的Value Vector；其他Value Vectors可以从Value Cache中提取并重复使用：</p><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/1e6a04d4/image-26.png"></p><h2 id="mqa">MQA</h2><p>KV Cache虽然可以解决kv重复计算的问题，但面对长上下文时，显存占用量巨大。</p><blockquote><p>以llama3-8B模型为例：模型序列长度<span class="math inline">\(L=8192\)</span>(8K)；Transformer层数<span class="math inline">\(N=32\)</span>，注意力头数<span class="math inline">\(H=32\)</span>，每个注意力头的维度<span class="math inline">\(D=128\)</span>，batch按照1算，数据类型为BF16（2个字节），需要的缓存为： <span class="math display">\[ token_{kv}=2\times 1\times 32\times 8192\times 128\times 32\times 2=4294967296 \]</span> 即4GB。</p></blockquote><p>MQA的核心思想是：<strong>所有注意力头共享一份Key和Value矩阵，仅保留Query的多头性质</strong>。即：Key和Value的计算是唯一的，而Query则根据不同的头进行独立转换。</p><blockquote><p>在下图中：</p><p>当 batch size=1 时，图中红色、绿色、蓝色虚线圈处的乘法全部为矩阵乘向量，是Memory Bound，算术强度不到 1。</p><p>当 batch size&gt;1 时（比如 Continuous Batching）：</p><ul><li>红色和蓝色部分：<strong>线性层计算是权重乘以激活</strong>，<strong>不同请求之间可以共享权重</strong>，因此是矩阵乘矩阵，并且 Batch Size 越大，算术强度越大，越趋近于计算密集型（FFN 层也类似）；</li><li>绿色部分：<strong>注意力计算是激活乘以激活</strong>。因为<strong>不同的请求之间没有任何相关性</strong>，即使 Batching，此处也是 Batched 矩阵乘向量，并且因为序列长度可能不同，这里不同请求的矩阵乘向量是不规则的。即，这里算术强度始终不到 1，是Memory Bound。</li></ul><p>因此绿色部分较难优化，输入序列越长，瓶颈越大。</p></blockquote><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/1e6a04d4/image-29.png"></p><blockquote><p>与MHA对比：</p><p><strong>MHA</strong>：输入分别经过<span class="math inline">\(W_Q, W_K, W_V\)</span>的变换，切成<span class="math inline">\(n\)</span>份（n为头数），维度从<span class="math inline">\(d_{model}\)</span>降到<span class="math inline">\(d_{head}\)</span>，分别进行attention计算再拼接；</p><p><strong>MQA</strong>：只对<span class="math inline">\(Q\)</span>切分，而<span class="math inline">\(K, V\)</span>直接在线形变换时将维度降至<span class="math inline">\(d_{head}\)</span>（而不是切分变小）</p></blockquote><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/1e6a04d4/image-27.png"></p><p>假设输入的维度为：<span class="math inline">\([b, s, d]\)</span>，其中<span class="math inline">\(b\)</span>为batch size，<span class="math inline">\(s\)</span>为sequence length，<span class="math inline">\(d\)</span>为hidden size。</p><ol type="1"><li><p><strong>线性变换</strong>：得到的<span class="math inline">\(Q\)</span>为<span class="math inline">\([b, s, d]\)</span>；<span class="math inline">\(K, V\)</span>为<span class="math inline">\([b, s, d_head]\)</span>.</p></li><li><p><strong>多头切分</strong>：</p><ul><li><p>将<span class="math inline">\(Q\)</span>按head切分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Q = Q.view(batch_size, -<span class="number">1</span>, <span class="variable language_">self</span>.num_heads, <span class="variable language_">self</span>.head_dim).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure></li><li><p>拓展<span class="math inline">\(K, V\)</span>以匹配<span class="math inline">\(Q\)</span>的维度：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">K = K.unsqueeze(<span class="number">1</span>).expand(-<span class="number">1</span>, <span class="variable language_">self</span>.num_heads, -<span class="number">1</span>, -<span class="number">1</span>)                      </span><br><span class="line">V = V.unsqueeze(<span class="number">1</span>).expand(-<span class="number">1</span>, <span class="variable language_">self</span>.num_heads, -<span class="number">1</span>, -<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li></ul></li><li><p><strong>注意力计算</strong>：计算<span class="math inline">\(Q, V\)</span>之间的点积： <span class="math display">\[ scores=\frac{Q_{split}K_{split}^T}{\sqrt{d_{head}}} \]</span> ​ 应用softmax获取注意力权重： <span class="math display">\[ W=softmax(scores) \]</span> ​ 使用注意力权重，对Value加权求和： <span class="math display">\[ context=WV_{split} \]</span></p></li><li><p><strong>多头合并</strong>：使用矩阵乘法 matmul广播，使得每个头都乘以这同一个张量，以此来实现KV参数共享。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">output = torch.matmul(attn, V)  </span><br><span class="line"><span class="comment"># (batch_size, num_heads, seq_len, head_dim)</span></span><br><span class="line">output = output.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(batch_size, -<span class="number">1</span>, <span class="variable language_">self</span>.d_model) <span class="comment"># (batch_size, seq_len, d_model)</span></span><br></pre></td></tr></table></figure></li></ol><p>数学公式：</p><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/1e6a04d4/image-28.png"></p><p>MQA代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiQueryAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, num_heads</span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiQueryAttention, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.num_heads = num_heads</span><br><span class="line">        <span class="variable language_">self</span>.d_model = d_model</span><br><span class="line">        <span class="variable language_">self</span>.head_dim = d_model // num_heads</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> (</span><br><span class="line">            <span class="variable language_">self</span>.head_dim * num_heads == d_model</span><br><span class="line">        ), <span class="string">&quot;d_model must be divisible by num_heads&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.query_linear = nn.Linear(d_model, d_model)</span><br><span class="line">        <span class="variable language_">self</span>.key_linear = nn.Linear(d_model, <span class="variable language_">self</span>.head_dim)</span><br><span class="line">        <span class="variable language_">self</span>.value_linear = nn.Linear(d_model, <span class="variable language_">self</span>.head_dim)</span><br><span class="line">        <span class="variable language_">self</span>.out_linear = nn.Linear(d_model, d_model)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, queries, keys, values, mask=<span class="literal">None</span></span>):</span><br><span class="line">        batch_size = queries.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 线性变换</span></span><br><span class="line">        Q = <span class="variable language_">self</span>.query_linear(queries)  <span class="comment"># (batch_size, seq_len, d_model)</span></span><br><span class="line">        K = <span class="variable language_">self</span>.key_linear(keys)       <span class="comment"># (batch_size, seq_len, head_dim)</span></span><br><span class="line">        V = <span class="variable language_">self</span>.value_linear(values)   <span class="comment"># (batch_size, seq_len, head_dim)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 分割为多个头</span></span><br><span class="line">        Q = Q.view(batch_size, -<span class="number">1</span>, <span class="variable language_">self</span>.num_heads, <span class="variable language_">self</span>.head_dim).transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># (batch_size, num_heads, seq_len, head_dim)</span></span><br><span class="line">        K = K.unsqueeze(<span class="number">1</span>).expand(-<span class="number">1</span>, <span class="variable language_">self</span>.num_heads, -<span class="number">1</span>, -<span class="number">1</span>)                      <span class="comment"># (batch_size, num_heads, seq_len, head_dim)</span></span><br><span class="line">        V = V.unsqueeze(<span class="number">1</span>).expand(-<span class="number">1</span>, <span class="variable language_">self</span>.num_heads, -<span class="number">1</span>, -<span class="number">1</span>)                      <span class="comment"># (batch_size, num_heads, seq_len, head_dim)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算注意力得分</span></span><br><span class="line">        scores = torch.matmul(Q, K.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / torch.sqrt(torch.tensor(<span class="variable language_">self</span>.head_dim, dtype=torch.float32))</span><br><span class="line">        <span class="keyword">if</span> mask isnotNone:</span><br><span class="line">            scores = scores.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e9</span>)</span><br><span class="line">        attn = F.softmax(scores, dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 计算注意力输出</span></span><br><span class="line">        output = torch.matmul(attn, V)  <span class="comment"># (batch_size, num_heads, seq_len, head_dim)</span></span><br><span class="line">    </span><br><span class="line">        output = output.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(batch_size, -<span class="number">1</span>, <span class="variable language_">self</span>.d_model)  <span class="comment"># (batch_size, seq_len, d_model)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.out_linear(output)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">1</span></span><br><span class="line">seq_len = <span class="number">3</span></span><br><span class="line">d_model = <span class="number">4</span></span><br><span class="line">num_heads = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机生成输入张量</span></span><br><span class="line">queries = torch.rand(batch_size, seq_len, d_model)</span><br><span class="line">keys = torch.rand(batch_size, seq_len, d_model)</span><br><span class="line">values = torch.rand(batch_size, seq_len, d_model)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化 MQA 模型</span></span><br><span class="line">mqa = MultiQueryAttention(d_model, num_heads)</span><br><span class="line"></span><br><span class="line">mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>)  <span class="comment"># (1, 1, seq_len, seq_len)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;mask:&#x27;</span>,mask)</span><br><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line">output = mqa(queries, keys, values,mask)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输出张量：&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure><h3 id="内存">内存</h3><p>MQA所需要缓存的KV值，从所有头减为一个头，KV Cache减少为之前的<span class="math inline">\(\frac{1}{h}\)</span>。</p><p>性能测试如下：</p><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/1e6a04d4/image-30.png"></p><ol type="1"><li>训练速度基本不变；</li><li>推理时间和beam-search时间大幅缩短；</li><li>推理过程中：Encoder推理速度基本不变；Decoder推理大幅加速。</li></ol><p><strong>MQA不改变计算量，但大幅降低了显存使用（降低KV Cache）</strong>：</p><ol type="1"><li>降低KV Cache的空间占用率；节省的显存空间可用于增加批次大小、提升吞吐量；</li><li>头数量的减少，导致从显存中读取的数据量减少，减少了计算单元的等待时间，从内存密集型趋近于计算密集型。</li></ol><h3 id="表征能力">表征能力</h3><p>共享K，V可能导致模型捕捉上下文的能力下降，限制模型的表征能力，导致任务效果相比MHA略有损失。</p><h3 id="通信">通信</h3><p>在多卡并行情况下，<strong>MQA减少了访存，但是增加了并行通信开销</strong>。由于<strong>K和V张量在所有头部之间共享，每个GPU上都需要有自己的备份</strong>。与下图(a)中MHA并行策略相比，<strong>MQA需要使用all-to-all对进行输入输出激活张量resharding，从而产生额外的通信成本</strong>。具体如下图(b)所示。另外，因为每个卡上都有备份，这可能会导致MQA的内存成本节省将会丧失。</p><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/1e6a04d4/image-31.png"></p><h2 id="gqa">GQA</h2><p>MHA和MQA的折中方案：采用<strong>分组</strong>机制，<strong>让多个 Query 共享少量的 Key 和 Value</strong>，减少自注意力计算的复杂度，同时保持 Transformer 的表达能力。</p><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/1e6a04d4/image-32.png"></p><ol type="1"><li><p><strong>Query多头计算</strong>：Query依然是<strong>每个头独立计算</strong>。假设有<span class="math inline">\(h\)</span>个注意力头，计算方式如下： <span class="math display">\[ Q_i=XW_Q^i, i=1,2,...,h \]</span> 其中：<span class="math inline">\(W_Q^i\)</span>是第<span class="math inline">\(i\)</span>个头的Query投影矩阵；计算出的<span class="math inline">\(Q_i\)</span>形状为<span class="math inline">\([b, s, d_{head}]\)</span>.（<span class="math inline">\(d_head=\frac{d}{h}\)</span>）</p></li><li><p><strong>共享分组：Key和Value计算</strong>。将Key和Value分成<span class="math inline">\(g\)</span>组，其中<span class="math inline">\(g&lt;h\)</span>，即： <span class="math display">\[ K_j=XW_K^j, V_j=XW_V^j, j=1,2,...,g \]</span> 计算出的<span class="math inline">\(K_j, V_j\)</span>形状为<span class="math inline">\([b, s, d_g]\)</span>（<span class="math inline">\(d_g=\frac{d}{g}\)</span>）</p></li><li><p><strong>计算注意力分数</strong>： <span class="math display">\[ A_i=softmax(\frac{Q_iK_j^T}{\sqrt{d_g}}) \]</span> 其中：<span class="math inline">\(Q_i\)</span>来自每个Query头；<span class="math inline">\(K_j\)</span>来自共享的Key组。计算得到的<span class="math inline">\(A_i\)</span>形状为<span class="math inline">\([b, s, s]\)</span>.</p></li><li><p><strong>计算加权Value</strong>： <span class="math display">\[ Z_i=A_iV_j \]</span></p></li></ol><p>​ 其中：<span class="math inline">\(V_j\)</span>是共享的Value组。计算得到的<span class="math inline">\(Z_i\)</span>形状为<span class="math inline">\([b, s, d_{head}]\)</span></p><ol start="5" type="1"><li><strong>输出计算</strong>：拼接所有注意力头计算的结果<span class="math inline">\(Z_i\)</span>会被拼接： <span class="math display">\[ Z=[Z_1, Z_2, ..., Z_h]W_O \]</span> 其中，<span class="math inline">\(W_O\)</span>是输出投影矩阵，最终得到形状为<span class="math inline">\([b, s, d]\)</span>的输出。</li></ol><p>GQA代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GQA</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, num_heads, num_groups</span>):</span><br><span class="line">        <span class="built_in">super</span>(GQA, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="keyword">assert</span> num_heads % num_groups == <span class="number">0</span>, <span class="string">&quot;Heads should be evenly divisible by groups&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.num_heads = num_heads</span><br><span class="line">        <span class="variable language_">self</span>.num_groups = num_groups</span><br><span class="line">        <span class="variable language_">self</span>.d_model = d_model</span><br><span class="line">        <span class="variable language_">self</span>.d_head = d_model // num_heads</span><br><span class="line">        <span class="variable language_">self</span>.d_group = d_model // num_groups  <span class="comment"># Key-Value 分组维度</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Query 仍然是独立的</span></span><br><span class="line">        <span class="variable language_">self</span>.W_q = nn.Linear(d_model, d_model, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># Key 和 Value 共享</span></span><br><span class="line">        <span class="variable language_">self</span>.W_k = nn.Linear(d_model, d_model // num_groups * num_heads, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.W_v = nn.Linear(d_model, d_model // num_groups * num_heads, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.W_o = nn.Linear(d_model, d_model, bias=<span class="literal">False</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        batch_size, seq_len, _ = x.shape</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算 Query, Key, Value</span></span><br><span class="line">        Q = <span class="variable language_">self</span>.W_q(x).view(batch_size, seq_len, <span class="variable language_">self</span>.num_heads, <span class="variable language_">self</span>.d_head)</span><br><span class="line">        K = <span class="variable language_">self</span>.W_k(x).view(batch_size, seq_len, <span class="variable language_">self</span>.num_groups, <span class="variable language_">self</span>.d_group)</span><br><span class="line">        V = <span class="variable language_">self</span>.W_v(x).view(batch_size, seq_len, <span class="variable language_">self</span>.num_groups, <span class="variable language_">self</span>.d_group)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算注意力分数</span></span><br><span class="line">        attention_scores = torch.einsum(<span class="string">&quot;bqhd,bkgd-&gt;bhqk&quot;</span>, Q, K) / (<span class="variable language_">self</span>.d_group ** <span class="number">0.5</span>)</span><br><span class="line">        attention_weights = torch.softmax(attention_scores, dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算注意力加权值</span></span><br><span class="line">        Z = torch.einsum(<span class="string">&quot;bhqk,bkgd-&gt;bqhd&quot;</span>, attention_weights, V)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 重新 reshape 并输出</span></span><br><span class="line">        Z = Z.reshape(batch_size, seq_len, <span class="variable language_">self</span>.d_model)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.W_o(Z)</span><br></pre></td></tr></table></figure><p>MHA，MLA，MQA对比：</p><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/1e6a04d4/image-33.png"></p><p>在MHA下，对于所有输入批次和序列中的每个token，KV Cache的总大小为： <span class="math display">\[ 2\times b\times l\times h\times d\times n \]</span> 其中，<span class="math inline">\(b\)</span>为batch size，<span class="math inline">\(l\)</span>为总序列长度（输入+输出序列），<span class="math inline">\(h\)</span>为注意力头数量，<span class="math inline">\(d\)</span>为每个head的维度，<span class="math inline">\(n\)</span>为层数。</p><p><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/posts/1e6a04d4/image-34.png"></p><p>上图中，<span class="math inline">\(g\)</span>为KV头的组数。当<span class="math inline">\(g=h\)</span>时是MLA；当<span class="math inline">\(g=1\)</span>时是MQA；当<span class="math inline">\(1&lt;g&lt;h\)</span>时，只将KV Cache压缩到<span class="math inline">\(\frac{g}{h}\)</span>。</p><p>GQA和MQA的性能收益主要来源于KV Cache的减少，支持放入更多tokens；但GQA和MQA的性能容易受到并行策略的影响。</p><p><strong>GQA和MQA的瓶颈主要在于加载 KV</strong>。如果GQA kernel在Q head维度上做并行（一个Q head对应一个block），则会导致共享一个KV head的block被调度在不同的SM上，每个SM 都会对同一份KV head 做重复加载。则内存减少的收益会大大降低。因此需要减少Q head的并行度。</p><blockquote><p>在llama2/3-70B中，GQA中<span class="math inline">\(g=8\)</span>，其他用了GQA的同体量模型基本上也保持了这个设置，这是出于对推理效率的考虑。70B体量的模型，如果不进行极端的量化，不可能部署到单卡（A100/H100 80G）上；一般情况下一台机可以装8张卡，而Attention的每个Head实际上是独立运算然后拼接起来的，因此，正好可以<strong>每张卡负责计算一组K、V对应的Attention Head</strong>。这样可以在尽可能保证K、V多样性的同时最大程度上减少卡间通信。</p></blockquote><h2 id="参考">参考</h2><p><a target="_blank" rel="noopener" href="http://nlp.seas.harvard.edu/annotated-transformer/">The Annotated Transformer</a></p><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.09297">MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer Decoding</a></p><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2302.14017">Full Stack Optimization of Transformer Inference: a Survey</a></p><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1911.02150">Fast Transformer Decoding: One Write-Head is All You Need</a></p><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2211.05102">Efficiently Scaling Transformer Inference</a></p><p><a target="_blank" rel="noopener" href="https://www.zhihu.com/column/c_1889336819960743598">探秘Transformer系列之（27）--- MQA &amp; GQA</a></p><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/700588653">缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA</a></p></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://wenliuyi.github.io">Liuyi Wen</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://wenliuyi.github.io/posts/1e6a04d4.html">http://wenliuyi.github.io/posts/1e6a04d4.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://wenliuyi.github.io" target="_blank">Liuyi Wen's Blog</a>！</span></div></div><div class="tag_share"><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related full-width" href="/posts/42e930ef.html" title="Transformer系列：1. 从RNN到Transformer"><div class="cover" style="background:var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">Transformer系列：1. 从RNN到Transformer</div></div><div class="info-2"><div class="info-item-1">场景： 图像信息：任务为理解图像内容，采用卷积神经网络； 序列信息：任务为理解语音/文字/视频，采用循环神经网络。 对于序列信息，由于按时序输入的数据之间非独立，前后数据之间具备相关性，因此网络需要存储信息的能力。 RNN 网络结构 RNN通过使用带自反馈的神经元，能够处理任意长度的序列 时序sequence：RNN能建模序列数据，序列指的是前、后输入数据\((x^{(t)}, x^{(t+1)})\)不独立，相互影响； 循环recurrent：对每个输入的操作都是一样的，循环往复地重复这些相同操作，每时刻有相同参数W和U（参数共享）； 记忆memory： 隐藏层\(h_{(t)}\)中捕捉了所有时刻t之前的信息，理论上\(h_{(t)}\)记忆的内容可以无限长，然而实际上记忆还是有限的； 正向计算 反向传播BPTT 梯度消失 / 梯度爆炸 循环神经网络的递归结构，导致梯度消失/梯度爆炸现象更明显 梯度爆炸：可采用梯度截断解决 由于梯度消失，RNN无法处理长期依赖关系。 比如，考虑一个语言模型，试图根据之前单词预测下一个； 如果要预测“The clouds...</div></div></div></a></nav><hr class="custom-hr"><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/img/butterfly-icon.png" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info-name">Liuyi Wen</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">28</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">5</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">13</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/WenLiuyi"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">The Journey Is the Reward.</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#scaled-dot-product-attention"><span class="toc-number">1.</span> <span class="toc-text">Scaled Dot-Product Attention</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#mha"><span class="toc-number">2.</span> <span class="toc-text">MHA</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#kv-cache"><span class="toc-number">3.</span> <span class="toc-text">KV Cache</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#key-cache"><span class="toc-number">3.1.</span> <span class="toc-text">Key Cache</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#value-cache"><span class="toc-number">3.2.</span> <span class="toc-text">Value Cache</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#mqa"><span class="toc-number">4.</span> <span class="toc-text">MQA</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%85%E5%AD%98"><span class="toc-number">4.1.</span> <span class="toc-text">内存</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A1%A8%E5%BE%81%E8%83%BD%E5%8A%9B"><span class="toc-number">4.2.</span> <span class="toc-text">表征能力</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%9A%E4%BF%A1"><span class="toc-number">4.3.</span> <span class="toc-text">通信</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#gqa"><span class="toc-number">5.</span> <span class="toc-text">GQA</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83"><span class="toc-number">6.</span> <span class="toc-text">参考</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/1e6a04d4.html" title="Transformer系列：2. Attention机制，MHA，MQA和GQA">Transformer系列：2. Attention机制，MHA，MQA和GQA</a><time datetime="2025-06-29T07:47:26.000Z" title="发表于 2025-06-29 15:47:26">2025-06-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/42e930ef.html" title="Transformer系列：1. 从RNN到Transformer">Transformer系列：1. 从RNN到Transformer</a><time datetime="2025-05-13T10:27:28.000Z" title="发表于 2025-05-13 18:27:28">2025-05-13</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/76aa9d6e.html" title="Transformer的KV Cache">Transformer的KV Cache</a><time datetime="2025-05-06T01:49:26.000Z" title="发表于 2025-05-06 09:49:26">2025-05-06</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/2ac460b1.html" title="verl框架：2. 对比OpenRLHF+colocate思路解析">verl框架：2. 对比OpenRLHF+colocate思路解析</a><time datetime="2025-05-06T01:49:06.000Z" title="发表于 2025-05-06 09:49:06">2025-05-06</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/5d9f220e.html" title="verl框架：1. Ray集群介绍+verl中基于Ray的执行流程解析">verl框架：1. Ray集群介绍+verl中基于Ray的执行流程解析</a><time datetime="2025-05-03T11:46:47.000Z" title="发表于 2025-05-03 19:46:47">2025-05-03</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By Liuyi Wen</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(()=>{const t=()=>{if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"all"},chtml:{scale:1.1},options:{enableMenu:!0,renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const n=!!e.type.match(/; *mode=display/),a=new t.options.MathItem(e.textContent,t.inputJax[0],n),d=document.createTextNode("");e.parentNode.replaceChild(d,e),a.start={node:d,delim:"",n:0},a.end={node:d,delim:"",n:0},t.math.push(a)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}};btf.addGlobalFn("encrypt",t,"mathjax"),window.pjax?t():window.addEventListener("load",t)})()</script><script>(()=>{const n="shuoshuo"===GLOBAL_CONFIG_SITE.pageType,e=(e,o)=>{n&&(window.shuoshuoComment.destroyValine=()=>{e.children.length&&(e.innerHTML="",e.classList.add("no-comment"))});const t={el:"#vcomment",appId:"bsxtUJWr1muoPS1pmoXLOPZ2-gzGzoHsz",appKey:"wm2wUYvKLEySwyRnFn7xAbJI",avatar:"monsterid",serverURLs:"",emojiMaps:"",visitor:!1,path:n?o:window.location.pathname};new Valine(t)},o=async(n,o)=>{"function"==typeof Valine||await btf.getScript("https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"),e(n,o)};n?window.shuoshuoComment={loadComment:o}:btf.loadComment(document.getElementById("vcomment"),o)})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-info"><div class="algolia-stats"></div><div class="algolia-poweredBy"></div></div></div></div></div><div id="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/algoliasearch/dist/lite/builds/browser.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instantsearch.js/dist/instantsearch.production.min.js"></script><script src="/js/search/algolia.js"></script></div></div>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body></html>